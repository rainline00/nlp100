{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．\n",
    "\n",
    "# 70. 単語ベクトルの和による特徴量\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例xiの特徴ベクトルxiを並べた行列Xと，正解ラベルを並べた行列（ベクトル）Yを作成したい．\n",
    "\n",
    "X=⎛⎝⎜⎜⎜⎜x1x2…xn⎞⎠⎟⎟⎟⎟∈ℝn×d,Y=⎛⎝⎜⎜⎜⎜y1y2…yn⎞⎠⎟⎟⎟⎟∈ℕn\n",
    "ここで，nは学習データの事例数であり，xi∈ℝdとyi∈ℕはそれぞれ，i∈{1,…,n}番目の事例の特徴量ベクトルと正解ラベルを表す． なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．ℕ<4で4未満の自然数（0を含む）を表すことにすれば，任意の事例の正解ラベルyiはyi∈ℕ<4で表現できる． 以降では，ラベルの種類数をLで表す（今回の分類タスクではL=4である）．\n",
    "\n",
    "i番目の事例の特徴ベクトルxiは，次式で求める．\n",
    "\n",
    "xi=1Ti∑t=1Tiemb(wi,t)\n",
    "ここで，i番目の事例はTi個の（記事見出しの）単語列(wi,1,wi,2,…,wi,Ti)から構成され，emb(w)∈ℝdは単語wに対応する単語ベクトル（次元数はd）である．すなわち，i番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものがxiである．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．300次元の単語ベクトルを用いたので，d=300である．\n",
    "\n",
    "i番目の事例のラベルyiは，次のように定義する．\n",
    "\n",
    "yi=⎧⎩⎨⎪⎪0123(記事xiが「ビジネス」カテゴリの場合)(記事xiが「科学技術」カテゴリの場合)(記事xiが「エンターテイメント」カテゴリの場合)(記事xiが「健康」カテゴリの場合)\n",
    "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
    "\n",
    "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
    "\n",
    "学習データの特徴量行列: Xtrain∈ℝNt×d\n",
    "学習データのラベルベクトル: Ytrain∈ℕNt\n",
    "検証データの特徴量行列: Xvalid∈ℝNv×d\n",
    "検証データのラベルベクトル: Yvalid∈ℕNv\n",
    "評価データの特徴量行列: Xtest∈ℝNe×d\n",
    "評価データのラベルベクトル: Ytest∈ℕNe\n",
    "なお，Nt,Nv,Neはそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../chap07/input/GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "import re\n",
    "from swem import SWEM\n",
    "# b = business, t = science and technology, e = entertainment, m = health\n",
    "category_map = {\"b\": 0, \"t\": 1, \"e\": 2, \"m\": 3}\n",
    "swem = SWEM(word_vectors)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "dir_path = \"../chap06/output/\"\n",
    "files = [\"train\", \"valid\", \"test\"]\n",
    "for file in files:\n",
    "    path = dir_path + file + \".txt\"\n",
    "    df = pd.read_table(path, header=None, names=(\"category\", \"title\"))\n",
    "    df[\"category\"] = df[\"category\"].map(category_map)\n",
    "    df[\"title\"] = hero.clean(df[\"title\"], pipeline=[\n",
    "        hero.preprocessing.fillna,\n",
    "        hero.preprocessing.lowercase,\n",
    "        hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_punctuation,\n",
    "        hero.preprocessing.remove_diacritics,\n",
    "        hero.preprocessing.remove_stopwords\n",
    "    ])\n",
    "    df[\"title\"] = [swem.infer_vector(tokens=title, method=\"avg\") for title in df[\"title\"]]\n",
    "    X = np.array([X for X in df[\"title\"]])\n",
    "    y = np.array(df[\"category\"], dtype=np.int64)\n",
    "    print(X.shape, y.shape)\n",
    "    path_to = \"./output/\" + file\n",
    "    np.savetxt(path_to + \"_X\", X)\n",
    "    np.savetxt(path_to + \"_y\", y, fmt=\"%d\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10672, 300) (10672,)\n",
      "(1334, 300) (1334,)\n",
      "(1334, 300) (1334,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 71. 単層ニューラルネットワークによる予測\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
    "\n",
    "ŷ 1=softmax(x1W),Ŷ =softmax(X[1:4]W)\n",
    "ただし，softmaxはソフトマックス関数，X[1:4]∈ℝ4×dは特徴ベクトルx1,x2,x3,x4を縦に並べた行列である．\n",
    "\n",
    "X[1:4]=⎛⎝⎜⎜⎜⎜x1x2x3x4⎞⎠⎟⎟⎟⎟\n",
    "行列W∈ℝd×Lは単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，ŷ 1∈ℝLは未学習の行列Wで事例x1を分類したときに，各カテゴリに属する確率を表すベクトルである． 同様に，Ŷ ∈ℝn×Lは，学習データの事例x1,x2,x3,x4について，各カテゴリに属する確率を行列として表現している．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "X = np.loadtxt(\"./output/train_X\")\n",
    "y = np.loadtxt(\"./output/train_y\", dtype=np.int64)\n",
    "np.random.seed(seed=0)\n",
    "W = np.random.rand(300, 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "from scipy.special import softmax\n",
    "y_1 = softmax(np.matmul(X[0], W))\n",
    "Y = softmax(np.matmul(X[:4], W), axis=1)\n",
    "print(y_1)\n",
    "print(Y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.29065256 0.20196849 0.28568237 0.22169657]\n",
      "[[0.29065256 0.20196849 0.28568237 0.22169657]\n",
      " [0.2799393  0.19786299 0.28166245 0.24053525]\n",
      " [0.25940632 0.19147765 0.29258893 0.25652709]\n",
      " [0.25143516 0.2071726  0.30296661 0.23842563]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pytorchを使った実装"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(300, 4),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = SimpleNet().to(\"cpu\")\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SimpleNet(\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=4, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "#x = torch.from_numpy(X[0]).float()\n",
    "X_4 = torch.from_numpy(X[:4]).float()\n",
    "#print(model(x))\n",
    "print(model(X_4))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2600, 0.2413, 0.2407, 0.2581],\n",
      "        [0.2561, 0.2392, 0.2432, 0.2616],\n",
      "        [0.2584, 0.2415, 0.2418, 0.2584],\n",
      "        [0.2575, 0.2386, 0.2422, 0.2618]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 72. 損失と勾配の計算\n",
    "学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．なお，ある事例xiに対して損失は次式で計算される．\n",
    "\n",
    "li=−log[事例xiがyiに分類される確率]\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "import math\n",
    "def closs_entropy_loss(y_pred, y_true):\n",
    "    return -math.log(y_pred[y_true])\n",
    "\n",
    "loss = closs_entropy_loss(y_1, y[0])\n",
    "print(\"loss for x1:\", loss)\n",
    "loss_1to4 = sum([closs_entropy_loss(y_pred, y[i]) for i, y_pred in enumerate(Y)])\n",
    "print(\"loss for x1~x4:\", loss_1to4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss for x1: 1.235626663422327\n",
      "loss for x1~x4: 4.925791861011294\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# pytorchを用いた実装\n",
    "loss = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "X_4 = torch.from_numpy(X[:4]).float()\n",
    "y_true_4 = torch.from_numpy(y[:4]).long()\n",
    "output = loss(model(X_4), y_true_4)\n",
    "print(\"loss:\", output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: tensor(5.5509, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 73. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_X = torch.from_numpy(np.loadtxt(\"./output/train_X\")).float()\n",
    "train_y = torch.from_numpy(np.loadtxt(\"./output/train_y\")).long()\n",
    "train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "test_X = torch.from_numpy(np.loadtxt(\"./output/test_X\")).float()\n",
    "test_y = torch.from_numpy(np.loadtxt(\"./output/test_y\")).long()\n",
    "test_dataset = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "learning_rate = 1e-3\n",
    "#batch_size = 64\n",
    "epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.376356  [    0/10672]\n",
      "loss: 1.377466  [  100/10672]\n",
      "loss: 1.373157  [  200/10672]\n",
      "loss: 1.369860  [  300/10672]\n",
      "loss: 1.387646  [  400/10672]\n",
      "loss: 1.385123  [  500/10672]\n",
      "loss: 1.398395  [  600/10672]\n",
      "loss: 1.352973  [  700/10672]\n",
      "loss: 1.404158  [  800/10672]\n",
      "loss: 1.374236  [  900/10672]\n",
      "loss: 1.409981  [ 1000/10672]\n",
      "loss: 1.377031  [ 1100/10672]\n",
      "loss: 1.372840  [ 1200/10672]\n",
      "loss: 1.337023  [ 1300/10672]\n",
      "loss: 1.371597  [ 1400/10672]\n",
      "loss: 1.332374  [ 1500/10672]\n",
      "loss: 1.365273  [ 1600/10672]\n",
      "loss: 1.323455  [ 1700/10672]\n",
      "loss: 1.357345  [ 1800/10672]\n",
      "loss: 1.359729  [ 1900/10672]\n",
      "loss: 1.359824  [ 2000/10672]\n",
      "loss: 1.352996  [ 2100/10672]\n",
      "loss: 1.350341  [ 2200/10672]\n",
      "loss: 1.346632  [ 2300/10672]\n",
      "loss: 1.317494  [ 2400/10672]\n",
      "loss: 1.312751  [ 2500/10672]\n",
      "loss: 1.315899  [ 2600/10672]\n",
      "loss: 1.451793  [ 2700/10672]\n",
      "loss: 1.306592  [ 2800/10672]\n",
      "loss: 1.449003  [ 2900/10672]\n",
      "loss: 1.456150  [ 3000/10672]\n",
      "loss: 1.332735  [ 3100/10672]\n",
      "loss: 1.329353  [ 3200/10672]\n",
      "loss: 1.290983  [ 3300/10672]\n",
      "loss: 1.466374  [ 3400/10672]\n",
      "loss: 1.469070  [ 3500/10672]\n",
      "loss: 1.473823  [ 3600/10672]\n",
      "loss: 1.331139  [ 3700/10672]\n",
      "loss: 1.278072  [ 3800/10672]\n",
      "loss: 1.328789  [ 3900/10672]\n",
      "loss: 1.338975  [ 4000/10672]\n",
      "loss: 1.476758  [ 4100/10672]\n",
      "loss: 1.480240  [ 4200/10672]\n",
      "loss: 1.273050  [ 4300/10672]\n",
      "loss: 1.481678  [ 4400/10672]\n",
      "loss: 1.320392  [ 4500/10672]\n",
      "loss: 1.319936  [ 4600/10672]\n",
      "loss: 1.321712  [ 4700/10672]\n",
      "loss: 1.493159  [ 4800/10672]\n",
      "loss: 1.266876  [ 4900/10672]\n",
      "loss: 1.488235  [ 5000/10672]\n",
      "loss: 1.257803  [ 5100/10672]\n",
      "loss: 1.256280  [ 5200/10672]\n",
      "loss: 1.312209  [ 5300/10672]\n",
      "loss: 1.248645  [ 5400/10672]\n",
      "loss: 1.316039  [ 5500/10672]\n",
      "loss: 1.243889  [ 5600/10672]\n",
      "loss: 1.504573  [ 5700/10672]\n",
      "loss: 1.312582  [ 5800/10672]\n",
      "loss: 1.244577  [ 5900/10672]\n",
      "loss: 1.506614  [ 6000/10672]\n",
      "loss: 1.309730  [ 6100/10672]\n",
      "loss: 1.250592  [ 6200/10672]\n",
      "loss: 1.304915  [ 6300/10672]\n",
      "loss: 1.234132  [ 6400/10672]\n",
      "loss: 1.521263  [ 6500/10672]\n",
      "loss: 1.226484  [ 6600/10672]\n",
      "loss: 1.307431  [ 6700/10672]\n",
      "loss: 1.224186  [ 6800/10672]\n",
      "loss: 1.308097  [ 6900/10672]\n",
      "loss: 1.222885  [ 7000/10672]\n",
      "loss: 1.217599  [ 7100/10672]\n",
      "loss: 1.305879  [ 7200/10672]\n",
      "loss: 1.230807  [ 7300/10672]\n",
      "loss: 1.298216  [ 7400/10672]\n",
      "loss: 1.227951  [ 7500/10672]\n",
      "loss: 1.305661  [ 7600/10672]\n",
      "loss: 1.300579  [ 7700/10672]\n",
      "loss: 1.299083  [ 7800/10672]\n",
      "loss: 1.209036  [ 7900/10672]\n",
      "loss: 1.207884  [ 8000/10672]\n",
      "loss: 1.218758  [ 8100/10672]\n",
      "loss: 1.304184  [ 8200/10672]\n",
      "loss: 1.304391  [ 8300/10672]\n",
      "loss: 1.221139  [ 8400/10672]\n",
      "loss: 1.203626  [ 8500/10672]\n",
      "loss: 1.229322  [ 8600/10672]\n",
      "loss: 1.301468  [ 8700/10672]\n",
      "loss: 1.291077  [ 8800/10672]\n",
      "loss: 1.195877  [ 8900/10672]\n",
      "loss: 1.546952  [ 9000/10672]\n",
      "loss: 1.306631  [ 9100/10672]\n",
      "loss: 1.197331  [ 9200/10672]\n",
      "loss: 1.203290  [ 9300/10672]\n",
      "loss: 1.190396  [ 9400/10672]\n",
      "loss: 1.307640  [ 9500/10672]\n",
      "loss: 1.569282  [ 9600/10672]\n",
      "loss: 1.304515  [ 9700/10672]\n",
      "loss: 1.185676  [ 9800/10672]\n",
      "loss: 1.182719  [ 9900/10672]\n",
      "loss: 1.192118  [10000/10672]\n",
      "loss: 1.178995  [10100/10672]\n",
      "loss: 1.179406  [10200/10672]\n",
      "loss: 1.301736  [10300/10672]\n",
      "loss: 1.305944  [10400/10672]\n",
      "loss: 1.304089  [10500/10672]\n",
      "loss: 1.550219  [10600/10672]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.166675  [    0/10672]\n",
      "loss: 1.172651  [  100/10672]\n",
      "loss: 1.174547  [  200/10672]\n",
      "loss: 1.171450  [  300/10672]\n",
      "loss: 1.307895  [  400/10672]\n",
      "loss: 1.305958  [  500/10672]\n",
      "loss: 1.571803  [  600/10672]\n",
      "loss: 1.154533  [  700/10672]\n",
      "loss: 1.567844  [  800/10672]\n",
      "loss: 1.305599  [  900/10672]\n",
      "loss: 1.577561  [ 1000/10672]\n",
      "loss: 1.309689  [ 1100/10672]\n",
      "loss: 1.303513  [ 1200/10672]\n",
      "loss: 1.159794  [ 1300/10672]\n",
      "loss: 1.303813  [ 1400/10672]\n",
      "loss: 1.160408  [ 1500/10672]\n",
      "loss: 1.300216  [ 1600/10672]\n",
      "loss: 1.150307  [ 1700/10672]\n",
      "loss: 1.296260  [ 1800/10672]\n",
      "loss: 1.297954  [ 1900/10672]\n",
      "loss: 1.299541  [ 2000/10672]\n",
      "loss: 1.295368  [ 2100/10672]\n",
      "loss: 1.293183  [ 2200/10672]\n",
      "loss: 1.290635  [ 2300/10672]\n",
      "loss: 1.169867  [ 2400/10672]\n",
      "loss: 1.162743  [ 2500/10672]\n",
      "loss: 1.172718  [ 2600/10672]\n",
      "loss: 1.580713  [ 2700/10672]\n",
      "loss: 1.163002  [ 2800/10672]\n",
      "loss: 1.562563  [ 2900/10672]\n",
      "loss: 1.577198  [ 3000/10672]\n",
      "loss: 1.280978  [ 3100/10672]\n",
      "loss: 1.279036  [ 3200/10672]\n",
      "loss: 1.152141  [ 3300/10672]\n",
      "loss: 1.581194  [ 3400/10672]\n",
      "loss: 1.587705  [ 3500/10672]\n",
      "loss: 1.587568  [ 3600/10672]\n",
      "loss: 1.284958  [ 3700/10672]\n",
      "loss: 1.151158  [ 3800/10672]\n",
      "loss: 1.288885  [ 3900/10672]\n",
      "loss: 1.293739  [ 4000/10672]\n",
      "loss: 1.582094  [ 4100/10672]\n",
      "loss: 1.586011  [ 4200/10672]\n",
      "loss: 1.154936  [ 4300/10672]\n",
      "loss: 1.582309  [ 4400/10672]\n",
      "loss: 1.282560  [ 4500/10672]\n",
      "loss: 1.284268  [ 4600/10672]\n",
      "loss: 1.283260  [ 4700/10672]\n",
      "loss: 1.589253  [ 4800/10672]\n",
      "loss: 1.163010  [ 4900/10672]\n",
      "loss: 1.580143  [ 5000/10672]\n",
      "loss: 1.154561  [ 5100/10672]\n",
      "loss: 1.153475  [ 5200/10672]\n",
      "loss: 1.284404  [ 5300/10672]\n",
      "loss: 1.147984  [ 5400/10672]\n",
      "loss: 1.277894  [ 5500/10672]\n",
      "loss: 1.146624  [ 5600/10672]\n",
      "loss: 1.592543  [ 5700/10672]\n",
      "loss: 1.276818  [ 5800/10672]\n",
      "loss: 1.153213  [ 5900/10672]\n",
      "loss: 1.590653  [ 6000/10672]\n",
      "loss: 1.277027  [ 6100/10672]\n",
      "loss: 1.164651  [ 6200/10672]\n",
      "loss: 1.271432  [ 6300/10672]\n",
      "loss: 1.148602  [ 6400/10672]\n",
      "loss: 1.600923  [ 6500/10672]\n",
      "loss: 1.141333  [ 6600/10672]\n",
      "loss: 1.280770  [ 6700/10672]\n",
      "loss: 1.142669  [ 6800/10672]\n",
      "loss: 1.277854  [ 6900/10672]\n",
      "loss: 1.143985  [ 7000/10672]\n",
      "loss: 1.138273  [ 7100/10672]\n",
      "loss: 1.277572  [ 7200/10672]\n",
      "loss: 1.159481  [ 7300/10672]\n",
      "loss: 1.269400  [ 7400/10672]\n",
      "loss: 1.155112  [ 7500/10672]\n",
      "loss: 1.278948  [ 7600/10672]\n",
      "loss: 1.273265  [ 7700/10672]\n",
      "loss: 1.273642  [ 7800/10672]\n",
      "loss: 1.139855  [ 7900/10672]\n",
      "loss: 1.139316  [ 8000/10672]\n",
      "loss: 1.153134  [ 8100/10672]\n",
      "loss: 1.277983  [ 8200/10672]\n",
      "loss: 1.276656  [ 8300/10672]\n",
      "loss: 1.159864  [ 8400/10672]\n",
      "loss: 1.140017  [ 8500/10672]\n",
      "loss: 1.170964  [ 8600/10672]\n",
      "loss: 1.276716  [ 8700/10672]\n",
      "loss: 1.264257  [ 8800/10672]\n",
      "loss: 1.135747  [ 8900/10672]\n",
      "loss: 1.605852  [ 9000/10672]\n",
      "loss: 1.277834  [ 9100/10672]\n",
      "loss: 1.140762  [ 9200/10672]\n",
      "loss: 1.149059  [ 9300/10672]\n",
      "loss: 1.137253  [ 9400/10672]\n",
      "loss: 1.283662  [ 9500/10672]\n",
      "loss: 1.621415  [ 9600/10672]\n",
      "loss: 1.281101  [ 9700/10672]\n",
      "loss: 1.134954  [ 9800/10672]\n",
      "loss: 1.132011  [ 9900/10672]\n",
      "loss: 1.143498  [10000/10672]\n",
      "loss: 1.130264  [10100/10672]\n",
      "loss: 1.129046  [10200/10672]\n",
      "loss: 1.278691  [10300/10672]\n",
      "loss: 1.285854  [10400/10672]\n",
      "loss: 1.283360  [10500/10672]\n",
      "loss: 1.600403  [10600/10672]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.120908  [    0/10672]\n",
      "loss: 1.129043  [  100/10672]\n",
      "loss: 1.128481  [  200/10672]\n",
      "loss: 1.126238  [  300/10672]\n",
      "loss: 1.288132  [  400/10672]\n",
      "loss: 1.282155  [  500/10672]\n",
      "loss: 1.615802  [  600/10672]\n",
      "loss: 1.113285  [  700/10672]\n",
      "loss: 1.611439  [  800/10672]\n",
      "loss: 1.291498  [  900/10672]\n",
      "loss: 1.619395  [ 1000/10672]\n",
      "loss: 1.288051  [ 1100/10672]\n",
      "loss: 1.280691  [ 1200/10672]\n",
      "loss: 1.125162  [ 1300/10672]\n",
      "loss: 1.277584  [ 1400/10672]\n",
      "loss: 1.125249  [ 1500/10672]\n",
      "loss: 1.275646  [ 1600/10672]\n",
      "loss: 1.116274  [ 1700/10672]\n",
      "loss: 1.274459  [ 1800/10672]\n",
      "loss: 1.272823  [ 1900/10672]\n",
      "loss: 1.275413  [ 2000/10672]\n",
      "loss: 1.272557  [ 2100/10672]\n",
      "loss: 1.269695  [ 2200/10672]\n",
      "loss: 1.268262  [ 2300/10672]\n",
      "loss: 1.141891  [ 2400/10672]\n",
      "loss: 1.135722  [ 2500/10672]\n",
      "loss: 1.144642  [ 2600/10672]\n",
      "loss: 1.616586  [ 2700/10672]\n",
      "loss: 1.137363  [ 2800/10672]\n",
      "loss: 1.600773  [ 2900/10672]\n",
      "loss: 1.613975  [ 3000/10672]\n",
      "loss: 1.257180  [ 3100/10672]\n",
      "loss: 1.253359  [ 3200/10672]\n",
      "loss: 1.128560  [ 3300/10672]\n",
      "loss: 1.616225  [ 3400/10672]\n",
      "loss: 1.620837  [ 3500/10672]\n",
      "loss: 1.620910  [ 3600/10672]\n",
      "loss: 1.260125  [ 3700/10672]\n",
      "loss: 1.128938  [ 3800/10672]\n",
      "loss: 1.266881  [ 3900/10672]\n",
      "loss: 1.267379  [ 4000/10672]\n",
      "loss: 1.615325  [ 4100/10672]\n",
      "loss: 1.617546  [ 4200/10672]\n",
      "loss: 1.135010  [ 4300/10672]\n",
      "loss: 1.614723  [ 4400/10672]\n",
      "loss: 1.258630  [ 4500/10672]\n",
      "loss: 1.262844  [ 4600/10672]\n",
      "loss: 1.258515  [ 4700/10672]\n",
      "loss: 1.619662  [ 4800/10672]\n",
      "loss: 1.144198  [ 4900/10672]\n",
      "loss: 1.611392  [ 5000/10672]\n",
      "loss: 1.139746  [ 5100/10672]\n",
      "loss: 1.136504  [ 5200/10672]\n",
      "loss: 1.265854  [ 5300/10672]\n",
      "loss: 1.131572  [ 5400/10672]\n",
      "loss: 1.250914  [ 5500/10672]\n",
      "loss: 1.132492  [ 5600/10672]\n",
      "loss: 1.621060  [ 5700/10672]\n",
      "loss: 1.251691  [ 5800/10672]\n",
      "loss: 1.140713  [ 5900/10672]\n",
      "loss: 1.618261  [ 6000/10672]\n",
      "loss: 1.253969  [ 6100/10672]\n",
      "loss: 1.148034  [ 6200/10672]\n",
      "loss: 1.246161  [ 6300/10672]\n",
      "loss: 1.135437  [ 6400/10672]\n",
      "loss: 1.626621  [ 6500/10672]\n",
      "loss: 1.127280  [ 6600/10672]\n",
      "loss: 1.259382  [ 6700/10672]\n",
      "loss: 1.131400  [ 6800/10672]\n",
      "loss: 1.254732  [ 6900/10672]\n",
      "loss: 1.133485  [ 7000/10672]\n",
      "loss: 1.124508  [ 7100/10672]\n",
      "loss: 1.255815  [ 7200/10672]\n",
      "loss: 1.149537  [ 7300/10672]\n",
      "loss: 1.246687  [ 7400/10672]\n",
      "loss: 1.141030  [ 7500/10672]\n",
      "loss: 1.257663  [ 7600/10672]\n",
      "loss: 1.252041  [ 7700/10672]\n",
      "loss: 1.253219  [ 7800/10672]\n",
      "loss: 1.129932  [ 7900/10672]\n",
      "loss: 1.128005  [ 8000/10672]\n",
      "loss: 1.142112  [ 8100/10672]\n",
      "loss: 1.255583  [ 8200/10672]\n",
      "loss: 1.254096  [ 8300/10672]\n",
      "loss: 1.149308  [ 8400/10672]\n",
      "loss: 1.127868  [ 8500/10672]\n",
      "loss: 1.158817  [ 8600/10672]\n",
      "loss: 1.256130  [ 8700/10672]\n",
      "loss: 1.241515  [ 8800/10672]\n",
      "loss: 1.125455  [ 8900/10672]\n",
      "loss: 1.627200  [ 9000/10672]\n",
      "loss: 1.254893  [ 9100/10672]\n",
      "loss: 1.131455  [ 9200/10672]\n",
      "loss: 1.140705  [ 9300/10672]\n",
      "loss: 1.130675  [ 9400/10672]\n",
      "loss: 1.263205  [ 9500/10672]\n",
      "loss: 1.638907  [ 9600/10672]\n",
      "loss: 1.261804  [ 9700/10672]\n",
      "loss: 1.128163  [ 9800/10672]\n",
      "loss: 1.124167  [ 9900/10672]\n",
      "loss: 1.135968  [10000/10672]\n",
      "loss: 1.123328  [10100/10672]\n",
      "loss: 1.119223  [10200/10672]\n",
      "loss: 1.258625  [10300/10672]\n",
      "loss: 1.267330  [10400/10672]\n",
      "loss: 1.264612  [10500/10672]\n",
      "loss: 1.620944  [10600/10672]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.113085  [    0/10672]\n",
      "loss: 1.122769  [  100/10672]\n",
      "loss: 1.118165  [  200/10672]\n",
      "loss: 1.116583  [  300/10672]\n",
      "loss: 1.271044  [  400/10672]\n",
      "loss: 1.262303  [  500/10672]\n",
      "loss: 1.632794  [  600/10672]\n",
      "loss: 1.106029  [  700/10672]\n",
      "loss: 1.628950  [  800/10672]\n",
      "loss: 1.278094  [  900/10672]\n",
      "loss: 1.635467  [ 1000/10672]\n",
      "loss: 1.271325  [ 1100/10672]\n",
      "loss: 1.261833  [ 1200/10672]\n",
      "loss: 1.120565  [ 1300/10672]\n",
      "loss: 1.256549  [ 1400/10672]\n",
      "loss: 1.118830  [ 1500/10672]\n",
      "loss: 1.255521  [ 1600/10672]\n",
      "loss: 1.110395  [ 1700/10672]\n",
      "loss: 1.257210  [ 1800/10672]\n",
      "loss: 1.252995  [ 1900/10672]\n",
      "loss: 1.256820  [ 2000/10672]\n",
      "loss: 1.253962  [ 2100/10672]\n",
      "loss: 1.250511  [ 2200/10672]\n",
      "loss: 1.251091  [ 2300/10672]\n",
      "loss: 1.137995  [ 2400/10672]\n",
      "loss: 1.132522  [ 2500/10672]\n",
      "loss: 1.139890  [ 2600/10672]\n",
      "loss: 1.631834  [ 2700/10672]\n",
      "loss: 1.134208  [ 2800/10672]\n",
      "loss: 1.618544  [ 2900/10672]\n",
      "loss: 1.629963  [ 3000/10672]\n",
      "loss: 1.239777  [ 3100/10672]\n",
      "loss: 1.232942  [ 3200/10672]\n",
      "loss: 1.125560  [ 3300/10672]\n",
      "loss: 1.631580  [ 3400/10672]\n",
      "loss: 1.635025  [ 3500/10672]\n",
      "loss: 1.635174  [ 3600/10672]\n",
      "loss: 1.241793  [ 3700/10672]\n",
      "loss: 1.125583  [ 3800/10672]\n",
      "loss: 1.250767  [ 3900/10672]\n",
      "loss: 1.247991  [ 4000/10672]\n",
      "loss: 1.630326  [ 4100/10672]\n",
      "loss: 1.631684  [ 4200/10672]\n",
      "loss: 1.132558  [ 4300/10672]\n",
      "loss: 1.629591  [ 4400/10672]\n",
      "loss: 1.240914  [ 4500/10672]\n",
      "loss: 1.248104  [ 4600/10672]\n",
      "loss: 1.240552  [ 4700/10672]\n",
      "loss: 1.633508  [ 4800/10672]\n",
      "loss: 1.140875  [ 4900/10672]\n",
      "loss: 1.626318  [ 5000/10672]\n",
      "loss: 1.139537  [ 5100/10672]\n",
      "loss: 1.133899  [ 5200/10672]\n",
      "loss: 1.253934  [ 5300/10672]\n",
      "loss: 1.129259  [ 5400/10672]\n",
      "loss: 1.231192  [ 5500/10672]\n",
      "loss: 1.131577  [ 5600/10672]\n",
      "loss: 1.634315  [ 5700/10672]\n",
      "loss: 1.234149  [ 5800/10672]\n",
      "loss: 1.140666  [ 5900/10672]\n",
      "loss: 1.631440  [ 6000/10672]\n",
      "loss: 1.238567  [ 6100/10672]\n",
      "loss: 1.143414  [ 6200/10672]\n",
      "loss: 1.228388  [ 6300/10672]\n",
      "loss: 1.133923  [ 6400/10672]\n",
      "loss: 1.638570  [ 6500/10672]\n",
      "loss: 1.124386  [ 6600/10672]\n",
      "loss: 1.245074  [ 6700/10672]\n",
      "loss: 1.130295  [ 6800/10672]\n",
      "loss: 1.239145  [ 6900/10672]\n",
      "loss: 1.132934  [ 7000/10672]\n",
      "loss: 1.120451  [ 7100/10672]\n",
      "loss: 1.241689  [ 7200/10672]\n",
      "loss: 1.149332  [ 7300/10672]\n",
      "loss: 1.231764  [ 7400/10672]\n",
      "loss: 1.136093  [ 7500/10672]\n",
      "loss: 1.243944  [ 7600/10672]\n",
      "loss: 1.238632  [ 7700/10672]\n",
      "loss: 1.240437  [ 7800/10672]\n",
      "loss: 1.128490  [ 7900/10672]\n",
      "loss: 1.125237  [ 8000/10672]\n",
      "loss: 1.139552  [ 8100/10672]\n",
      "loss: 1.240479  [ 8200/10672]\n",
      "loss: 1.239079  [ 8300/10672]\n",
      "loss: 1.146997  [ 8400/10672]\n",
      "loss: 1.123643  [ 8500/10672]\n",
      "loss: 1.154960  [ 8600/10672]\n",
      "loss: 1.243045  [ 8700/10672]\n",
      "loss: 1.226408  [ 8800/10672]\n",
      "loss: 1.122144  [ 8900/10672]\n",
      "loss: 1.637873  [ 9000/10672]\n",
      "loss: 1.239459  [ 9100/10672]\n",
      "loss: 1.128605  [ 9200/10672]\n",
      "loss: 1.139300  [ 9300/10672]\n",
      "loss: 1.130479  [ 9400/10672]\n",
      "loss: 1.249858  [ 9500/10672]\n",
      "loss: 1.647340  [ 9600/10672]\n",
      "loss: 1.249830  [ 9700/10672]\n",
      "loss: 1.127308  [ 9800/10672]\n",
      "loss: 1.122182  [ 9900/10672]\n",
      "loss: 1.134713  [10000/10672]\n",
      "loss: 1.121833  [10100/10672]\n",
      "loss: 1.114965  [10200/10672]\n",
      "loss: 1.245713  [10300/10672]\n",
      "loss: 1.255647  [10400/10672]\n",
      "loss: 1.252812  [10500/10672]\n",
      "loss: 1.632025  [10600/10672]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.110168  [    0/10672]\n",
      "loss: 1.121315  [  100/10672]\n",
      "loss: 1.112703  [  200/10672]\n",
      "loss: 1.111832  [  300/10672]\n",
      "loss: 1.260871  [  400/10672]\n",
      "loss: 1.249397  [  500/10672]\n",
      "loss: 1.641669  [  600/10672]\n",
      "loss: 1.103062  [  700/10672]\n",
      "loss: 1.638323  [  800/10672]\n",
      "loss: 1.271600  [  900/10672]\n",
      "loss: 1.643817  [ 1000/10672]\n",
      "loss: 1.261584  [ 1100/10672]\n",
      "loss: 1.249997  [ 1200/10672]\n",
      "loss: 1.119642  [ 1300/10672]\n",
      "loss: 1.242472  [ 1400/10672]\n",
      "loss: 1.116001  [ 1500/10672]\n",
      "loss: 1.242377  [ 1600/10672]\n",
      "loss: 1.107803  [ 1700/10672]\n",
      "loss: 1.247071  [ 1800/10672]\n",
      "loss: 1.240199  [ 1900/10672]\n",
      "loss: 1.245327  [ 2000/10672]\n",
      "loss: 1.242440  [ 2100/10672]\n",
      "loss: 1.238417  [ 2200/10672]\n",
      "loss: 1.241118  [ 2300/10672]\n",
      "loss: 1.136874  [ 2400/10672]\n",
      "loss: 1.131703  [ 2500/10672]\n",
      "loss: 1.138029  [ 2600/10672]\n",
      "loss: 1.640197  [ 2700/10672]\n",
      "loss: 1.133229  [ 2800/10672]\n",
      "loss: 1.628832  [ 2900/10672]\n",
      "loss: 1.638828  [ 3000/10672]\n",
      "loss: 1.229678  [ 3100/10672]\n",
      "loss: 1.219663  [ 3200/10672]\n",
      "loss: 1.124154  [ 3300/10672]\n",
      "loss: 1.640142  [ 3400/10672]\n",
      "loss: 1.642834  [ 3500/10672]\n",
      "loss: 1.642969  [ 3600/10672]\n",
      "loss: 1.230392  [ 3700/10672]\n",
      "loss: 1.124061  [ 3800/10672]\n",
      "loss: 1.241422  [ 3900/10672]\n",
      "loss: 1.235139  [ 4000/10672]\n",
      "loss: 1.638805  [ 4100/10672]\n",
      "loss: 1.639660  [ 4200/10672]\n",
      "loss: 1.131874  [ 4300/10672]\n",
      "loss: 1.638077  [ 4400/10672]\n",
      "loss: 1.229889  [ 4500/10672]\n",
      "loss: 1.240079  [ 4600/10672]\n",
      "loss: 1.229172  [ 4700/10672]\n",
      "loss: 1.641378  [ 4800/10672]\n",
      "loss: 1.139497  [ 4900/10672]\n",
      "loss: 1.635054  [ 5000/10672]\n",
      "loss: 1.140409  [ 5100/10672]\n",
      "loss: 1.132520  [ 5200/10672]\n",
      "loss: 1.248591  [ 5300/10672]\n",
      "loss: 1.128261  [ 5400/10672]\n",
      "loss: 1.217811  [ 5500/10672]\n",
      "loss: 1.131541  [ 5600/10672]\n",
      "loss: 1.641924  [ 5700/10672]\n",
      "loss: 1.222952  [ 5800/10672]\n",
      "loss: 1.141445  [ 5900/10672]\n",
      "loss: 1.639139  [ 6000/10672]\n",
      "loss: 1.229449  [ 6100/10672]\n",
      "loss: 1.140301  [ 6200/10672]\n",
      "loss: 1.216852  [ 6300/10672]\n",
      "loss: 1.133510  [ 6400/10672]\n",
      "loss: 1.645427  [ 6500/10672]\n",
      "loss: 1.122463  [ 6600/10672]\n",
      "loss: 1.236751  [ 6700/10672]\n",
      "loss: 1.129595  [ 6800/10672]\n",
      "loss: 1.229488  [ 6900/10672]\n",
      "loss: 1.132856  [ 7000/10672]\n",
      "loss: 1.117017  [ 7100/10672]\n",
      "loss: 1.233428  [ 7200/10672]\n",
      "loss: 1.150130  [ 7300/10672]\n",
      "loss: 1.222782  [ 7400/10672]\n",
      "loss: 1.132065  [ 7500/10672]\n",
      "loss: 1.235947  [ 7600/10672]\n",
      "loss: 1.231015  [ 7700/10672]\n",
      "loss: 1.233407  [ 7800/10672]\n",
      "loss: 1.127586  [ 7900/10672]\n",
      "loss: 1.123254  [ 8000/10672]\n",
      "loss: 1.137944  [ 8100/10672]\n",
      "loss: 1.230807  [ 8200/10672]\n",
      "loss: 1.229483  [ 8300/10672]\n",
      "loss: 1.145885  [ 8400/10672]\n",
      "loss: 1.120394  [ 8500/10672]\n",
      "loss: 1.152773  [ 8600/10672]\n",
      "loss: 1.235308  [ 8700/10672]\n",
      "loss: 1.216721  [ 8800/10672]\n",
      "loss: 1.119379  [ 8900/10672]\n",
      "loss: 1.644225  [ 9000/10672]\n",
      "loss: 1.229090  [ 9100/10672]\n",
      "loss: 1.126208  [ 9200/10672]\n",
      "loss: 1.138803  [ 9300/10672]\n",
      "loss: 1.130775  [ 9400/10672]\n",
      "loss: 1.241400  [ 9500/10672]\n",
      "loss: 1.652276  [ 9600/10672]\n",
      "loss: 1.242793  [ 9700/10672]\n",
      "loss: 1.126927  [ 9800/10672]\n",
      "loss: 1.120726  [ 9900/10672]\n",
      "loss: 1.134415  [10000/10672]\n",
      "loss: 1.120701  [10100/10672]\n",
      "loss: 1.111282  [10200/10672]\n",
      "loss: 1.237578  [10300/10672]\n",
      "loss: 1.248612  [10400/10672]\n",
      "loss: 1.245679  [10500/10672]\n",
      "loss: 1.638964  [10600/10672]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.107649  [    0/10672]\n",
      "loss: 1.120247  [  100/10672]\n",
      "loss: 1.107807  [  200/10672]\n",
      "loss: 1.107703  [  300/10672]\n",
      "loss: 1.255187  [  400/10672]\n",
      "loss: 1.240910  [  500/10672]\n",
      "loss: 1.647102  [  600/10672]\n",
      "loss: 1.100473  [  700/10672]\n",
      "loss: 1.644157  [  800/10672]\n",
      "loss: 1.269617  [  900/10672]\n",
      "loss: 1.648903  [ 1000/10672]\n",
      "loss: 1.256148  [ 1100/10672]\n",
      "loss: 1.242519  [ 1200/10672]\n",
      "loss: 1.119027  [ 1300/10672]\n",
      "loss: 1.232635  [ 1400/10672]\n",
      "loss: 1.113546  [ 1500/10672]\n",
      "loss: 1.233489  [ 1600/10672]\n",
      "loss: 1.105446  [ 1700/10672]\n",
      "loss: 1.241230  [ 1800/10672]\n",
      "loss: 1.231616  [ 1900/10672]\n",
      "loss: 1.238065  [ 2000/10672]\n",
      "loss: 1.235174  [ 2100/10672]\n",
      "loss: 1.230567  [ 2200/10672]\n",
      "loss: 1.235403  [ 2300/10672]\n",
      "loss: 1.135988  [ 2400/10672]\n",
      "loss: 1.130886  [ 2500/10672]\n",
      "loss: 1.136550  [ 2600/10672]\n",
      "loss: 1.645452  [ 2700/10672]\n",
      "loss: 1.132233  [ 2800/10672]\n",
      "loss: 1.635550  [ 2900/10672]\n",
      "loss: 1.644439  [ 3000/10672]\n",
      "loss: 1.223761  [ 3100/10672]\n",
      "loss: 1.210497  [ 3200/10672]\n",
      "loss: 1.122526  [ 3300/10672]\n",
      "loss: 1.645588  [ 3400/10672]\n",
      "loss: 1.647755  [ 3500/10672]\n",
      "loss: 1.647832  [ 3600/10672]\n",
      "loss: 1.222908  [ 3700/10672]\n",
      "loss: 1.122614  [ 3800/10672]\n",
      "loss: 1.235899  [ 3900/10672]\n",
      "loss: 1.225970  [ 4000/10672]\n",
      "loss: 1.644230  [ 4100/10672]\n",
      "loss: 1.644756  [ 4200/10672]\n",
      "loss: 1.131338  [ 4300/10672]\n",
      "loss: 1.643538  [ 4400/10672]\n",
      "loss: 1.222581  [ 4500/10672]\n",
      "loss: 1.235772  [ 4600/10672]\n",
      "loss: 1.221437  [ 4700/10672]\n",
      "loss: 1.646433  [ 4800/10672]\n",
      "loss: 1.138542  [ 4900/10672]\n",
      "loss: 1.640779  [ 5000/10672]\n",
      "loss: 1.141208  [ 5100/10672]\n",
      "loss: 1.131179  [ 5200/10672]\n",
      "loss: 1.246833  [ 5300/10672]\n",
      "loss: 1.127383  [ 5400/10672]\n",
      "loss: 1.207889  [ 5500/10672]\n",
      "loss: 1.131406  [ 5600/10672]\n",
      "loss: 1.646837  [ 5700/10672]\n",
      "loss: 1.215187  [ 5800/10672]\n",
      "loss: 1.142152  [ 5900/10672]\n",
      "loss: 1.644174  [ 6000/10672]\n",
      "loss: 1.223713  [ 6100/10672]\n",
      "loss: 1.137600  [ 6200/10672]\n",
      "loss: 1.208661  [ 6300/10672]\n",
      "loss: 1.133267  [ 6400/10672]\n",
      "loss: 1.649856  [ 6500/10672]\n",
      "loss: 1.120661  [ 6600/10672]\n",
      "loss: 1.231613  [ 6700/10672]\n",
      "loss: 1.128719  [ 6800/10672]\n",
      "loss: 1.222987  [ 6900/10672]\n",
      "loss: 1.132663  [ 7000/10672]\n",
      "loss: 1.113587  [ 7100/10672]\n",
      "loss: 1.228278  [ 7200/10672]\n",
      "loss: 1.151174  [ 7300/10672]\n",
      "loss: 1.216931  [ 7400/10672]\n",
      "loss: 1.128270  [ 7500/10672]\n",
      "loss: 1.230968  [ 7600/10672]\n",
      "loss: 1.226445  [ 7700/10672]\n",
      "loss: 1.229388  [ 7800/10672]\n",
      "loss: 1.126712  [ 7900/10672]\n",
      "loss: 1.121458  [ 8000/10672]\n",
      "loss: 1.136634  [ 8100/10672]\n",
      "loss: 1.223981  [ 8200/10672]\n",
      "loss: 1.222738  [ 8300/10672]\n",
      "loss: 1.145256  [ 8400/10672]\n",
      "loss: 1.117502  [ 8500/10672]\n",
      "loss: 1.151384  [ 8600/10672]\n",
      "loss: 1.230362  [ 8700/10672]\n",
      "loss: 1.209848  [ 8800/10672]\n",
      "loss: 1.116754  [ 8900/10672]\n",
      "loss: 1.648405  [ 9000/10672]\n",
      "loss: 1.221391  [ 9100/10672]\n",
      "loss: 1.123922  [ 9200/10672]\n",
      "loss: 1.138682  [ 9300/10672]\n",
      "loss: 1.131211  [ 9400/10672]\n",
      "loss: 1.235489  [ 9500/10672]\n",
      "loss: 1.655504  [ 9600/10672]\n",
      "loss: 1.238317  [ 9700/10672]\n",
      "loss: 1.126701  [ 9800/10672]\n",
      "loss: 1.119465  [ 9900/10672]\n",
      "loss: 1.134568  [10000/10672]\n",
      "loss: 1.119681  [10100/10672]\n",
      "loss: 1.107845  [10200/10672]\n",
      "loss: 1.231900  [10300/10672]\n",
      "loss: 1.243956  [10400/10672]\n",
      "loss: 1.240933  [10500/10672]\n",
      "loss: 1.643714  [10600/10672]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.105300  [    0/10672]\n",
      "loss: 1.119338  [  100/10672]\n",
      "loss: 1.103196  [  200/10672]\n",
      "loss: 1.103889  [  300/10672]\n",
      "loss: 1.251801  [  400/10672]\n",
      "loss: 1.234712  [  500/10672]\n",
      "loss: 1.650750  [  600/10672]\n",
      "loss: 1.098071  [  700/10672]\n",
      "loss: 1.648124  [  800/10672]\n",
      "loss: 1.269899  [  900/10672]\n",
      "loss: 1.652302  [ 1000/10672]\n",
      "loss: 1.252947  [ 1100/10672]\n",
      "loss: 1.237276  [ 1200/10672]\n",
      "loss: 1.118586  [ 1300/10672]\n",
      "loss: 1.225007  [ 1400/10672]\n",
      "loss: 1.111315  [ 1500/10672]\n",
      "loss: 1.226788  [ 1600/10672]\n",
      "loss: 1.103237  [ 1700/10672]\n",
      "loss: 1.237587  [ 1800/10672]\n",
      "loss: 1.225212  [ 1900/10672]\n",
      "loss: 1.232989  [ 2000/10672]\n",
      "loss: 1.230074  [ 2100/10672]\n",
      "loss: 1.224876  [ 2200/10672]\n",
      "loss: 1.231864  [ 2300/10672]\n",
      "loss: 1.135278  [ 2400/10672]\n",
      "loss: 1.130116  [ 2500/10672]\n",
      "loss: 1.135338  [ 2600/10672]\n",
      "loss: 1.649044  [ 2700/10672]\n",
      "loss: 1.131282  [ 2800/10672]\n",
      "loss: 1.640283  [ 2900/10672]\n",
      "loss: 1.648296  [ 3000/10672]\n",
      "loss: 1.219980  [ 3100/10672]\n",
      "loss: 1.203413  [ 3200/10672]\n",
      "loss: 1.120837  [ 3300/10672]\n",
      "loss: 1.649351  [ 3400/10672]\n",
      "loss: 1.651127  [ 3500/10672]\n",
      "loss: 1.651128  [ 3600/10672]\n",
      "loss: 1.217434  [ 3700/10672]\n",
      "loss: 1.121284  [ 3800/10672]\n",
      "loss: 1.232319  [ 3900/10672]\n",
      "loss: 1.218735  [ 4000/10672]\n",
      "loss: 1.647982  [ 4100/10672]\n",
      "loss: 1.648276  [ 4200/10672]\n",
      "loss: 1.130963  [ 4300/10672]\n",
      "loss: 1.647333  [ 4400/10672]\n",
      "loss: 1.217163  [ 4500/10672]\n",
      "loss: 1.233365  [ 4600/10672]\n",
      "loss: 1.215582  [ 4700/10672]\n",
      "loss: 1.649941  [ 4800/10672]\n",
      "loss: 1.137922  [ 4900/10672]\n",
      "loss: 1.644812  [ 5000/10672]\n",
      "loss: 1.142059  [ 5100/10672]\n",
      "loss: 1.129953  [ 5200/10672]\n",
      "loss: 1.246893  [ 5300/10672]\n",
      "loss: 1.126666  [ 5400/10672]\n",
      "loss: 1.199766  [ 5500/10672]\n",
      "loss: 1.131311  [ 5600/10672]\n",
      "loss: 1.650260  [ 5700/10672]\n",
      "loss: 1.209208  [ 5800/10672]\n",
      "loss: 1.142918  [ 5900/10672]\n",
      "loss: 1.647717  [ 6000/10672]\n",
      "loss: 1.219736  [ 6100/10672]\n",
      "loss: 1.135242  [ 6200/10672]\n",
      "loss: 1.202204  [ 6300/10672]\n",
      "loss: 1.133223  [ 6400/10672]\n",
      "loss: 1.652944  [ 6500/10672]\n",
      "loss: 1.119035  [ 6600/10672]\n",
      "loss: 1.228108  [ 6700/10672]\n",
      "loss: 1.127850  [ 6800/10672]\n",
      "loss: 1.218123  [ 6900/10672]\n",
      "loss: 1.132511  [ 7000/10672]\n",
      "loss: 1.110275  [ 7100/10672]\n",
      "loss: 1.224747  [ 7200/10672]\n",
      "loss: 1.152468  [ 7300/10672]\n",
      "loss: 1.212706  [ 7400/10672]\n",
      "loss: 1.124726  [ 7500/10672]\n",
      "loss: 1.227559  [ 7600/10672]\n",
      "loss: 1.223462  [ 7700/10672]\n",
      "loss: 1.226921  [ 7800/10672]\n",
      "loss: 1.125963  [ 7900/10672]\n",
      "loss: 1.119879  [ 8000/10672]\n",
      "loss: 1.135607  [ 8100/10672]\n",
      "loss: 1.218634  [ 8200/10672]\n",
      "loss: 1.217491  [ 8300/10672]\n",
      "loss: 1.145019  [ 8400/10672]\n",
      "loss: 1.114930  [ 8500/10672]\n",
      "loss: 1.150579  [ 8600/10672]\n",
      "loss: 1.226877  [ 8700/10672]\n",
      "loss: 1.204443  [ 8800/10672]\n",
      "loss: 1.114321  [ 8900/10672]\n",
      "loss: 1.651347  [ 9000/10672]\n",
      "loss: 1.215127  [ 9100/10672]\n",
      "loss: 1.121813  [ 9200/10672]\n",
      "loss: 1.138888  [ 9300/10672]\n",
      "loss: 1.131834  [ 9400/10672]\n",
      "loss: 1.230924  [ 9500/10672]\n",
      "loss: 1.657773  [ 9600/10672]\n",
      "loss: 1.235195  [ 9700/10672]\n",
      "loss: 1.126669  [ 9800/10672]\n",
      "loss: 1.118422  [ 9900/10672]\n",
      "loss: 1.135089  [10000/10672]\n",
      "loss: 1.118828  [10100/10672]\n",
      "loss: 1.104658  [10200/10672]\n",
      "loss: 1.227515  [10300/10672]\n",
      "loss: 1.240540  [10400/10672]\n",
      "loss: 1.237432  [10500/10672]\n",
      "loss: 1.647166  [10600/10672]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.103155  [    0/10672]\n",
      "loss: 1.118619  [  100/10672]\n",
      "loss: 1.098863  [  200/10672]\n",
      "loss: 1.100364  [  300/10672]\n",
      "loss: 1.249626  [  400/10672]\n",
      "loss: 1.229746  [  500/10672]\n",
      "loss: 1.653358  [  600/10672]\n",
      "loss: 1.095874  [  700/10672]\n",
      "loss: 1.650988  [  800/10672]\n",
      "loss: 1.271346  [  900/10672]\n",
      "loss: 1.654722  [ 1000/10672]\n",
      "loss: 1.250962  [ 1100/10672]\n",
      "loss: 1.233230  [ 1200/10672]\n",
      "loss: 1.118340  [ 1300/10672]\n",
      "loss: 1.218592  [ 1400/10672]\n",
      "loss: 1.109311  [ 1500/10672]\n",
      "loss: 1.221270  [ 1600/10672]\n",
      "loss: 1.101207  [ 1700/10672]\n",
      "loss: 1.235126  [ 1800/10672]\n",
      "loss: 1.220001  [ 1900/10672]\n",
      "loss: 1.229106  [ 2000/10672]\n",
      "loss: 1.226135  [ 2100/10672]\n",
      "loss: 1.220342  [ 2200/10672]\n",
      "loss: 1.229499  [ 2300/10672]\n",
      "loss: 1.134762  [ 2400/10672]\n",
      "loss: 1.129464  [ 2500/10672]\n",
      "loss: 1.134376  [ 2600/10672]\n",
      "loss: 1.651644  [ 2700/10672]\n",
      "loss: 1.130446  [ 2800/10672]\n",
      "loss: 1.643800  [ 2900/10672]\n",
      "loss: 1.651102  [ 3000/10672]\n",
      "loss: 1.217363  [ 3100/10672]\n",
      "loss: 1.197453  [ 3200/10672]\n",
      "loss: 1.119201  [ 3300/10672]\n",
      "loss: 1.652105  [ 3400/10672]\n",
      "loss: 1.653577  [ 3500/10672]\n",
      "loss: 1.653497  [ 3600/10672]\n",
      "loss: 1.213065  [ 3700/10672]\n",
      "loss: 1.120108  [ 3800/10672]\n",
      "loss: 1.229793  [ 3900/10672]\n",
      "loss: 1.212593  [ 4000/10672]\n",
      "loss: 1.650725  [ 4100/10672]\n",
      "loss: 1.650846  [ 4200/10672]\n",
      "loss: 1.130765  [ 4300/10672]\n",
      "loss: 1.650114  [ 4400/10672]\n",
      "loss: 1.212776  [ 4500/10672]\n",
      "loss: 1.232005  [ 4600/10672]\n",
      "loss: 1.210775  [ 4700/10672]\n",
      "loss: 1.652514  [ 4800/10672]\n",
      "loss: 1.137584  [ 4900/10672]\n",
      "loss: 1.647803  [ 5000/10672]\n",
      "loss: 1.143021  [ 5100/10672]\n",
      "loss: 1.128872  [ 5200/10672]\n",
      "loss: 1.247946  [ 5300/10672]\n",
      "loss: 1.126121  [ 5400/10672]\n",
      "loss: 1.192665  [ 5500/10672]\n",
      "loss: 1.131311  [ 5600/10672]\n",
      "loss: 1.652776  [ 5700/10672]\n",
      "loss: 1.204240  [ 5800/10672]\n",
      "loss: 1.143794  [ 5900/10672]\n",
      "loss: 1.650342  [ 6000/10672]\n",
      "loss: 1.216755  [ 6100/10672]\n",
      "loss: 1.133172  [ 6200/10672]\n",
      "loss: 1.196728  [ 6300/10672]\n",
      "loss: 1.133373  [ 6400/10672]\n",
      "loss: 1.655218  [ 6500/10672]\n",
      "loss: 1.117586  [ 6600/10672]\n",
      "loss: 1.225515  [ 6700/10672]\n",
      "loss: 1.127053  [ 6800/10672]\n",
      "loss: 1.214188  [ 6900/10672]\n",
      "loss: 1.132452  [ 7000/10672]\n",
      "loss: 1.107107  [ 7100/10672]\n",
      "loss: 1.222138  [ 7200/10672]\n",
      "loss: 1.153981  [ 7300/10672]\n",
      "loss: 1.209405  [ 7400/10672]\n",
      "loss: 1.121408  [ 7500/10672]\n",
      "loss: 1.225044  [ 7600/10672]\n",
      "loss: 1.221385  [ 7700/10672]\n",
      "loss: 1.225332  [ 7800/10672]\n",
      "loss: 1.125351  [ 7900/10672]\n",
      "loss: 1.118497  [ 8000/10672]\n",
      "loss: 1.134814  [ 8100/10672]\n",
      "loss: 1.214136  [ 8200/10672]\n",
      "loss: 1.213113  [ 8300/10672]\n",
      "loss: 1.145088  [ 8400/10672]\n",
      "loss: 1.112621  [ 8500/10672]\n",
      "loss: 1.150204  [ 8600/10672]\n",
      "loss: 1.224235  [ 8700/10672]\n",
      "loss: 1.199882  [ 8800/10672]\n",
      "loss: 1.112067  [ 8900/10672]\n",
      "loss: 1.653521  [ 9000/10672]\n",
      "loss: 1.209712  [ 9100/10672]\n",
      "loss: 1.119870  [ 9200/10672]\n",
      "loss: 1.139355  [ 9300/10672]\n",
      "loss: 1.132628  [ 9400/10672]\n",
      "loss: 1.227143  [ 9500/10672]\n",
      "loss: 1.659456  [ 9600/10672]\n",
      "loss: 1.232861  [ 9700/10672]\n",
      "loss: 1.126811  [ 9800/10672]\n",
      "loss: 1.117568  [ 9900/10672]\n",
      "loss: 1.135895  [10000/10672]\n",
      "loss: 1.118129  [10100/10672]\n",
      "loss: 1.101683  [10200/10672]\n",
      "loss: 1.223881  [10300/10672]\n",
      "loss: 1.237839  [10400/10672]\n",
      "loss: 1.234649  [10500/10672]\n",
      "loss: 1.649791  [10600/10672]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.101187  [    0/10672]\n",
      "loss: 1.118066  [  100/10672]\n",
      "loss: 1.094761  [  200/10672]\n",
      "loss: 1.097075  [  300/10672]\n",
      "loss: 1.248156  [  400/10672]\n",
      "loss: 1.225514  [  500/10672]\n",
      "loss: 1.655310  [  600/10672]\n",
      "loss: 1.093853  [  700/10672]\n",
      "loss: 1.653150  [  800/10672]\n",
      "loss: 1.273455  [  900/10672]\n",
      "loss: 1.656526  [ 1000/10672]\n",
      "loss: 1.249703  [ 1100/10672]\n",
      "loss: 1.229889  [ 1200/10672]\n",
      "loss: 1.118261  [ 1300/10672]\n",
      "loss: 1.212911  [ 1400/10672]\n",
      "loss: 1.107498  [ 1500/10672]\n",
      "loss: 1.216456  [ 1600/10672]\n",
      "loss: 1.099334  [ 1700/10672]\n",
      "loss: 1.233366  [ 1800/10672]\n",
      "loss: 1.215510  [ 1900/10672]\n",
      "loss: 1.225942  [ 2000/10672]\n",
      "loss: 1.222887  [ 2100/10672]\n",
      "loss: 1.216497  [ 2200/10672]\n",
      "loss: 1.227835  [ 2300/10672]\n",
      "loss: 1.134411  [ 2400/10672]\n",
      "loss: 1.128925  [ 2500/10672]\n",
      "loss: 1.133617  [ 2600/10672]\n",
      "loss: 1.653610  [ 2700/10672]\n",
      "loss: 1.129722  [ 2800/10672]\n",
      "loss: 1.646520  [ 2900/10672]\n",
      "loss: 1.653234  [ 3000/10672]\n",
      "loss: 1.215447  [ 3100/10672]\n",
      "loss: 1.192170  [ 3200/10672]\n",
      "loss: 1.117636  [ 3300/10672]\n",
      "loss: 1.654212  [ 3400/10672]\n",
      "loss: 1.655436  [ 3500/10672]\n",
      "loss: 1.655275  [ 3600/10672]\n",
      "loss: 1.209368  [ 3700/10672]\n",
      "loss: 1.119068  [ 3800/10672]\n",
      "loss: 1.227901  [ 3900/10672]\n",
      "loss: 1.207134  [ 4000/10672]\n",
      "loss: 1.652814  [ 4100/10672]\n",
      "loss: 1.652800  [ 4200/10672]\n",
      "loss: 1.130712  [ 4300/10672]\n",
      "loss: 1.652237  [ 4400/10672]\n",
      "loss: 1.209015  [ 4500/10672]\n",
      "loss: 1.231280  [ 4600/10672]\n",
      "loss: 1.206614  [ 4700/10672]\n",
      "loss: 1.654480  [ 4800/10672]\n",
      "loss: 1.137464  [ 4900/10672]\n",
      "loss: 1.650109  [ 5000/10672]\n",
      "loss: 1.144084  [ 5100/10672]\n",
      "loss: 1.127914  [ 5200/10672]\n",
      "loss: 1.249598  [ 5300/10672]\n",
      "loss: 1.125716  [ 5400/10672]\n",
      "loss: 1.186205  [ 5500/10672]\n",
      "loss: 1.131400  [ 5600/10672]\n",
      "loss: 1.654704  [ 5700/10672]\n",
      "loss: 1.199903  [ 5800/10672]\n",
      "loss: 1.144767  [ 5900/10672]\n",
      "loss: 1.652366  [ 6000/10672]\n",
      "loss: 1.214394  [ 6100/10672]\n",
      "loss: 1.131320  [ 6200/10672]\n",
      "loss: 1.191869  [ 6300/10672]\n",
      "loss: 1.133678  [ 6400/10672]\n",
      "loss: 1.656965  [ 6500/10672]\n",
      "loss: 1.116282  [ 6600/10672]\n",
      "loss: 1.223486  [ 6700/10672]\n",
      "loss: 1.126329  [ 6800/10672]\n",
      "loss: 1.210833  [ 6900/10672]\n",
      "loss: 1.132480  [ 7000/10672]\n",
      "loss: 1.104064  [ 7100/10672]\n",
      "loss: 1.220104  [ 7200/10672]\n",
      "loss: 1.155664  [ 7300/10672]\n",
      "loss: 1.206683  [ 7400/10672]\n",
      "loss: 1.118270  [ 7500/10672]\n",
      "loss: 1.223090  [ 7600/10672]\n",
      "loss: 1.219875  [ 7700/10672]\n",
      "loss: 1.224287  [ 7800/10672]\n",
      "loss: 1.124853  [ 7900/10672]\n",
      "loss: 1.117270  [ 8000/10672]\n",
      "loss: 1.134199  [ 8100/10672]\n",
      "loss: 1.210172  [ 8200/10672]\n",
      "loss: 1.209286  [ 8300/10672]\n",
      "loss: 1.145387  [ 8400/10672]\n",
      "loss: 1.110513  [ 8500/10672]\n",
      "loss: 1.150147  [ 8600/10672]\n",
      "loss: 1.222127  [ 8700/10672]\n",
      "loss: 1.195859  [ 8800/10672]\n",
      "loss: 1.109955  [ 8900/10672]\n",
      "loss: 1.655188  [ 9000/10672]\n",
      "loss: 1.204844  [ 9100/10672]\n",
      "loss: 1.118062  [ 9200/10672]\n",
      "loss: 1.140017  [ 9300/10672]\n",
      "loss: 1.133554  [ 9400/10672]\n",
      "loss: 1.223864  [ 9500/10672]\n",
      "loss: 1.660758  [ 9600/10672]\n",
      "loss: 1.231026  [ 9700/10672]\n",
      "loss: 1.127087  [ 9800/10672]\n",
      "loss: 1.116860  [ 9900/10672]\n",
      "loss: 1.136910  [10000/10672]\n",
      "loss: 1.117549  [10100/10672]\n",
      "loss: 1.098872  [10200/10672]\n",
      "loss: 1.220728  [10300/10672]\n",
      "loss: 1.235589  [10400/10672]\n",
      "loss: 1.232321  [10500/10672]\n",
      "loss: 1.651854  [10600/10672]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.099361  [    0/10672]\n",
      "loss: 1.117638  [  100/10672]\n",
      "loss: 1.090839  [  200/10672]\n",
      "loss: 1.093964  [  300/10672]\n",
      "loss: 1.247134  [  400/10672]\n",
      "loss: 1.221758  [  500/10672]\n",
      "loss: 1.656821  [  600/10672]\n",
      "loss: 1.091966  [  700/10672]\n",
      "loss: 1.654836  [  800/10672]\n",
      "loss: 1.275974  [  900/10672]\n",
      "loss: 1.657918  [ 1000/10672]\n",
      "loss: 1.248916  [ 1100/10672]\n",
      "loss: 1.227004  [ 1200/10672]\n",
      "loss: 1.118307  [ 1300/10672]\n",
      "loss: 1.207717  [ 1400/10672]\n",
      "loss: 1.105829  [ 1500/10672]\n",
      "loss: 1.212103  [ 1600/10672]\n",
      "loss: 1.097582  [ 1700/10672]\n",
      "loss: 1.232060  [ 1800/10672]\n",
      "loss: 1.211493  [ 1900/10672]\n",
      "loss: 1.223247  [ 2000/10672]\n",
      "loss: 1.220086  [ 2100/10672]\n",
      "loss: 1.213098  [ 2200/10672]\n",
      "loss: 1.226626  [ 2300/10672]\n",
      "loss: 1.134183  [ 2400/10672]\n",
      "loss: 1.128475  [ 2500/10672]\n",
      "loss: 1.133010  [ 2600/10672]\n",
      "loss: 1.655144  [ 2700/10672]\n",
      "loss: 1.129085  [ 2800/10672]\n",
      "loss: 1.648689  [ 2900/10672]\n",
      "loss: 1.654908  [ 3000/10672]\n",
      "loss: 1.213990  [ 3100/10672]\n",
      "loss: 1.187332  [ 3200/10672]\n",
      "loss: 1.116132  [ 3300/10672]\n",
      "loss: 1.655879  [ 3400/10672]\n",
      "loss: 1.656895  [ 3500/10672]\n",
      "loss: 1.656656  [ 3600/10672]\n",
      "loss: 1.206115  [ 3700/10672]\n",
      "loss: 1.118131  [ 3800/10672]\n",
      "loss: 1.226423  [ 3900/10672]\n",
      "loss: 1.202135  [ 4000/10672]\n",
      "loss: 1.654456  [ 4100/10672]\n",
      "loss: 1.654334  [ 4200/10672]\n",
      "loss: 1.130768  [ 4300/10672]\n",
      "loss: 1.653908  [ 4400/10672]\n",
      "loss: 1.205665  [ 4500/10672]\n",
      "loss: 1.230974  [ 4600/10672]\n",
      "loss: 1.202886  [ 4700/10672]\n",
      "loss: 1.656032  [ 4800/10672]\n",
      "loss: 1.137504  [ 4900/10672]\n",
      "loss: 1.651940  [ 5000/10672]\n",
      "loss: 1.145226  [ 5100/10672]\n",
      "loss: 1.127050  [ 5200/10672]\n",
      "loss: 1.251640  [ 5300/10672]\n",
      "loss: 1.125417  [ 5400/10672]\n",
      "loss: 1.180182  [ 5500/10672]\n",
      "loss: 1.131559  [ 5600/10672]\n",
      "loss: 1.656229  [ 5700/10672]\n",
      "loss: 1.195989  [ 5800/10672]\n",
      "loss: 1.145816  [ 5900/10672]\n",
      "loss: 1.653972  [ 6000/10672]\n",
      "loss: 1.212450  [ 6100/10672]\n",
      "loss: 1.129634  [ 6200/10672]\n",
      "loss: 1.187427  [ 6300/10672]\n",
      "loss: 1.134098  [ 6400/10672]\n",
      "loss: 1.658352  [ 6500/10672]\n",
      "loss: 1.115087  [ 6600/10672]\n",
      "loss: 1.221832  [ 6700/10672]\n",
      "loss: 1.125663  [ 6800/10672]\n",
      "loss: 1.207866  [ 6900/10672]\n",
      "loss: 1.132575  [ 7000/10672]\n",
      "loss: 1.101123  [ 7100/10672]\n",
      "loss: 1.218458  [ 7200/10672]\n",
      "loss: 1.157472  [ 7300/10672]\n",
      "loss: 1.204351  [ 7400/10672]\n",
      "loss: 1.115272  [ 7500/10672]\n",
      "loss: 1.221512  [ 7600/10672]\n",
      "loss: 1.218747  [ 7700/10672]\n",
      "loss: 1.223604  [ 7800/10672]\n",
      "loss: 1.124439  [ 7900/10672]\n",
      "loss: 1.116159  [ 8000/10672]\n",
      "loss: 1.133716  [ 8100/10672]\n",
      "loss: 1.206572  [ 8200/10672]\n",
      "loss: 1.205837  [ 8300/10672]\n",
      "loss: 1.145853  [ 8400/10672]\n",
      "loss: 1.108556  [ 8500/10672]\n",
      "loss: 1.150321  [ 8600/10672]\n",
      "loss: 1.220383  [ 8700/10672]\n",
      "loss: 1.192203  [ 8800/10672]\n",
      "loss: 1.107953  [ 8900/10672]\n",
      "loss: 1.656503  [ 9000/10672]\n",
      "loss: 1.200357  [ 9100/10672]\n",
      "loss: 1.116355  [ 9200/10672]\n",
      "loss: 1.140823  [ 9300/10672]\n",
      "loss: 1.134577  [ 9400/10672]\n",
      "loss: 1.220928  [ 9500/10672]\n",
      "loss: 1.661797  [ 9600/10672]\n",
      "loss: 1.229531  [ 9700/10672]\n",
      "loss: 1.127464  [ 9800/10672]\n",
      "loss: 1.116260  [ 9900/10672]\n",
      "loss: 1.138079  [10000/10672]\n",
      "loss: 1.117059  [10100/10672]\n",
      "loss: 1.096186  [10200/10672]\n",
      "loss: 1.217904  [10300/10672]\n",
      "loss: 1.233648  [10400/10672]\n",
      "loss: 1.230302  [10500/10672]\n",
      "loss: 1.653522  [10600/10672]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.097641  [    0/10672]\n",
      "loss: 1.117300  [  100/10672]\n",
      "loss: 1.087058  [  200/10672]\n",
      "loss: 1.090991  [  300/10672]\n",
      "loss: 1.246417  [  400/10672]\n",
      "loss: 1.218335  [  500/10672]\n",
      "loss: 1.658025  [  600/10672]\n",
      "loss: 1.090179  [  700/10672]\n",
      "loss: 1.656188  [  800/10672]\n",
      "loss: 1.278766  [  900/10672]\n",
      "loss: 1.659023  [ 1000/10672]\n",
      "loss: 1.248454  [ 1100/10672]\n",
      "loss: 1.224433  [ 1200/10672]\n",
      "loss: 1.118443  [ 1300/10672]\n",
      "loss: 1.202868  [ 1400/10672]\n",
      "loss: 1.104268  [ 1500/10672]\n",
      "loss: 1.208072  [ 1600/10672]\n",
      "loss: 1.095921  [ 1700/10672]\n",
      "loss: 1.231068  [ 1800/10672]\n",
      "loss: 1.207809  [ 1900/10672]\n",
      "loss: 1.220881  [ 2000/10672]\n",
      "loss: 1.217598  [ 2100/10672]\n",
      "loss: 1.210011  [ 2200/10672]\n",
      "loss: 1.225731  [ 2300/10672]\n",
      "loss: 1.134046  [ 2400/10672]\n",
      "loss: 1.128091  [ 2500/10672]\n",
      "loss: 1.132516  [ 2600/10672]\n",
      "loss: 1.656375  [ 2700/10672]\n",
      "loss: 1.128513  [ 2800/10672]\n",
      "loss: 1.650461  [ 2900/10672]\n",
      "loss: 1.656257  [ 3000/10672]\n",
      "loss: 1.212854  [ 3100/10672]\n",
      "loss: 1.182809  [ 3200/10672]\n",
      "loss: 1.114675  [ 3300/10672]\n",
      "loss: 1.657235  [ 3400/10672]\n",
      "loss: 1.658071  [ 3500/10672]\n",
      "loss: 1.657759  [ 3600/10672]\n",
      "loss: 1.203174  [ 3700/10672]\n",
      "loss: 1.117271  [ 3800/10672]\n",
      "loss: 1.225231  [ 3900/10672]\n",
      "loss: 1.197466  [ 4000/10672]\n",
      "loss: 1.655780  [ 4100/10672]\n",
      "loss: 1.655570  [ 4200/10672]\n",
      "loss: 1.130903  [ 4300/10672]\n",
      "loss: 1.655257  [ 4400/10672]\n",
      "loss: 1.202605  [ 4500/10672]\n",
      "loss: 1.230958  [ 4600/10672]\n",
      "loss: 1.199466  [ 4700/10672]\n",
      "loss: 1.657289  [ 4800/10672]\n",
      "loss: 1.137663  [ 4900/10672]\n",
      "loss: 1.653429  [ 5000/10672]\n",
      "loss: 1.146427  [ 5100/10672]\n",
      "loss: 1.126253  [ 5200/10672]\n",
      "loss: 1.253953  [ 5300/10672]\n",
      "loss: 1.125196  [ 5400/10672]\n",
      "loss: 1.174479  [ 5500/10672]\n",
      "loss: 1.131766  [ 5600/10672]\n",
      "loss: 1.657465  [ 5700/10672]\n",
      "loss: 1.192378  [ 5800/10672]\n",
      "loss: 1.146920  [ 5900/10672]\n",
      "loss: 1.655280  [ 6000/10672]\n",
      "loss: 1.210801  [ 6100/10672]\n",
      "loss: 1.128071  [ 6200/10672]\n",
      "loss: 1.183288  [ 6300/10672]\n",
      "loss: 1.134604  [ 6400/10672]\n",
      "loss: 1.659483  [ 6500/10672]\n",
      "loss: 1.113976  [ 6600/10672]\n",
      "loss: 1.220442  [ 6700/10672]\n",
      "loss: 1.125041  [ 6800/10672]\n",
      "loss: 1.205176  [ 6900/10672]\n",
      "loss: 1.132721  [ 7000/10672]\n",
      "loss: 1.098262  [ 7100/10672]\n",
      "loss: 1.217088  [ 7200/10672]\n",
      "loss: 1.159370  [ 7300/10672]\n",
      "loss: 1.202297  [ 7400/10672]\n",
      "loss: 1.112382  [ 7500/10672]\n",
      "loss: 1.220201  [ 7600/10672]\n",
      "loss: 1.217889  [ 7700/10672]\n",
      "loss: 1.223176  [ 7800/10672]\n",
      "loss: 1.124088  [ 7900/10672]\n",
      "loss: 1.115135  [ 8000/10672]\n",
      "loss: 1.133328  [ 8100/10672]\n",
      "loss: 1.203234  [ 8200/10672]\n",
      "loss: 1.202660  [ 8300/10672]\n",
      "loss: 1.146444  [ 8400/10672]\n",
      "loss: 1.106713  [ 8500/10672]\n",
      "loss: 1.150670  [ 8600/10672]\n",
      "loss: 1.218901  [ 8700/10672]\n",
      "loss: 1.188814  [ 8800/10672]\n",
      "loss: 1.106034  [ 8900/10672]\n",
      "loss: 1.657565  [ 9000/10672]\n",
      "loss: 1.196147  [ 9100/10672]\n",
      "loss: 1.114725  [ 9200/10672]\n",
      "loss: 1.141733  [ 9300/10672]\n",
      "loss: 1.135671  [ 9400/10672]\n",
      "loss: 1.218242  [ 9500/10672]\n",
      "loss: 1.662650  [ 9600/10672]\n",
      "loss: 1.228281  [ 9700/10672]\n",
      "loss: 1.127914  [ 9800/10672]\n",
      "loss: 1.115741  [ 9900/10672]\n",
      "loss: 1.139361  [10000/10672]\n",
      "loss: 1.116632  [10100/10672]\n",
      "loss: 1.093597  [10200/10672]\n",
      "loss: 1.215318  [10300/10672]\n",
      "loss: 1.231927  [10400/10672]\n",
      "loss: 1.228503  [10500/10672]\n",
      "loss: 1.654899  [10600/10672]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.096004  [    0/10672]\n",
      "loss: 1.117029  [  100/10672]\n",
      "loss: 1.083389  [  200/10672]\n",
      "loss: 1.088123  [  300/10672]\n",
      "loss: 1.245917  [  400/10672]\n",
      "loss: 1.215154  [  500/10672]\n",
      "loss: 1.659005  [  600/10672]\n",
      "loss: 1.088467  [  700/10672]\n",
      "loss: 1.657294  [  800/10672]\n",
      "loss: 1.281744  [  900/10672]\n",
      "loss: 1.659920  [ 1000/10672]\n",
      "loss: 1.248227  [ 1100/10672]\n",
      "loss: 1.222088  [ 1200/10672]\n",
      "loss: 1.118646  [ 1300/10672]\n",
      "loss: 1.198275  [ 1400/10672]\n",
      "loss: 1.102790  [ 1500/10672]\n",
      "loss: 1.204277  [ 1600/10672]\n",
      "loss: 1.094329  [ 1700/10672]\n",
      "loss: 1.230302  [ 1800/10672]\n",
      "loss: 1.204372  [ 1900/10672]\n",
      "loss: 1.218753  [ 2000/10672]\n",
      "loss: 1.215338  [ 2100/10672]\n",
      "loss: 1.207152  [ 2200/10672]\n",
      "loss: 1.225064  [ 2300/10672]\n",
      "loss: 1.133976  [ 2400/10672]\n",
      "loss: 1.127756  [ 2500/10672]\n",
      "loss: 1.132107  [ 2600/10672]\n",
      "loss: 1.657382  [ 2700/10672]\n",
      "loss: 1.127990  [ 2800/10672]\n",
      "loss: 1.651939  [ 2900/10672]\n",
      "loss: 1.657368  [ 3000/10672]\n",
      "loss: 1.211950  [ 3100/10672]\n",
      "loss: 1.178522  [ 3200/10672]\n",
      "loss: 1.113256  [ 3300/10672]\n",
      "loss: 1.658363  [ 3400/10672]\n",
      "loss: 1.659040  [ 3500/10672]\n",
      "loss: 1.658660  [ 3600/10672]\n",
      "loss: 1.200461  [ 3700/10672]\n",
      "loss: 1.116470  [ 3800/10672]\n",
      "loss: 1.224246  [ 3900/10672]\n",
      "loss: 1.193045  [ 4000/10672]\n",
      "loss: 1.656869  [ 4100/10672]\n",
      "loss: 1.656587  [ 4200/10672]\n",
      "loss: 1.131096  [ 4300/10672]\n",
      "loss: 1.656367  [ 4400/10672]\n",
      "loss: 1.199755  [ 4500/10672]\n",
      "loss: 1.231154  [ 4600/10672]\n",
      "loss: 1.196274  [ 4700/10672]\n",
      "loss: 1.658328  [ 4800/10672]\n",
      "loss: 1.137912  [ 4900/10672]\n",
      "loss: 1.654664  [ 5000/10672]\n",
      "loss: 1.147672  [ 5100/10672]\n",
      "loss: 1.125508  [ 5200/10672]\n",
      "loss: 1.256457  [ 5300/10672]\n",
      "loss: 1.125032  [ 5400/10672]\n",
      "loss: 1.169017  [ 5500/10672]\n",
      "loss: 1.132009  [ 5600/10672]\n",
      "loss: 1.658490  [ 5700/10672]\n",
      "loss: 1.188991  [ 5800/10672]\n",
      "loss: 1.148065  [ 5900/10672]\n",
      "loss: 1.656365  [ 6000/10672]\n",
      "loss: 1.209371  [ 6100/10672]\n",
      "loss: 1.126604  [ 6200/10672]\n",
      "loss: 1.179379  [ 6300/10672]\n",
      "loss: 1.135174  [ 6400/10672]\n",
      "loss: 1.660427  [ 6500/10672]\n",
      "loss: 1.112929  [ 6600/10672]\n",
      "loss: 1.219248  [ 6700/10672]\n",
      "loss: 1.124453  [ 6800/10672]\n",
      "loss: 1.202689  [ 6900/10672]\n",
      "loss: 1.132906  [ 7000/10672]\n",
      "loss: 1.095469  [ 7100/10672]\n",
      "loss: 1.215919  [ 7200/10672]\n",
      "loss: 1.161336  [ 7300/10672]\n",
      "loss: 1.200450  [ 7400/10672]\n",
      "loss: 1.109580  [ 7500/10672]\n",
      "loss: 1.219086  [ 7600/10672]\n",
      "loss: 1.217229  [ 7700/10672]\n",
      "loss: 1.222932  [ 7800/10672]\n",
      "loss: 1.123784  [ 7900/10672]\n",
      "loss: 1.114178  [ 8000/10672]\n",
      "loss: 1.133013  [ 8100/10672]\n",
      "loss: 1.200092  [ 8200/10672]\n",
      "loss: 1.199687  [ 8300/10672]\n",
      "loss: 1.147131  [ 8400/10672]\n",
      "loss: 1.104961  [ 8500/10672]\n",
      "loss: 1.151153  [ 8600/10672]\n",
      "loss: 1.217613  [ 8700/10672]\n",
      "loss: 1.185624  [ 8800/10672]\n",
      "loss: 1.104181  [ 8900/10672]\n",
      "loss: 1.658440  [ 9000/10672]\n",
      "loss: 1.192146  [ 9100/10672]\n",
      "loss: 1.113156  [ 9200/10672]\n",
      "loss: 1.142725  [ 9300/10672]\n",
      "loss: 1.136818  [ 9400/10672]\n",
      "loss: 1.215742  [ 9500/10672]\n",
      "loss: 1.663366  [ 9600/10672]\n",
      "loss: 1.227208  [ 9700/10672]\n",
      "loss: 1.128420  [ 9800/10672]\n",
      "loss: 1.115283  [ 9900/10672]\n",
      "loss: 1.140728  [10000/10672]\n",
      "loss: 1.116254  [10100/10672]\n",
      "loss: 1.091084  [10200/10672]\n",
      "loss: 1.212910  [10300/10672]\n",
      "loss: 1.230370  [10400/10672]\n",
      "loss: 1.226869  [10500/10672]\n",
      "loss: 1.656057  [10600/10672]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.094434  [    0/10672]\n",
      "loss: 1.116807  [  100/10672]\n",
      "loss: 1.079812  [  200/10672]\n",
      "loss: 1.085340  [  300/10672]\n",
      "loss: 1.245575  [  400/10672]\n",
      "loss: 1.212157  [  500/10672]\n",
      "loss: 1.659818  [  600/10672]\n",
      "loss: 1.086814  [  700/10672]\n",
      "loss: 1.658216  [  800/10672]\n",
      "loss: 1.284856  [  900/10672]\n",
      "loss: 1.660662  [ 1000/10672]\n",
      "loss: 1.248173  [ 1100/10672]\n",
      "loss: 1.219912  [ 1200/10672]\n",
      "loss: 1.118899  [ 1300/10672]\n",
      "loss: 1.193878  [ 1400/10672]\n",
      "loss: 1.101377  [ 1500/10672]\n",
      "loss: 1.200660  [ 1600/10672]\n",
      "loss: 1.092791  [ 1700/10672]\n",
      "loss: 1.229706  [ 1800/10672]\n",
      "loss: 1.201121  [ 1900/10672]\n",
      "loss: 1.216805  [ 2000/10672]\n",
      "loss: 1.213250  [ 2100/10672]\n",
      "loss: 1.204464  [ 2200/10672]\n",
      "loss: 1.224566  [ 2300/10672]\n",
      "loss: 1.133957  [ 2400/10672]\n",
      "loss: 1.127457  [ 2500/10672]\n",
      "loss: 1.131765  [ 2600/10672]\n",
      "loss: 1.658221  [ 2700/10672]\n",
      "loss: 1.127504  [ 2800/10672]\n",
      "loss: 1.653191  [ 2900/10672]\n",
      "loss: 1.658299  [ 3000/10672]\n",
      "loss: 1.211221  [ 3100/10672]\n",
      "loss: 1.174415  [ 3200/10672]\n",
      "loss: 1.111865  [ 3300/10672]\n",
      "loss: 1.659319  [ 3400/10672]\n",
      "loss: 1.659854  [ 3500/10672]\n",
      "loss: 1.659410  [ 3600/10672]\n",
      "loss: 1.197921  [ 3700/10672]\n",
      "loss: 1.115713  [ 3800/10672]\n",
      "loss: 1.223416  [ 3900/10672]\n",
      "loss: 1.188815  [ 4000/10672]\n",
      "loss: 1.657781  [ 4100/10672]\n",
      "loss: 1.657439  [ 4200/10672]\n",
      "loss: 1.131331  [ 4300/10672]\n",
      "loss: 1.657297  [ 4400/10672]\n",
      "loss: 1.197067  [ 4500/10672]\n",
      "loss: 1.231508  [ 4600/10672]\n",
      "loss: 1.193258  [ 4700/10672]\n",
      "loss: 1.659204  [ 4800/10672]\n",
      "loss: 1.138230  [ 4900/10672]\n",
      "loss: 1.655705  [ 5000/10672]\n",
      "loss: 1.148950  [ 5100/10672]\n",
      "loss: 1.124801  [ 5200/10672]\n",
      "loss: 1.259103  [ 5300/10672]\n",
      "loss: 1.124910  [ 5400/10672]\n",
      "loss: 1.163748  [ 5500/10672]\n",
      "loss: 1.132276  [ 5600/10672]\n",
      "loss: 1.659354  [ 5700/10672]\n",
      "loss: 1.185777  [ 5800/10672]\n",
      "loss: 1.149238  [ 5900/10672]\n",
      "loss: 1.657280  [ 6000/10672]\n",
      "loss: 1.208108  [ 6100/10672]\n",
      "loss: 1.125211  [ 6200/10672]\n",
      "loss: 1.175648  [ 6300/10672]\n",
      "loss: 1.135794  [ 6400/10672]\n",
      "loss: 1.661229  [ 6500/10672]\n",
      "loss: 1.111930  [ 6600/10672]\n",
      "loss: 1.218202  [ 6700/10672]\n",
      "loss: 1.123888  [ 6800/10672]\n",
      "loss: 1.200358  [ 6900/10672]\n",
      "loss: 1.133119  [ 7000/10672]\n",
      "loss: 1.092729  [ 7100/10672]\n",
      "loss: 1.214904  [ 7200/10672]\n",
      "loss: 1.163350  [ 7300/10672]\n",
      "loss: 1.198761  [ 7400/10672]\n",
      "loss: 1.106847  [ 7500/10672]\n",
      "loss: 1.218120  [ 7600/10672]\n",
      "loss: 1.216719  [ 7700/10672]\n",
      "loss: 1.222826  [ 7800/10672]\n",
      "loss: 1.123513  [ 7900/10672]\n",
      "loss: 1.113271  [ 8000/10672]\n",
      "loss: 1.132750  [ 8100/10672]\n",
      "loss: 1.197105  [ 8200/10672]\n",
      "loss: 1.196874  [ 8300/10672]\n",
      "loss: 1.147888  [ 8400/10672]\n",
      "loss: 1.103279  [ 8500/10672]\n",
      "loss: 1.151738  [ 8600/10672]\n",
      "loss: 1.216479  [ 8700/10672]\n",
      "loss: 1.182594  [ 8800/10672]\n",
      "loss: 1.102378  [ 8900/10672]\n",
      "loss: 1.659172  [ 9000/10672]\n",
      "loss: 1.188310  [ 9100/10672]\n",
      "loss: 1.111633  [ 9200/10672]\n",
      "loss: 1.143775  [ 9300/10672]\n",
      "loss: 1.138003  [ 9400/10672]\n",
      "loss: 1.213388  [ 9500/10672]\n",
      "loss: 1.663978  [ 9600/10672]\n",
      "loss: 1.226273  [ 9700/10672]\n",
      "loss: 1.128967  [ 9800/10672]\n",
      "loss: 1.114873  [ 9900/10672]\n",
      "loss: 1.142159  [10000/10672]\n",
      "loss: 1.115910  [10100/10672]\n",
      "loss: 1.088632  [10200/10672]\n",
      "loss: 1.210642  [10300/10672]\n",
      "loss: 1.228941  [10400/10672]\n",
      "loss: 1.225362  [10500/10672]\n",
      "loss: 1.657046  [10600/10672]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.092915  [    0/10672]\n",
      "loss: 1.116618  [  100/10672]\n",
      "loss: 1.076311  [  200/10672]\n",
      "loss: 1.082626  [  300/10672]\n",
      "loss: 1.245356  [  400/10672]\n",
      "loss: 1.209305  [  500/10672]\n",
      "loss: 1.660502  [  600/10672]\n",
      "loss: 1.085206  [  700/10672]\n",
      "loss: 1.658994  [  800/10672]\n",
      "loss: 1.288064  [  900/10672]\n",
      "loss: 1.661285  [ 1000/10672]\n",
      "loss: 1.248253  [ 1100/10672]\n",
      "loss: 1.217867  [ 1200/10672]\n",
      "loss: 1.119187  [ 1300/10672]\n",
      "loss: 1.189640  [ 1400/10672]\n",
      "loss: 1.100014  [ 1500/10672]\n",
      "loss: 1.197186  [ 1600/10672]\n",
      "loss: 1.091294  [ 1700/10672]\n",
      "loss: 1.229240  [ 1800/10672]\n",
      "loss: 1.198020  [ 1900/10672]\n",
      "loss: 1.214997  [ 2000/10672]\n",
      "loss: 1.211298  [ 2100/10672]\n",
      "loss: 1.201914  [ 2200/10672]\n",
      "loss: 1.224200  [ 2300/10672]\n",
      "loss: 1.133975  [ 2400/10672]\n",
      "loss: 1.127184  [ 2500/10672]\n",
      "loss: 1.131473  [ 2600/10672]\n",
      "loss: 1.658930  [ 2700/10672]\n",
      "loss: 1.127043  [ 2800/10672]\n",
      "loss: 1.654267  [ 2900/10672]\n",
      "loss: 1.659092  [ 3000/10672]\n",
      "loss: 1.210630  [ 3100/10672]\n",
      "loss: 1.170457  [ 3200/10672]\n",
      "loss: 1.110496  [ 3300/10672]\n",
      "loss: 1.660143  [ 3400/10672]\n",
      "loss: 1.660547  [ 3500/10672]\n",
      "loss: 1.660046  [ 3600/10672]\n",
      "loss: 1.195520  [ 3700/10672]\n",
      "loss: 1.114990  [ 3800/10672]\n",
      "loss: 1.222706  [ 3900/10672]\n",
      "loss: 1.184741  [ 4000/10672]\n",
      "loss: 1.658555  [ 4100/10672]\n",
      "loss: 1.658164  [ 4200/10672]\n",
      "loss: 1.131596  [ 4300/10672]\n",
      "loss: 1.658086  [ 4400/10672]\n",
      "loss: 1.194506  [ 4500/10672]\n",
      "loss: 1.231985  [ 4600/10672]\n",
      "loss: 1.190386  [ 4700/10672]\n",
      "loss: 1.659951  [ 4800/10672]\n",
      "loss: 1.138597  [ 4900/10672]\n",
      "loss: 1.656594  [ 5000/10672]\n",
      "loss: 1.150249  [ 5100/10672]\n",
      "loss: 1.124119  [ 5200/10672]\n",
      "loss: 1.261858  [ 5300/10672]\n",
      "loss: 1.124819  [ 5400/10672]\n",
      "loss: 1.158639  [ 5500/10672]\n",
      "loss: 1.132558  [ 5600/10672]\n",
      "loss: 1.660094  [ 5700/10672]\n",
      "loss: 1.182702  [ 5800/10672]\n",
      "loss: 1.150431  [ 5900/10672]\n",
      "loss: 1.658063  [ 6000/10672]\n",
      "loss: 1.206978  [ 6100/10672]\n",
      "loss: 1.123876  [ 6200/10672]\n",
      "loss: 1.172065  [ 6300/10672]\n",
      "loss: 1.136448  [ 6400/10672]\n",
      "loss: 1.661922  [ 6500/10672]\n",
      "loss: 1.110970  [ 6600/10672]\n",
      "loss: 1.217273  [ 6700/10672]\n",
      "loss: 1.123340  [ 6800/10672]\n",
      "loss: 1.198150  [ 6900/10672]\n",
      "loss: 1.133354  [ 7000/10672]\n",
      "loss: 1.090036  [ 7100/10672]\n",
      "loss: 1.214011  [ 7200/10672]\n",
      "loss: 1.165398  [ 7300/10672]\n",
      "loss: 1.197198  [ 7400/10672]\n",
      "loss: 1.104174  [ 7500/10672]\n",
      "loss: 1.217272  [ 7600/10672]\n",
      "loss: 1.216327  [ 7700/10672]\n",
      "loss: 1.222828  [ 7800/10672]\n",
      "loss: 1.123267  [ 7900/10672]\n",
      "loss: 1.112405  [ 8000/10672]\n",
      "loss: 1.132528  [ 8100/10672]\n",
      "loss: 1.194241  [ 8200/10672]\n",
      "loss: 1.194189  [ 8300/10672]\n",
      "loss: 1.148701  [ 8400/10672]\n",
      "loss: 1.101654  [ 8500/10672]\n",
      "loss: 1.152404  [ 8600/10672]\n",
      "loss: 1.215465  [ 8700/10672]\n",
      "loss: 1.179692  [ 8800/10672]\n",
      "loss: 1.100617  [ 8900/10672]\n",
      "loss: 1.659793  [ 9000/10672]\n",
      "loss: 1.184606  [ 9100/10672]\n",
      "loss: 1.110147  [ 9200/10672]\n",
      "loss: 1.144870  [ 9300/10672]\n",
      "loss: 1.139214  [ 9400/10672]\n",
      "loss: 1.211151  [ 9500/10672]\n",
      "loss: 1.664511  [ 9600/10672]\n",
      "loss: 1.225445  [ 9700/10672]\n",
      "loss: 1.129545  [ 9800/10672]\n",
      "loss: 1.114497  [ 9900/10672]\n",
      "loss: 1.143637  [10000/10672]\n",
      "loss: 1.115592  [10100/10672]\n",
      "loss: 1.086232  [10200/10672]\n",
      "loss: 1.208486  [10300/10672]\n",
      "loss: 1.227613  [10400/10672]\n",
      "loss: 1.223955  [10500/10672]\n",
      "loss: 1.657902  [10600/10672]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.091439  [    0/10672]\n",
      "loss: 1.116455  [  100/10672]\n",
      "loss: 1.072876  [  200/10672]\n",
      "loss: 1.079968  [  300/10672]\n",
      "loss: 1.245232  [  400/10672]\n",
      "loss: 1.206571  [  500/10672]\n",
      "loss: 1.661085  [  600/10672]\n",
      "loss: 1.083634  [  700/10672]\n",
      "loss: 1.659661  [  800/10672]\n",
      "loss: 1.291344  [  900/10672]\n",
      "loss: 1.661814  [ 1000/10672]\n",
      "loss: 1.248437  [ 1100/10672]\n",
      "loss: 1.215927  [ 1200/10672]\n",
      "loss: 1.119501  [ 1300/10672]\n",
      "loss: 1.185531  [ 1400/10672]\n",
      "loss: 1.098691  [ 1500/10672]\n",
      "loss: 1.193826  [ 1600/10672]\n",
      "loss: 1.089831  [ 1700/10672]\n",
      "loss: 1.228878  [ 1800/10672]\n",
      "loss: 1.195039  [ 1900/10672]\n",
      "loss: 1.213300  [ 2000/10672]\n",
      "loss: 1.209456  [ 2100/10672]\n",
      "loss: 1.199472  [ 2200/10672]\n",
      "loss: 1.223936  [ 2300/10672]\n",
      "loss: 1.134020  [ 2400/10672]\n",
      "loss: 1.126931  [ 2500/10672]\n",
      "loss: 1.131222  [ 2600/10672]\n",
      "loss: 1.659536  [ 2700/10672]\n",
      "loss: 1.126603  [ 2800/10672]\n",
      "loss: 1.655204  [ 2900/10672]\n",
      "loss: 1.659775  [ 3000/10672]\n",
      "loss: 1.210148  [ 3100/10672]\n",
      "loss: 1.166620  [ 3200/10672]\n",
      "loss: 1.109145  [ 3300/10672]\n",
      "loss: 1.660863  [ 3400/10672]\n",
      "loss: 1.661146  [ 3500/10672]\n",
      "loss: 1.660592  [ 3600/10672]\n",
      "loss: 1.193228  [ 3700/10672]\n",
      "loss: 1.114293  [ 3800/10672]\n",
      "loss: 1.222088  [ 3900/10672]\n",
      "loss: 1.180791  [ 4000/10672]\n",
      "loss: 1.659220  [ 4100/10672]\n",
      "loss: 1.658788  [ 4200/10672]\n",
      "loss: 1.131884  [ 4300/10672]\n",
      "loss: 1.658764  [ 4400/10672]\n",
      "loss: 1.192047  [ 4500/10672]\n",
      "loss: 1.232556  [ 4600/10672]\n",
      "loss: 1.187627  [ 4700/10672]\n",
      "loss: 1.660599  [ 4800/10672]\n",
      "loss: 1.139007  [ 4900/10672]\n",
      "loss: 1.657362  [ 5000/10672]\n",
      "loss: 1.151566  [ 5100/10672]\n",
      "loss: 1.123458  [ 5200/10672]\n",
      "loss: 1.264693  [ 5300/10672]\n",
      "loss: 1.124752  [ 5400/10672]\n",
      "loss: 1.153663  [ 5500/10672]\n",
      "loss: 1.132852  [ 5600/10672]\n",
      "loss: 1.660736  [ 5700/10672]\n",
      "loss: 1.179737  [ 5800/10672]\n",
      "loss: 1.151637  [ 5900/10672]\n",
      "loss: 1.658740  [ 6000/10672]\n",
      "loss: 1.205953  [ 6100/10672]\n",
      "loss: 1.122589  [ 6200/10672]\n",
      "loss: 1.168603  [ 6300/10672]\n",
      "loss: 1.137133  [ 6400/10672]\n",
      "loss: 1.662528  [ 6500/10672]\n",
      "loss: 1.110042  [ 6600/10672]\n",
      "loss: 1.216437  [ 6700/10672]\n",
      "loss: 1.122807  [ 6800/10672]\n",
      "loss: 1.196040  [ 6900/10672]\n",
      "loss: 1.133604  [ 7000/10672]\n",
      "loss: 1.087383  [ 7100/10672]\n",
      "loss: 1.213213  [ 7200/10672]\n",
      "loss: 1.167474  [ 7300/10672]\n",
      "loss: 1.195736  [ 7400/10672]\n",
      "loss: 1.101550  [ 7500/10672]\n",
      "loss: 1.216516  [ 7600/10672]\n",
      "loss: 1.216028  [ 7700/10672]\n",
      "loss: 1.222913  [ 7800/10672]\n",
      "loss: 1.123040  [ 7900/10672]\n",
      "loss: 1.111570  [ 8000/10672]\n",
      "loss: 1.132336  [ 8100/10672]\n",
      "loss: 1.191478  [ 8200/10672]\n",
      "loss: 1.191609  [ 8300/10672]\n",
      "loss: 1.149557  [ 8400/10672]\n",
      "loss: 1.100077  [ 8500/10672]\n",
      "loss: 1.153135  [ 8600/10672]\n",
      "loss: 1.214550  [ 8700/10672]\n",
      "loss: 1.176895  [ 8800/10672]\n",
      "loss: 1.098891  [ 8900/10672]\n",
      "loss: 1.660325  [ 9000/10672]\n",
      "loss: 1.181010  [ 9100/10672]\n",
      "loss: 1.108691  [ 9200/10672]\n",
      "loss: 1.146001  [ 9300/10672]\n",
      "loss: 1.140446  [ 9400/10672]\n",
      "loss: 1.209009  [ 9500/10672]\n",
      "loss: 1.664982  [ 9600/10672]\n",
      "loss: 1.224702  [ 9700/10672]\n",
      "loss: 1.130146  [ 9800/10672]\n",
      "loss: 1.114151  [ 9900/10672]\n",
      "loss: 1.145153  [10000/10672]\n",
      "loss: 1.115292  [10100/10672]\n",
      "loss: 1.083874  [10200/10672]\n",
      "loss: 1.206421  [10300/10672]\n",
      "loss: 1.226368  [10400/10672]\n",
      "loss: 1.222630  [10500/10672]\n",
      "loss: 1.658650  [10600/10672]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.090000  [    0/10672]\n",
      "loss: 1.116309  [  100/10672]\n",
      "loss: 1.069500  [  200/10672]\n",
      "loss: 1.077360  [  300/10672]\n",
      "loss: 1.245183  [  400/10672]\n",
      "loss: 1.203933  [  500/10672]\n",
      "loss: 1.661588  [  600/10672]\n",
      "loss: 1.082090  [  700/10672]\n",
      "loss: 1.660238  [  800/10672]\n",
      "loss: 1.294676  [  900/10672]\n",
      "loss: 1.662270  [ 1000/10672]\n",
      "loss: 1.248703  [ 1100/10672]\n",
      "loss: 1.214069  [ 1200/10672]\n",
      "loss: 1.119834  [ 1300/10672]\n",
      "loss: 1.181532  [ 1400/10672]\n",
      "loss: 1.097402  [ 1500/10672]\n",
      "loss: 1.190561  [ 1600/10672]\n",
      "loss: 1.088394  [ 1700/10672]\n",
      "loss: 1.228599  [ 1800/10672]\n",
      "loss: 1.192158  [ 1900/10672]\n",
      "loss: 1.211694  [ 2000/10672]\n",
      "loss: 1.207704  [ 2100/10672]\n",
      "loss: 1.197123  [ 2200/10672]\n",
      "loss: 1.223757  [ 2300/10672]\n",
      "loss: 1.134085  [ 2400/10672]\n",
      "loss: 1.126691  [ 2500/10672]\n",
      "loss: 1.131001  [ 2600/10672]\n",
      "loss: 1.660061  [ 2700/10672]\n",
      "loss: 1.126176  [ 2800/10672]\n",
      "loss: 1.656027  [ 2900/10672]\n",
      "loss: 1.660370  [ 3000/10672]\n",
      "loss: 1.209755  [ 3100/10672]\n",
      "loss: 1.162889  [ 3200/10672]\n",
      "loss: 1.107807  [ 3300/10672]\n",
      "loss: 1.661501  [ 3400/10672]\n",
      "loss: 1.661669  [ 3500/10672]\n",
      "loss: 1.661067  [ 3600/10672]\n",
      "loss: 1.191028  [ 3700/10672]\n",
      "loss: 1.113616  [ 3800/10672]\n",
      "loss: 1.221549  [ 3900/10672]\n",
      "loss: 1.176950  [ 4000/10672]\n",
      "loss: 1.659798  [ 4100/10672]\n",
      "loss: 1.659333  [ 4200/10672]\n",
      "loss: 1.132186  [ 4300/10672]\n",
      "loss: 1.659352  [ 4400/10672]\n",
      "loss: 1.189674  [ 4500/10672]\n",
      "loss: 1.233207  [ 4600/10672]\n",
      "loss: 1.184966  [ 4700/10672]\n",
      "loss: 1.661166  [ 4800/10672]\n",
      "loss: 1.139446  [ 4900/10672]\n",
      "loss: 1.658032  [ 5000/10672]\n",
      "loss: 1.152894  [ 5100/10672]\n",
      "loss: 1.122810  [ 5200/10672]\n",
      "loss: 1.267594  [ 5300/10672]\n",
      "loss: 1.124700  [ 5400/10672]\n",
      "loss: 1.148805  [ 5500/10672]\n",
      "loss: 1.133149  [ 5600/10672]\n",
      "loss: 1.661299  [ 5700/10672]\n",
      "loss: 1.176868  [ 5800/10672]\n",
      "loss: 1.152850  [ 5900/10672]\n",
      "loss: 1.659333  [ 6000/10672]\n",
      "loss: 1.205017  [ 6100/10672]\n",
      "loss: 1.121340  [ 6200/10672]\n",
      "loss: 1.165248  [ 6300/10672]\n",
      "loss: 1.137836  [ 6400/10672]\n",
      "loss: 1.663067  [ 6500/10672]\n",
      "loss: 1.109138  [ 6600/10672]\n",
      "loss: 1.215680  [ 6700/10672]\n",
      "loss: 1.122281  [ 6800/10672]\n",
      "loss: 1.194011  [ 6900/10672]\n",
      "loss: 1.133865  [ 7000/10672]\n",
      "loss: 1.084765  [ 7100/10672]\n",
      "loss: 1.212495  [ 7200/10672]\n",
      "loss: 1.169564  [ 7300/10672]\n",
      "loss: 1.194360  [ 7400/10672]\n",
      "loss: 1.098969  [ 7500/10672]\n",
      "loss: 1.215837  [ 7600/10672]\n",
      "loss: 1.215805  [ 7700/10672]\n",
      "loss: 1.223066  [ 7800/10672]\n",
      "loss: 1.122825  [ 7900/10672]\n",
      "loss: 1.110760  [ 8000/10672]\n",
      "loss: 1.132165  [ 8100/10672]\n",
      "loss: 1.188803  [ 8200/10672]\n",
      "loss: 1.189117  [ 8300/10672]\n",
      "loss: 1.150445  [ 8400/10672]\n",
      "loss: 1.098538  [ 8500/10672]\n",
      "loss: 1.153916  [ 8600/10672]\n",
      "loss: 1.213719  [ 8700/10672]\n",
      "loss: 1.174191  [ 8800/10672]\n",
      "loss: 1.097192  [ 8900/10672]\n",
      "loss: 1.660787  [ 9000/10672]\n",
      "loss: 1.177507  [ 9100/10672]\n",
      "loss: 1.107258  [ 9200/10672]\n",
      "loss: 1.147157  [ 9300/10672]\n",
      "loss: 1.141690  [ 9400/10672]\n",
      "loss: 1.206948  [ 9500/10672]\n",
      "loss: 1.665402  [ 9600/10672]\n",
      "loss: 1.224031  [ 9700/10672]\n",
      "loss: 1.130764  [ 9800/10672]\n",
      "loss: 1.113825  [ 9900/10672]\n",
      "loss: 1.146695  [10000/10672]\n",
      "loss: 1.115004  [10100/10672]\n",
      "loss: 1.081551  [10200/10672]\n",
      "loss: 1.204434  [10300/10672]\n",
      "loss: 1.225192  [10400/10672]\n",
      "loss: 1.221372  [10500/10672]\n",
      "loss: 1.659312  [10600/10672]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.088590  [    0/10672]\n",
      "loss: 1.116174  [  100/10672]\n",
      "loss: 1.066174  [  200/10672]\n",
      "loss: 1.074792  [  300/10672]\n",
      "loss: 1.245195  [  400/10672]\n",
      "loss: 1.201378  [  500/10672]\n",
      "loss: 1.662026  [  600/10672]\n",
      "loss: 1.080569  [  700/10672]\n",
      "loss: 1.660742  [  800/10672]\n",
      "loss: 1.298046  [  900/10672]\n",
      "loss: 1.662665  [ 1000/10672]\n",
      "loss: 1.249036  [ 1100/10672]\n",
      "loss: 1.212282  [ 1200/10672]\n",
      "loss: 1.120180  [ 1300/10672]\n",
      "loss: 1.177628  [ 1400/10672]\n",
      "loss: 1.096139  [ 1500/10672]\n",
      "loss: 1.187378  [ 1600/10672]\n",
      "loss: 1.086980  [ 1700/10672]\n",
      "loss: 1.228389  [ 1800/10672]\n",
      "loss: 1.189363  [ 1900/10672]\n",
      "loss: 1.210163  [ 2000/10672]\n",
      "loss: 1.206028  [ 2100/10672]\n",
      "loss: 1.194849  [ 2200/10672]\n",
      "loss: 1.223645  [ 2300/10672]\n",
      "loss: 1.134165  [ 2400/10672]\n",
      "loss: 1.126462  [ 2500/10672]\n",
      "loss: 1.130807  [ 2600/10672]\n",
      "loss: 1.660519  [ 2700/10672]\n",
      "loss: 1.125760  [ 2800/10672]\n",
      "loss: 1.656757  [ 2900/10672]\n",
      "loss: 1.660893  [ 3000/10672]\n",
      "loss: 1.209436  [ 3100/10672]\n",
      "loss: 1.159248  [ 3200/10672]\n",
      "loss: 1.106481  [ 3300/10672]\n",
      "loss: 1.662071  [ 3400/10672]\n",
      "loss: 1.662131  [ 3500/10672]\n",
      "loss: 1.661486  [ 3600/10672]\n",
      "loss: 1.188904  [ 3700/10672]\n",
      "loss: 1.112955  [ 3800/10672]\n",
      "loss: 1.221070  [ 3900/10672]\n",
      "loss: 1.173200  [ 4000/10672]\n",
      "loss: 1.660304  [ 4100/10672]\n",
      "loss: 1.659813  [ 4200/10672]\n",
      "loss: 1.132499  [ 4300/10672]\n",
      "loss: 1.659868  [ 4400/10672]\n",
      "loss: 1.187373  [ 4500/10672]\n",
      "loss: 1.233921  [ 4600/10672]\n",
      "loss: 1.182389  [ 4700/10672]\n",
      "loss: 1.661667  [ 4800/10672]\n",
      "loss: 1.139910  [ 4900/10672]\n",
      "loss: 1.658623  [ 5000/10672]\n",
      "loss: 1.154229  [ 5100/10672]\n",
      "loss: 1.122172  [ 5200/10672]\n",
      "loss: 1.270544  [ 5300/10672]\n",
      "loss: 1.124661  [ 5400/10672]\n",
      "loss: 1.144049  [ 5500/10672]\n",
      "loss: 1.133449  [ 5600/10672]\n",
      "loss: 1.661797  [ 5700/10672]\n",
      "loss: 1.174076  [ 5800/10672]\n",
      "loss: 1.154069  [ 5900/10672]\n",
      "loss: 1.659856  [ 6000/10672]\n",
      "loss: 1.204152  [ 6100/10672]\n",
      "loss: 1.120123  [ 6200/10672]\n",
      "loss: 1.161982  [ 6300/10672]\n",
      "loss: 1.138557  [ 6400/10672]\n",
      "loss: 1.663550  [ 6500/10672]\n",
      "loss: 1.108255  [ 6600/10672]\n",
      "loss: 1.214985  [ 6700/10672]\n",
      "loss: 1.121764  [ 6800/10672]\n",
      "loss: 1.192049  [ 6900/10672]\n",
      "loss: 1.134136  [ 7000/10672]\n",
      "loss: 1.082181  [ 7100/10672]\n",
      "loss: 1.211841  [ 7200/10672]\n",
      "loss: 1.171668  [ 7300/10672]\n",
      "loss: 1.193053  [ 7400/10672]\n",
      "loss: 1.096427  [ 7500/10672]\n",
      "loss: 1.215219  [ 7600/10672]\n",
      "loss: 1.215643  [ 7700/10672]\n",
      "loss: 1.223272  [ 7800/10672]\n",
      "loss: 1.122620  [ 7900/10672]\n",
      "loss: 1.109970  [ 8000/10672]\n",
      "loss: 1.132011  [ 8100/10672]\n",
      "loss: 1.186200  [ 8200/10672]\n",
      "loss: 1.186701  [ 8300/10672]\n",
      "loss: 1.151360  [ 8400/10672]\n",
      "loss: 1.097033  [ 8500/10672]\n",
      "loss: 1.154739  [ 8600/10672]\n",
      "loss: 1.212956  [ 8700/10672]\n",
      "loss: 1.171563  [ 8800/10672]\n",
      "loss: 1.095519  [ 8900/10672]\n",
      "loss: 1.661191  [ 9000/10672]\n",
      "loss: 1.174083  [ 9100/10672]\n",
      "loss: 1.105847  [ 9200/10672]\n",
      "loss: 1.148333  [ 9300/10672]\n",
      "loss: 1.142945  [ 9400/10672]\n",
      "loss: 1.204954  [ 9500/10672]\n",
      "loss: 1.665782  [ 9600/10672]\n",
      "loss: 1.223414  [ 9700/10672]\n",
      "loss: 1.131396  [ 9800/10672]\n",
      "loss: 1.113519  [ 9900/10672]\n",
      "loss: 1.148260  [10000/10672]\n",
      "loss: 1.114727  [10100/10672]\n",
      "loss: 1.079263  [10200/10672]\n",
      "loss: 1.202511  [10300/10672]\n",
      "loss: 1.224072  [10400/10672]\n",
      "loss: 1.220170  [10500/10672]\n",
      "loss: 1.659901  [10600/10672]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.087207  [    0/10672]\n",
      "loss: 1.116048  [  100/10672]\n",
      "loss: 1.062896  [  200/10672]\n",
      "loss: 1.072262  [  300/10672]\n",
      "loss: 1.245256  [  400/10672]\n",
      "loss: 1.198894  [  500/10672]\n",
      "loss: 1.662411  [  600/10672]\n",
      "loss: 1.079068  [  700/10672]\n",
      "loss: 1.661186  [  800/10672]\n",
      "loss: 1.301443  [  900/10672]\n",
      "loss: 1.663012  [ 1000/10672]\n",
      "loss: 1.249423  [ 1100/10672]\n",
      "loss: 1.210553  [ 1200/10672]\n",
      "loss: 1.120535  [ 1300/10672]\n",
      "loss: 1.173807  [ 1400/10672]\n",
      "loss: 1.094899  [ 1500/10672]\n",
      "loss: 1.184265  [ 1600/10672]\n",
      "loss: 1.085583  [ 1700/10672]\n",
      "loss: 1.228236  [ 1800/10672]\n",
      "loss: 1.186642  [ 1900/10672]\n",
      "loss: 1.208695  [ 2000/10672]\n",
      "loss: 1.204419  [ 2100/10672]\n",
      "loss: 1.192643  [ 2200/10672]\n",
      "loss: 1.223591  [ 2300/10672]\n",
      "loss: 1.134255  [ 2400/10672]\n",
      "loss: 1.126239  [ 2500/10672]\n",
      "loss: 1.130632  [ 2600/10672]\n",
      "loss: 1.660922  [ 2700/10672]\n",
      "loss: 1.125351  [ 2800/10672]\n",
      "loss: 1.657410  [ 2900/10672]\n",
      "loss: 1.661358  [ 3000/10672]\n",
      "loss: 1.209179  [ 3100/10672]\n",
      "loss: 1.155688  [ 3200/10672]\n",
      "loss: 1.105165  [ 3300/10672]\n",
      "loss: 1.662587  [ 3400/10672]\n",
      "loss: 1.662543  [ 3500/10672]\n",
      "loss: 1.661859  [ 3600/10672]\n",
      "loss: 1.186846  [ 3700/10672]\n",
      "loss: 1.112306  [ 3800/10672]\n",
      "loss: 1.220643  [ 3900/10672]\n",
      "loss: 1.169531  [ 4000/10672]\n",
      "loss: 1.660751  [ 4100/10672]\n",
      "loss: 1.660240  [ 4200/10672]\n",
      "loss: 1.132818  [ 4300/10672]\n",
      "loss: 1.660323  [ 4400/10672]\n",
      "loss: 1.185132  [ 4500/10672]\n",
      "loss: 1.234685  [ 4600/10672]\n",
      "loss: 1.179883  [ 4700/10672]\n",
      "loss: 1.662114  [ 4800/10672]\n",
      "loss: 1.140393  [ 4900/10672]\n",
      "loss: 1.659147  [ 5000/10672]\n",
      "loss: 1.155572  [ 5100/10672]\n",
      "loss: 1.121542  [ 5200/10672]\n",
      "loss: 1.273532  [ 5300/10672]\n",
      "loss: 1.124632  [ 5400/10672]\n",
      "loss: 1.139385  [ 5500/10672]\n",
      "loss: 1.133750  [ 5600/10672]\n",
      "loss: 1.662243  [ 5700/10672]\n",
      "loss: 1.171350  [ 5800/10672]\n",
      "loss: 1.155291  [ 5900/10672]\n",
      "loss: 1.660322  [ 6000/10672]\n",
      "loss: 1.203348  [ 6100/10672]\n",
      "loss: 1.118935  [ 6200/10672]\n",
      "loss: 1.158796  [ 6300/10672]\n",
      "loss: 1.139291  [ 6400/10672]\n",
      "loss: 1.663987  [ 6500/10672]\n",
      "loss: 1.107389  [ 6600/10672]\n",
      "loss: 1.214345  [ 6700/10672]\n",
      "loss: 1.121251  [ 6800/10672]\n",
      "loss: 1.190144  [ 6900/10672]\n",
      "loss: 1.134413  [ 7000/10672]\n",
      "loss: 1.079627  [ 7100/10672]\n",
      "loss: 1.211242  [ 7200/10672]\n",
      "loss: 1.173778  [ 7300/10672]\n",
      "loss: 1.191807  [ 7400/10672]\n",
      "loss: 1.093919  [ 7500/10672]\n",
      "loss: 1.214655  [ 7600/10672]\n",
      "loss: 1.215533  [ 7700/10672]\n",
      "loss: 1.223522  [ 7800/10672]\n",
      "loss: 1.122421  [ 7900/10672]\n",
      "loss: 1.109198  [ 8000/10672]\n",
      "loss: 1.131870  [ 8100/10672]\n",
      "loss: 1.183662  [ 8200/10672]\n",
      "loss: 1.184350  [ 8300/10672]\n",
      "loss: 1.152296  [ 8400/10672]\n",
      "loss: 1.095557  [ 8500/10672]\n",
      "loss: 1.155596  [ 8600/10672]\n",
      "loss: 1.212254  [ 8700/10672]\n",
      "loss: 1.169004  [ 8800/10672]\n",
      "loss: 1.093867  [ 8900/10672]\n",
      "loss: 1.661548  [ 9000/10672]\n",
      "loss: 1.170727  [ 9100/10672]\n",
      "loss: 1.104453  [ 9200/10672]\n",
      "loss: 1.149525  [ 9300/10672]\n",
      "loss: 1.144204  [ 9400/10672]\n",
      "loss: 1.203020  [ 9500/10672]\n",
      "loss: 1.666130  [ 9600/10672]\n",
      "loss: 1.222846  [ 9700/10672]\n",
      "loss: 1.132037  [ 9800/10672]\n",
      "loss: 1.113225  [ 9900/10672]\n",
      "loss: 1.149839  [10000/10672]\n",
      "loss: 1.114456  [10100/10672]\n",
      "loss: 1.077002  [10200/10672]\n",
      "loss: 1.200646  [10300/10672]\n",
      "loss: 1.223002  [10400/10672]\n",
      "loss: 1.219017  [10500/10672]\n",
      "loss: 1.660431  [10600/10672]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.085848  [    0/10672]\n",
      "loss: 1.115927  [  100/10672]\n",
      "loss: 1.059662  [  200/10672]\n",
      "loss: 1.069765  [  300/10672]\n",
      "loss: 1.245358  [  400/10672]\n",
      "loss: 1.196470  [  500/10672]\n",
      "loss: 1.662751  [  600/10672]\n",
      "loss: 1.077584  [  700/10672]\n",
      "loss: 1.661579  [  800/10672]\n",
      "loss: 1.304859  [  900/10672]\n",
      "loss: 1.663317  [ 1000/10672]\n",
      "loss: 1.249854  [ 1100/10672]\n",
      "loss: 1.208873  [ 1200/10672]\n",
      "loss: 1.120896  [ 1300/10672]\n",
      "loss: 1.170058  [ 1400/10672]\n",
      "loss: 1.093679  [ 1500/10672]\n",
      "loss: 1.181212  [ 1600/10672]\n",
      "loss: 1.084203  [ 1700/10672]\n",
      "loss: 1.228132  [ 1800/10672]\n",
      "loss: 1.183986  [ 1900/10672]\n",
      "loss: 1.207282  [ 2000/10672]\n",
      "loss: 1.202867  [ 2100/10672]\n",
      "loss: 1.190495  [ 2200/10672]\n",
      "loss: 1.223584  [ 2300/10672]\n",
      "loss: 1.134353  [ 2400/10672]\n",
      "loss: 1.126022  [ 2500/10672]\n",
      "loss: 1.130474  [ 2600/10672]\n",
      "loss: 1.661280  [ 2700/10672]\n",
      "loss: 1.124948  [ 2800/10672]\n",
      "loss: 1.657998  [ 2900/10672]\n",
      "loss: 1.661774  [ 3000/10672]\n",
      "loss: 1.208975  [ 3100/10672]\n",
      "loss: 1.152200  [ 3200/10672]\n",
      "loss: 1.103857  [ 3300/10672]\n",
      "loss: 1.663056  [ 3400/10672]\n",
      "loss: 1.662912  [ 3500/10672]\n",
      "loss: 1.662194  [ 3600/10672]\n",
      "loss: 1.184844  [ 3700/10672]\n",
      "loss: 1.111668  [ 3800/10672]\n",
      "loss: 1.220258  [ 3900/10672]\n",
      "loss: 1.165932  [ 4000/10672]\n",
      "loss: 1.661150  [ 4100/10672]\n",
      "loss: 1.660623  [ 4200/10672]\n",
      "loss: 1.133142  [ 4300/10672]\n",
      "loss: 1.660727  [ 4400/10672]\n",
      "loss: 1.182944  [ 4500/10672]\n",
      "loss: 1.235493  [ 4600/10672]\n",
      "loss: 1.177440  [ 4700/10672]\n",
      "loss: 1.662517  [ 4800/10672]\n",
      "loss: 1.140893  [ 4900/10672]\n",
      "loss: 1.659615  [ 5000/10672]\n",
      "loss: 1.156917  [ 5100/10672]\n",
      "loss: 1.120916  [ 5200/10672]\n",
      "loss: 1.276551  [ 5300/10672]\n",
      "loss: 1.124609  [ 5400/10672]\n",
      "loss: 1.134806  [ 5500/10672]\n",
      "loss: 1.134049  [ 5600/10672]\n",
      "loss: 1.662645  [ 5700/10672]\n",
      "loss: 1.168685  [ 5800/10672]\n",
      "loss: 1.156512  [ 5900/10672]\n",
      "loss: 1.660739  [ 6000/10672]\n",
      "loss: 1.202597  [ 6100/10672]\n",
      "loss: 1.117769  [ 6200/10672]\n",
      "loss: 1.155684  [ 6300/10672]\n",
      "loss: 1.140034  [ 6400/10672]\n",
      "loss: 1.664387  [ 6500/10672]\n",
      "loss: 1.106539  [ 6600/10672]\n",
      "loss: 1.213751  [ 6700/10672]\n",
      "loss: 1.120742  [ 6800/10672]\n",
      "loss: 1.188287  [ 6900/10672]\n",
      "loss: 1.134696  [ 7000/10672]\n",
      "loss: 1.077102  [ 7100/10672]\n",
      "loss: 1.210687  [ 7200/10672]\n",
      "loss: 1.175892  [ 7300/10672]\n",
      "loss: 1.190613  [ 7400/10672]\n",
      "loss: 1.091443  [ 7500/10672]\n",
      "loss: 1.214133  [ 7600/10672]\n",
      "loss: 1.215464  [ 7700/10672]\n",
      "loss: 1.223806  [ 7800/10672]\n",
      "loss: 1.122228  [ 7900/10672]\n",
      "loss: 1.108441  [ 8000/10672]\n",
      "loss: 1.131738  [ 8100/10672]\n",
      "loss: 1.181180  [ 8200/10672]\n",
      "loss: 1.182055  [ 8300/10672]\n",
      "loss: 1.153249  [ 8400/10672]\n",
      "loss: 1.094108  [ 8500/10672]\n",
      "loss: 1.156482  [ 8600/10672]\n",
      "loss: 1.211603  [ 8700/10672]\n",
      "loss: 1.166506  [ 8800/10672]\n",
      "loss: 1.092235  [ 8900/10672]\n",
      "loss: 1.661865  [ 9000/10672]\n",
      "loss: 1.167431  [ 9100/10672]\n",
      "loss: 1.103075  [ 9200/10672]\n",
      "loss: 1.150729  [ 9300/10672]\n",
      "loss: 1.145468  [ 9400/10672]\n",
      "loss: 1.201136  [ 9500/10672]\n",
      "loss: 1.666449  [ 9600/10672]\n",
      "loss: 1.222316  [ 9700/10672]\n",
      "loss: 1.132687  [ 9800/10672]\n",
      "loss: 1.112945  [ 9900/10672]\n",
      "loss: 1.151432  [10000/10672]\n",
      "loss: 1.114190  [10100/10672]\n",
      "loss: 1.074769  [10200/10672]\n",
      "loss: 1.198829  [10300/10672]\n",
      "loss: 1.221974  [10400/10672]\n",
      "loss: 1.217904  [10500/10672]\n",
      "loss: 1.660910  [10600/10672]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.084512  [    0/10672]\n",
      "loss: 1.115808  [  100/10672]\n",
      "loss: 1.056470  [  200/10672]\n",
      "loss: 1.067300  [  300/10672]\n",
      "loss: 1.245493  [  400/10672]\n",
      "loss: 1.194101  [  500/10672]\n",
      "loss: 1.663054  [  600/10672]\n",
      "loss: 1.076114  [  700/10672]\n",
      "loss: 1.661930  [  800/10672]\n",
      "loss: 1.308287  [  900/10672]\n",
      "loss: 1.663589  [ 1000/10672]\n",
      "loss: 1.250321  [ 1100/10672]\n",
      "loss: 1.207234  [ 1200/10672]\n",
      "loss: 1.121262  [ 1300/10672]\n",
      "loss: 1.166376  [ 1400/10672]\n",
      "loss: 1.092477  [ 1500/10672]\n",
      "loss: 1.178214  [ 1600/10672]\n",
      "loss: 1.082838  [ 1700/10672]\n",
      "loss: 1.228067  [ 1800/10672]\n",
      "loss: 1.181385  [ 1900/10672]\n",
      "loss: 1.205912  [ 2000/10672]\n",
      "loss: 1.201362  [ 2100/10672]\n",
      "loss: 1.188397  [ 2200/10672]\n",
      "loss: 1.223616  [ 2300/10672]\n",
      "loss: 1.134457  [ 2400/10672]\n",
      "loss: 1.125808  [ 2500/10672]\n",
      "loss: 1.130332  [ 2600/10672]\n",
      "loss: 1.661599  [ 2700/10672]\n",
      "loss: 1.124549  [ 2800/10672]\n",
      "loss: 1.658531  [ 2900/10672]\n",
      "loss: 1.662148  [ 3000/10672]\n",
      "loss: 1.208815  [ 3100/10672]\n",
      "loss: 1.148779  [ 3200/10672]\n",
      "loss: 1.102557  [ 3300/10672]\n",
      "loss: 1.663488  [ 3400/10672]\n",
      "loss: 1.663246  [ 3500/10672]\n",
      "loss: 1.662498  [ 3600/10672]\n",
      "loss: 1.182891  [ 3700/10672]\n",
      "loss: 1.111038  [ 3800/10672]\n",
      "loss: 1.219910  [ 3900/10672]\n",
      "loss: 1.162397  [ 4000/10672]\n",
      "loss: 1.661506  [ 4100/10672]\n",
      "loss: 1.660970  [ 4200/10672]\n",
      "loss: 1.133466  [ 4300/10672]\n",
      "loss: 1.661089  [ 4400/10672]\n",
      "loss: 1.180803  [ 4500/10672]\n",
      "loss: 1.236337  [ 4600/10672]\n",
      "loss: 1.175054  [ 4700/10672]\n",
      "loss: 1.662881  [ 4800/10672]\n",
      "loss: 1.141403  [ 4900/10672]\n",
      "loss: 1.660036  [ 5000/10672]\n",
      "loss: 1.158262  [ 5100/10672]\n",
      "loss: 1.120292  [ 5200/10672]\n",
      "loss: 1.279594  [ 5300/10672]\n",
      "loss: 1.124589  [ 5400/10672]\n",
      "loss: 1.130306  [ 5500/10672]\n",
      "loss: 1.134342  [ 5600/10672]\n",
      "loss: 1.663010  [ 5700/10672]\n",
      "loss: 1.166072  [ 5800/10672]\n",
      "loss: 1.157730  [ 5900/10672]\n",
      "loss: 1.661116  [ 6000/10672]\n",
      "loss: 1.201894  [ 6100/10672]\n",
      "loss: 1.116622  [ 6200/10672]\n",
      "loss: 1.152639  [ 6300/10672]\n",
      "loss: 1.140784  [ 6400/10672]\n",
      "loss: 1.664756  [ 6500/10672]\n",
      "loss: 1.105701  [ 6600/10672]\n",
      "loss: 1.213198  [ 6700/10672]\n",
      "loss: 1.120234  [ 6800/10672]\n",
      "loss: 1.186474  [ 6900/10672]\n",
      "loss: 1.134981  [ 7000/10672]\n",
      "loss: 1.074602  [ 7100/10672]\n",
      "loss: 1.210174  [ 7200/10672]\n",
      "loss: 1.178005  [ 7300/10672]\n",
      "loss: 1.189465  [ 7400/10672]\n",
      "loss: 1.088995  [ 7500/10672]\n",
      "loss: 1.213651  [ 7600/10672]\n",
      "loss: 1.215433  [ 7700/10672]\n",
      "loss: 1.224122  [ 7800/10672]\n",
      "loss: 1.122035  [ 7900/10672]\n",
      "loss: 1.107694  [ 8000/10672]\n",
      "loss: 1.131611  [ 8100/10672]\n",
      "loss: 1.178752  [ 8200/10672]\n",
      "loss: 1.179814  [ 8300/10672]\n",
      "loss: 1.154212  [ 8400/10672]\n",
      "loss: 1.092680  [ 8500/10672]\n",
      "loss: 1.157388  [ 8600/10672]\n",
      "loss: 1.211001  [ 8700/10672]\n",
      "loss: 1.164065  [ 8800/10672]\n",
      "loss: 1.090618  [ 8900/10672]\n",
      "loss: 1.662148  [ 9000/10672]\n",
      "loss: 1.164190  [ 9100/10672]\n",
      "loss: 1.101708  [ 9200/10672]\n",
      "loss: 1.151940  [ 9300/10672]\n",
      "loss: 1.146731  [ 9400/10672]\n",
      "loss: 1.199299  [ 9500/10672]\n",
      "loss: 1.666746  [ 9600/10672]\n",
      "loss: 1.221822  [ 9700/10672]\n",
      "loss: 1.133340  [ 9800/10672]\n",
      "loss: 1.112672  [ 9900/10672]\n",
      "loss: 1.153030  [10000/10672]\n",
      "loss: 1.113924  [10100/10672]\n",
      "loss: 1.072558  [10200/10672]\n",
      "loss: 1.197058  [10300/10672]\n",
      "loss: 1.220985  [10400/10672]\n",
      "loss: 1.216829  [10500/10672]\n",
      "loss: 1.661346  [10600/10672]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.083194  [    0/10672]\n",
      "loss: 1.115689  [  100/10672]\n",
      "loss: 1.053317  [  200/10672]\n",
      "loss: 1.064862  [  300/10672]\n",
      "loss: 1.245657  [  400/10672]\n",
      "loss: 1.191781  [  500/10672]\n",
      "loss: 1.663326  [  600/10672]\n",
      "loss: 1.074656  [  700/10672]\n",
      "loss: 1.662245  [  800/10672]\n",
      "loss: 1.311721  [  900/10672]\n",
      "loss: 1.663832  [ 1000/10672]\n",
      "loss: 1.250820  [ 1100/10672]\n",
      "loss: 1.205634  [ 1200/10672]\n",
      "loss: 1.121626  [ 1300/10672]\n",
      "loss: 1.162757  [ 1400/10672]\n",
      "loss: 1.091288  [ 1500/10672]\n",
      "loss: 1.175267  [ 1600/10672]\n",
      "loss: 1.081482  [ 1700/10672]\n",
      "loss: 1.228041  [ 1800/10672]\n",
      "loss: 1.178838  [ 1900/10672]\n",
      "loss: 1.204587  [ 2000/10672]\n",
      "loss: 1.199906  [ 2100/10672]\n",
      "loss: 1.186347  [ 2200/10672]\n",
      "loss: 1.223686  [ 2300/10672]\n",
      "loss: 1.134562  [ 2400/10672]\n",
      "loss: 1.125594  [ 2500/10672]\n",
      "loss: 1.130200  [ 2600/10672]\n",
      "loss: 1.661885  [ 2700/10672]\n",
      "loss: 1.124150  [ 2800/10672]\n",
      "loss: 1.659018  [ 2900/10672]\n",
      "loss: 1.662488  [ 3000/10672]\n",
      "loss: 1.208697  [ 3100/10672]\n",
      "loss: 1.145421  [ 3200/10672]\n",
      "loss: 1.101262  [ 3300/10672]\n",
      "loss: 1.663888  [ 3400/10672]\n",
      "loss: 1.663551  [ 3500/10672]\n",
      "loss: 1.662776  [ 3600/10672]\n",
      "loss: 1.180984  [ 3700/10672]\n",
      "loss: 1.110413  [ 3800/10672]\n",
      "loss: 1.219594  [ 3900/10672]\n",
      "loss: 1.158921  [ 4000/10672]\n",
      "loss: 1.661828  [ 4100/10672]\n",
      "loss: 1.661287  [ 4200/10672]\n",
      "loss: 1.133790  [ 4300/10672]\n",
      "loss: 1.661414  [ 4400/10672]\n",
      "loss: 1.178705  [ 4500/10672]\n",
      "loss: 1.237213  [ 4600/10672]\n",
      "loss: 1.172721  [ 4700/10672]\n",
      "loss: 1.663214  [ 4800/10672]\n",
      "loss: 1.141922  [ 4900/10672]\n",
      "loss: 1.660416  [ 5000/10672]\n",
      "loss: 1.159607  [ 5100/10672]\n",
      "loss: 1.119668  [ 5200/10672]\n",
      "loss: 1.282655  [ 5300/10672]\n",
      "loss: 1.124570  [ 5400/10672]\n",
      "loss: 1.125881  [ 5500/10672]\n",
      "loss: 1.134630  [ 5600/10672]\n",
      "loss: 1.663343  [ 5700/10672]\n",
      "loss: 1.163507  [ 5800/10672]\n",
      "loss: 1.158945  [ 5900/10672]\n",
      "loss: 1.661457  [ 6000/10672]\n",
      "loss: 1.201230  [ 6100/10672]\n",
      "loss: 1.115492  [ 6200/10672]\n",
      "loss: 1.149655  [ 6300/10672]\n",
      "loss: 1.141539  [ 6400/10672]\n",
      "loss: 1.665098  [ 6500/10672]\n",
      "loss: 1.104873  [ 6600/10672]\n",
      "loss: 1.212682  [ 6700/10672]\n",
      "loss: 1.119727  [ 6800/10672]\n",
      "loss: 1.184698  [ 6900/10672]\n",
      "loss: 1.135268  [ 7000/10672]\n",
      "loss: 1.072127  [ 7100/10672]\n",
      "loss: 1.209695  [ 7200/10672]\n",
      "loss: 1.180117  [ 7300/10672]\n",
      "loss: 1.188358  [ 7400/10672]\n",
      "loss: 1.086575  [ 7500/10672]\n",
      "loss: 1.213201  [ 7600/10672]\n",
      "loss: 1.215433  [ 7700/10672]\n",
      "loss: 1.224461  [ 7800/10672]\n",
      "loss: 1.121845  [ 7900/10672]\n",
      "loss: 1.106958  [ 8000/10672]\n",
      "loss: 1.131490  [ 8100/10672]\n",
      "loss: 1.176369  [ 8200/10672]\n",
      "loss: 1.177618  [ 8300/10672]\n",
      "loss: 1.155186  [ 8400/10672]\n",
      "loss: 1.091273  [ 8500/10672]\n",
      "loss: 1.158314  [ 8600/10672]\n",
      "loss: 1.210439  [ 8700/10672]\n",
      "loss: 1.161674  [ 8800/10672]\n",
      "loss: 1.089019  [ 8900/10672]\n",
      "loss: 1.662403  [ 9000/10672]\n",
      "loss: 1.160998  [ 9100/10672]\n",
      "loss: 1.100355  [ 9200/10672]\n",
      "loss: 1.153157  [ 9300/10672]\n",
      "loss: 1.147994  [ 9400/10672]\n",
      "loss: 1.197503  [ 9500/10672]\n",
      "loss: 1.667024  [ 9600/10672]\n",
      "loss: 1.221356  [ 9700/10672]\n",
      "loss: 1.133998  [ 9800/10672]\n",
      "loss: 1.112408  [ 9900/10672]\n",
      "loss: 1.154635  [10000/10672]\n",
      "loss: 1.113661  [10100/10672]\n",
      "loss: 1.070371  [10200/10672]\n",
      "loss: 1.195326  [10300/10672]\n",
      "loss: 1.220028  [10400/10672]\n",
      "loss: 1.215785  [10500/10672]\n",
      "loss: 1.661746  [10600/10672]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.081894  [    0/10672]\n",
      "loss: 1.115569  [  100/10672]\n",
      "loss: 1.050201  [  200/10672]\n",
      "loss: 1.062451  [  300/10672]\n",
      "loss: 1.245846  [  400/10672]\n",
      "loss: 1.189505  [  500/10672]\n",
      "loss: 1.663570  [  600/10672]\n",
      "loss: 1.073210  [  700/10672]\n",
      "loss: 1.662529  [  800/10672]\n",
      "loss: 1.315157  [  900/10672]\n",
      "loss: 1.664050  [ 1000/10672]\n",
      "loss: 1.251344  [ 1100/10672]\n",
      "loss: 1.204064  [ 1200/10672]\n",
      "loss: 1.121993  [ 1300/10672]\n",
      "loss: 1.159192  [ 1400/10672]\n",
      "loss: 1.090115  [ 1500/10672]\n",
      "loss: 1.172363  [ 1600/10672]\n",
      "loss: 1.080140  [ 1700/10672]\n",
      "loss: 1.228043  [ 1800/10672]\n",
      "loss: 1.176336  [ 1900/10672]\n",
      "loss: 1.203295  [ 2000/10672]\n",
      "loss: 1.198489  [ 2100/10672]\n",
      "loss: 1.184337  [ 2200/10672]\n",
      "loss: 1.223783  [ 2300/10672]\n",
      "loss: 1.134670  [ 2400/10672]\n",
      "loss: 1.125381  [ 2500/10672]\n",
      "loss: 1.130078  [ 2600/10672]\n",
      "loss: 1.662143  [ 2700/10672]\n",
      "loss: 1.123754  [ 2800/10672]\n",
      "loss: 1.659464  [ 2900/10672]\n",
      "loss: 1.662797  [ 3000/10672]\n",
      "loss: 1.208614  [ 3100/10672]\n",
      "loss: 1.142120  [ 3200/10672]\n",
      "loss: 1.099973  [ 3300/10672]\n",
      "loss: 1.664260  [ 3400/10672]\n",
      "loss: 1.663830  [ 3500/10672]\n",
      "loss: 1.663033  [ 3600/10672]\n",
      "loss: 1.179116  [ 3700/10672]\n",
      "loss: 1.109794  [ 3800/10672]\n",
      "loss: 1.219304  [ 3900/10672]\n",
      "loss: 1.155500  [ 4000/10672]\n",
      "loss: 1.662119  [ 4100/10672]\n",
      "loss: 1.661577  [ 4200/10672]\n",
      "loss: 1.134111  [ 4300/10672]\n",
      "loss: 1.661709  [ 4400/10672]\n",
      "loss: 1.176645  [ 4500/10672]\n",
      "loss: 1.238115  [ 4600/10672]\n",
      "loss: 1.170434  [ 4700/10672]\n",
      "loss: 1.663519  [ 4800/10672]\n",
      "loss: 1.142448  [ 4900/10672]\n",
      "loss: 1.660762  [ 5000/10672]\n",
      "loss: 1.160950  [ 5100/10672]\n",
      "loss: 1.119045  [ 5200/10672]\n",
      "loss: 1.285728  [ 5300/10672]\n",
      "loss: 1.124553  [ 5400/10672]\n",
      "loss: 1.121524  [ 5500/10672]\n",
      "loss: 1.134913  [ 5600/10672]\n",
      "loss: 1.663650  [ 5700/10672]\n",
      "loss: 1.160984  [ 5800/10672]\n",
      "loss: 1.160156  [ 5900/10672]\n",
      "loss: 1.661769  [ 6000/10672]\n",
      "loss: 1.200602  [ 6100/10672]\n",
      "loss: 1.114379  [ 6200/10672]\n",
      "loss: 1.146728  [ 6300/10672]\n",
      "loss: 1.142298  [ 6400/10672]\n",
      "loss: 1.665418  [ 6500/10672]\n",
      "loss: 1.104056  [ 6600/10672]\n",
      "loss: 1.212195  [ 6700/10672]\n",
      "loss: 1.119223  [ 6800/10672]\n",
      "loss: 1.182953  [ 6900/10672]\n",
      "loss: 1.135558  [ 7000/10672]\n",
      "loss: 1.069679  [ 7100/10672]\n",
      "loss: 1.209244  [ 7200/10672]\n",
      "loss: 1.182226  [ 7300/10672]\n",
      "loss: 1.187286  [ 7400/10672]\n",
      "loss: 1.084182  [ 7500/10672]\n",
      "loss: 1.212778  [ 7600/10672]\n",
      "loss: 1.215459  [ 7700/10672]\n",
      "loss: 1.224820  [ 7800/10672]\n",
      "loss: 1.121655  [ 7900/10672]\n",
      "loss: 1.106231  [ 8000/10672]\n",
      "loss: 1.131370  [ 8100/10672]\n",
      "loss: 1.174029  [ 8200/10672]\n",
      "loss: 1.175464  [ 8300/10672]\n",
      "loss: 1.156166  [ 8400/10672]\n",
      "loss: 1.089884  [ 8500/10672]\n",
      "loss: 1.159254  [ 8600/10672]\n",
      "loss: 1.209915  [ 8700/10672]\n",
      "loss: 1.159330  [ 8800/10672]\n",
      "loss: 1.087432  [ 8900/10672]\n",
      "loss: 1.662634  [ 9000/10672]\n",
      "loss: 1.157851  [ 9100/10672]\n",
      "loss: 1.099012  [ 9200/10672]\n",
      "loss: 1.154378  [ 9300/10672]\n",
      "loss: 1.149255  [ 9400/10672]\n",
      "loss: 1.195744  [ 9500/10672]\n",
      "loss: 1.667286  [ 9600/10672]\n",
      "loss: 1.220915  [ 9700/10672]\n",
      "loss: 1.134658  [ 9800/10672]\n",
      "loss: 1.112151  [ 9900/10672]\n",
      "loss: 1.156243  [10000/10672]\n",
      "loss: 1.113396  [10100/10672]\n",
      "loss: 1.068204  [10200/10672]\n",
      "loss: 1.193631  [10300/10672]\n",
      "loss: 1.219103  [10400/10672]\n",
      "loss: 1.214770  [10500/10672]\n",
      "loss: 1.662113  [10600/10672]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.080611  [    0/10672]\n",
      "loss: 1.115446  [  100/10672]\n",
      "loss: 1.047121  [  200/10672]\n",
      "loss: 1.060065  [  300/10672]\n",
      "loss: 1.246055  [  400/10672]\n",
      "loss: 1.187270  [  500/10672]\n",
      "loss: 1.663791  [  600/10672]\n",
      "loss: 1.071773  [  700/10672]\n",
      "loss: 1.662787  [  800/10672]\n",
      "loss: 1.318593  [  900/10672]\n",
      "loss: 1.664248  [ 1000/10672]\n",
      "loss: 1.251889  [ 1100/10672]\n",
      "loss: 1.202524  [ 1200/10672]\n",
      "loss: 1.122357  [ 1300/10672]\n",
      "loss: 1.155681  [ 1400/10672]\n",
      "loss: 1.088952  [ 1500/10672]\n",
      "loss: 1.169501  [ 1600/10672]\n",
      "loss: 1.078807  [ 1700/10672]\n",
      "loss: 1.228073  [ 1800/10672]\n",
      "loss: 1.173878  [ 1900/10672]\n",
      "loss: 1.202034  [ 2000/10672]\n",
      "loss: 1.197108  [ 2100/10672]\n",
      "loss: 1.182365  [ 2200/10672]\n",
      "loss: 1.223906  [ 2300/10672]\n",
      "loss: 1.134777  [ 2400/10672]\n",
      "loss: 1.125169  [ 2500/10672]\n",
      "loss: 1.129965  [ 2600/10672]\n",
      "loss: 1.662378  [ 2700/10672]\n",
      "loss: 1.123358  [ 2800/10672]\n",
      "loss: 1.659874  [ 2900/10672]\n",
      "loss: 1.663080  [ 3000/10672]\n",
      "loss: 1.208561  [ 3100/10672]\n",
      "loss: 1.138872  [ 3200/10672]\n",
      "loss: 1.098690  [ 3300/10672]\n",
      "loss: 1.664609  [ 3400/10672]\n",
      "loss: 1.664087  [ 3500/10672]\n",
      "loss: 1.663272  [ 3600/10672]\n",
      "loss: 1.177284  [ 3700/10672]\n",
      "loss: 1.109180  [ 3800/10672]\n",
      "loss: 1.219037  [ 3900/10672]\n",
      "loss: 1.152126  [ 4000/10672]\n",
      "loss: 1.662384  [ 4100/10672]\n",
      "loss: 1.661846  [ 4200/10672]\n",
      "loss: 1.134431  [ 4300/10672]\n",
      "loss: 1.661976  [ 4400/10672]\n",
      "loss: 1.174618  [ 4500/10672]\n",
      "loss: 1.239037  [ 4600/10672]\n",
      "loss: 1.168189  [ 4700/10672]\n",
      "loss: 1.663800  [ 4800/10672]\n",
      "loss: 1.142980  [ 4900/10672]\n",
      "loss: 1.661077  [ 5000/10672]\n",
      "loss: 1.162293  [ 5100/10672]\n",
      "loss: 1.118422  [ 5200/10672]\n",
      "loss: 1.288810  [ 5300/10672]\n",
      "loss: 1.124536  [ 5400/10672]\n",
      "loss: 1.117233  [ 5500/10672]\n",
      "loss: 1.135190  [ 5600/10672]\n",
      "loss: 1.663933  [ 5700/10672]\n",
      "loss: 1.158499  [ 5800/10672]\n",
      "loss: 1.161362  [ 5900/10672]\n",
      "loss: 1.662054  [ 6000/10672]\n",
      "loss: 1.200006  [ 6100/10672]\n",
      "loss: 1.113279  [ 6200/10672]\n",
      "loss: 1.143854  [ 6300/10672]\n",
      "loss: 1.143060  [ 6400/10672]\n",
      "loss: 1.665718  [ 6500/10672]\n",
      "loss: 1.103248  [ 6600/10672]\n",
      "loss: 1.211737  [ 6700/10672]\n",
      "loss: 1.118719  [ 6800/10672]\n",
      "loss: 1.181239  [ 6900/10672]\n",
      "loss: 1.135848  [ 7000/10672]\n",
      "loss: 1.067253  [ 7100/10672]\n",
      "loss: 1.208820  [ 7200/10672]\n",
      "loss: 1.184328  [ 7300/10672]\n",
      "loss: 1.186247  [ 7400/10672]\n",
      "loss: 1.081813  [ 7500/10672]\n",
      "loss: 1.212380  [ 7600/10672]\n",
      "loss: 1.215508  [ 7700/10672]\n",
      "loss: 1.225196  [ 7800/10672]\n",
      "loss: 1.121464  [ 7900/10672]\n",
      "loss: 1.105511  [ 8000/10672]\n",
      "loss: 1.131251  [ 8100/10672]\n",
      "loss: 1.171728  [ 8200/10672]\n",
      "loss: 1.173349  [ 8300/10672]\n",
      "loss: 1.157152  [ 8400/10672]\n",
      "loss: 1.088513  [ 8500/10672]\n",
      "loss: 1.160206  [ 8600/10672]\n",
      "loss: 1.209425  [ 8700/10672]\n",
      "loss: 1.157030  [ 8800/10672]\n",
      "loss: 1.085860  [ 8900/10672]\n",
      "loss: 1.662844  [ 9000/10672]\n",
      "loss: 1.154745  [ 9100/10672]\n",
      "loss: 1.097679  [ 9200/10672]\n",
      "loss: 1.155601  [ 9300/10672]\n",
      "loss: 1.150513  [ 9400/10672]\n",
      "loss: 1.194019  [ 9500/10672]\n",
      "loss: 1.667534  [ 9600/10672]\n",
      "loss: 1.220495  [ 9700/10672]\n",
      "loss: 1.135319  [ 9800/10672]\n",
      "loss: 1.111899  [ 9900/10672]\n",
      "loss: 1.157853  [10000/10672]\n",
      "loss: 1.113132  [10100/10672]\n",
      "loss: 1.066059  [10200/10672]\n",
      "loss: 1.191967  [10300/10672]\n",
      "loss: 1.218203  [10400/10672]\n",
      "loss: 1.213779  [10500/10672]\n",
      "loss: 1.662453  [10600/10672]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.079344  [    0/10672]\n",
      "loss: 1.115321  [  100/10672]\n",
      "loss: 1.044077  [  200/10672]\n",
      "loss: 1.057703  [  300/10672]\n",
      "loss: 1.246280  [  400/10672]\n",
      "loss: 1.185071  [  500/10672]\n",
      "loss: 1.663993  [  600/10672]\n",
      "loss: 1.070346  [  700/10672]\n",
      "loss: 1.663021  [  800/10672]\n",
      "loss: 1.322021  [  900/10672]\n",
      "loss: 1.664426  [ 1000/10672]\n",
      "loss: 1.252451  [ 1100/10672]\n",
      "loss: 1.201010  [ 1200/10672]\n",
      "loss: 1.122720  [ 1300/10672]\n",
      "loss: 1.152219  [ 1400/10672]\n",
      "loss: 1.087802  [ 1500/10672]\n",
      "loss: 1.166677  [ 1600/10672]\n",
      "loss: 1.077483  [ 1700/10672]\n",
      "loss: 1.228126  [ 1800/10672]\n",
      "loss: 1.171459  [ 1900/10672]\n",
      "loss: 1.200801  [ 2000/10672]\n",
      "loss: 1.195762  [ 2100/10672]\n",
      "loss: 1.180429  [ 2200/10672]\n",
      "loss: 1.224053  [ 2300/10672]\n",
      "loss: 1.134884  [ 2400/10672]\n",
      "loss: 1.124955  [ 2500/10672]\n",
      "loss: 1.129859  [ 2600/10672]\n",
      "loss: 1.662591  [ 2700/10672]\n",
      "loss: 1.122961  [ 2800/10672]\n",
      "loss: 1.660254  [ 2900/10672]\n",
      "loss: 1.663341  [ 3000/10672]\n",
      "loss: 1.208536  [ 3100/10672]\n",
      "loss: 1.135676  [ 3200/10672]\n",
      "loss: 1.097412  [ 3300/10672]\n",
      "loss: 1.664937  [ 3400/10672]\n",
      "loss: 1.664325  [ 3500/10672]\n",
      "loss: 1.663495  [ 3600/10672]\n",
      "loss: 1.175483  [ 3700/10672]\n",
      "loss: 1.108569  [ 3800/10672]\n",
      "loss: 1.218791  [ 3900/10672]\n",
      "loss: 1.148800  [ 4000/10672]\n",
      "loss: 1.662626  [ 4100/10672]\n",
      "loss: 1.662096  [ 4200/10672]\n",
      "loss: 1.134747  [ 4300/10672]\n",
      "loss: 1.662219  [ 4400/10672]\n",
      "loss: 1.172624  [ 4500/10672]\n",
      "loss: 1.239980  [ 4600/10672]\n",
      "loss: 1.165986  [ 4700/10672]\n",
      "loss: 1.664061  [ 4800/10672]\n",
      "loss: 1.143515  [ 4900/10672]\n",
      "loss: 1.661366  [ 5000/10672]\n",
      "loss: 1.163631  [ 5100/10672]\n",
      "loss: 1.117796  [ 5200/10672]\n",
      "loss: 1.291899  [ 5300/10672]\n",
      "loss: 1.124518  [ 5400/10672]\n",
      "loss: 1.113006  [ 5500/10672]\n",
      "loss: 1.135458  [ 5600/10672]\n",
      "loss: 1.664197  [ 5700/10672]\n",
      "loss: 1.156051  [ 5800/10672]\n",
      "loss: 1.162561  [ 5900/10672]\n",
      "loss: 1.662317  [ 6000/10672]\n",
      "loss: 1.199438  [ 6100/10672]\n",
      "loss: 1.112191  [ 6200/10672]\n",
      "loss: 1.141031  [ 6300/10672]\n",
      "loss: 1.143823  [ 6400/10672]\n",
      "loss: 1.666003  [ 6500/10672]\n",
      "loss: 1.102447  [ 6600/10672]\n",
      "loss: 1.211304  [ 6700/10672]\n",
      "loss: 1.118213  [ 6800/10672]\n",
      "loss: 1.179552  [ 6900/10672]\n",
      "loss: 1.136137  [ 7000/10672]\n",
      "loss: 1.064851  [ 7100/10672]\n",
      "loss: 1.208420  [ 7200/10672]\n",
      "loss: 1.186423  [ 7300/10672]\n",
      "loss: 1.185239  [ 7400/10672]\n",
      "loss: 1.079467  [ 7500/10672]\n",
      "loss: 1.212006  [ 7600/10672]\n",
      "loss: 1.215578  [ 7700/10672]\n",
      "loss: 1.225588  [ 7800/10672]\n",
      "loss: 1.121270  [ 7900/10672]\n",
      "loss: 1.104797  [ 8000/10672]\n",
      "loss: 1.131130  [ 8100/10672]\n",
      "loss: 1.169466  [ 8200/10672]\n",
      "loss: 1.171272  [ 8300/10672]\n",
      "loss: 1.158140  [ 8400/10672]\n",
      "loss: 1.087157  [ 8500/10672]\n",
      "loss: 1.161167  [ 8600/10672]\n",
      "loss: 1.208967  [ 8700/10672]\n",
      "loss: 1.154772  [ 8800/10672]\n",
      "loss: 1.084299  [ 8900/10672]\n",
      "loss: 1.663036  [ 9000/10672]\n",
      "loss: 1.151680  [ 9100/10672]\n",
      "loss: 1.096353  [ 9200/10672]\n",
      "loss: 1.156822  [ 9300/10672]\n",
      "loss: 1.151764  [ 9400/10672]\n",
      "loss: 1.192327  [ 9500/10672]\n",
      "loss: 1.667770  [ 9600/10672]\n",
      "loss: 1.220095  [ 9700/10672]\n",
      "loss: 1.135978  [ 9800/10672]\n",
      "loss: 1.111649  [ 9900/10672]\n",
      "loss: 1.159459  [10000/10672]\n",
      "loss: 1.112862  [10100/10672]\n",
      "loss: 1.063930  [10200/10672]\n",
      "loss: 1.190338  [10300/10672]\n",
      "loss: 1.217331  [10400/10672]\n",
      "loss: 1.212815  [10500/10672]\n",
      "loss: 1.662769  [10600/10672]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.078090  [    0/10672]\n",
      "loss: 1.115189  [  100/10672]\n",
      "loss: 1.041065  [  200/10672]\n",
      "loss: 1.055363  [  300/10672]\n",
      "loss: 1.246523  [  400/10672]\n",
      "loss: 1.182909  [  500/10672]\n",
      "loss: 1.664176  [  600/10672]\n",
      "loss: 1.068926  [  700/10672]\n",
      "loss: 1.663235  [  800/10672]\n",
      "loss: 1.325445  [  900/10672]\n",
      "loss: 1.664589  [ 1000/10672]\n",
      "loss: 1.253031  [ 1100/10672]\n",
      "loss: 1.199520  [ 1200/10672]\n",
      "loss: 1.123078  [ 1300/10672]\n",
      "loss: 1.148806  [ 1400/10672]\n",
      "loss: 1.086660  [ 1500/10672]\n",
      "loss: 1.163892  [ 1600/10672]\n",
      "loss: 1.076165  [ 1700/10672]\n",
      "loss: 1.228203  [ 1800/10672]\n",
      "loss: 1.169079  [ 1900/10672]\n",
      "loss: 1.199597  [ 2000/10672]\n",
      "loss: 1.194449  [ 2100/10672]\n",
      "loss: 1.178527  [ 2200/10672]\n",
      "loss: 1.224221  [ 2300/10672]\n",
      "loss: 1.134987  [ 2400/10672]\n",
      "loss: 1.124737  [ 2500/10672]\n",
      "loss: 1.129757  [ 2600/10672]\n",
      "loss: 1.662787  [ 2700/10672]\n",
      "loss: 1.122562  [ 2800/10672]\n",
      "loss: 1.660608  [ 2900/10672]\n",
      "loss: 1.663582  [ 3000/10672]\n",
      "loss: 1.208539  [ 3100/10672]\n",
      "loss: 1.132530  [ 3200/10672]\n",
      "loss: 1.096138  [ 3300/10672]\n",
      "loss: 1.665249  [ 3400/10672]\n",
      "loss: 1.664547  [ 3500/10672]\n",
      "loss: 1.663705  [ 3600/10672]\n",
      "loss: 1.173715  [ 3700/10672]\n",
      "loss: 1.107960  [ 3800/10672]\n",
      "loss: 1.218563  [ 3900/10672]\n",
      "loss: 1.145519  [ 4000/10672]\n",
      "loss: 1.662848  [ 4100/10672]\n",
      "loss: 1.662329  [ 4200/10672]\n",
      "loss: 1.135057  [ 4300/10672]\n",
      "loss: 1.662442  [ 4400/10672]\n",
      "loss: 1.170659  [ 4500/10672]\n",
      "loss: 1.240940  [ 4600/10672]\n",
      "loss: 1.163821  [ 4700/10672]\n",
      "loss: 1.664304  [ 4800/10672]\n",
      "loss: 1.144053  [ 4900/10672]\n",
      "loss: 1.661632  [ 5000/10672]\n",
      "loss: 1.164966  [ 5100/10672]\n",
      "loss: 1.117167  [ 5200/10672]\n",
      "loss: 1.294991  [ 5300/10672]\n",
      "loss: 1.124497  [ 5400/10672]\n",
      "loss: 1.108841  [ 5500/10672]\n",
      "loss: 1.135719  [ 5600/10672]\n",
      "loss: 1.664443  [ 5700/10672]\n",
      "loss: 1.153636  [ 5800/10672]\n",
      "loss: 1.163753  [ 5900/10672]\n",
      "loss: 1.662560  [ 6000/10672]\n",
      "loss: 1.198898  [ 6100/10672]\n",
      "loss: 1.111113  [ 6200/10672]\n",
      "loss: 1.138257  [ 6300/10672]\n",
      "loss: 1.144585  [ 6400/10672]\n",
      "loss: 1.666273  [ 6500/10672]\n",
      "loss: 1.101654  [ 6600/10672]\n",
      "loss: 1.210896  [ 6700/10672]\n",
      "loss: 1.117707  [ 6800/10672]\n",
      "loss: 1.177889  [ 6900/10672]\n",
      "loss: 1.136426  [ 7000/10672]\n",
      "loss: 1.062471  [ 7100/10672]\n",
      "loss: 1.208041  [ 7200/10672]\n",
      "loss: 1.188509  [ 7300/10672]\n",
      "loss: 1.184258  [ 7400/10672]\n",
      "loss: 1.077144  [ 7500/10672]\n",
      "loss: 1.211650  [ 7600/10672]\n",
      "loss: 1.215665  [ 7700/10672]\n",
      "loss: 1.225991  [ 7800/10672]\n",
      "loss: 1.121076  [ 7900/10672]\n",
      "loss: 1.104089  [ 8000/10672]\n",
      "loss: 1.131009  [ 8100/10672]\n",
      "loss: 1.167239  [ 8200/10672]\n",
      "loss: 1.169228  [ 8300/10672]\n",
      "loss: 1.159131  [ 8400/10672]\n",
      "loss: 1.085816  [ 8500/10672]\n",
      "loss: 1.162136  [ 8600/10672]\n",
      "loss: 1.208538  [ 8700/10672]\n",
      "loss: 1.152552  [ 8800/10672]\n",
      "loss: 1.082750  [ 8900/10672]\n",
      "loss: 1.663212  [ 9000/10672]\n",
      "loss: 1.148650  [ 9100/10672]\n",
      "loss: 1.095036  [ 9200/10672]\n",
      "loss: 1.158044  [ 9300/10672]\n",
      "loss: 1.153012  [ 9400/10672]\n",
      "loss: 1.190663  [ 9500/10672]\n",
      "loss: 1.667996  [ 9600/10672]\n",
      "loss: 1.219712  [ 9700/10672]\n",
      "loss: 1.136639  [ 9800/10672]\n",
      "loss: 1.111404  [ 9900/10672]\n",
      "loss: 1.161066  [10000/10672]\n",
      "loss: 1.112592  [10100/10672]\n",
      "loss: 1.061822  [10200/10672]\n",
      "loss: 1.188734  [10300/10672]\n",
      "loss: 1.216479  [10400/10672]\n",
      "loss: 1.211870  [10500/10672]\n",
      "loss: 1.663063  [10600/10672]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.076853  [    0/10672]\n",
      "loss: 1.115054  [  100/10672]\n",
      "loss: 1.038088  [  200/10672]\n",
      "loss: 1.053046  [  300/10672]\n",
      "loss: 1.246776  [  400/10672]\n",
      "loss: 1.180777  [  500/10672]\n",
      "loss: 1.664345  [  600/10672]\n",
      "loss: 1.067516  [  700/10672]\n",
      "loss: 1.663432  [  800/10672]\n",
      "loss: 1.328856  [  900/10672]\n",
      "loss: 1.664738  [ 1000/10672]\n",
      "loss: 1.253620  [ 1100/10672]\n",
      "loss: 1.198050  [ 1200/10672]\n",
      "loss: 1.123433  [ 1300/10672]\n",
      "loss: 1.145436  [ 1400/10672]\n",
      "loss: 1.085529  [ 1500/10672]\n",
      "loss: 1.161139  [ 1600/10672]\n",
      "loss: 1.074858  [ 1700/10672]\n",
      "loss: 1.228297  [ 1800/10672]\n",
      "loss: 1.166732  [ 1900/10672]\n",
      "loss: 1.198412  [ 2000/10672]\n",
      "loss: 1.193164  [ 2100/10672]\n",
      "loss: 1.176654  [ 2200/10672]\n",
      "loss: 1.224405  [ 2300/10672]\n",
      "loss: 1.135089  [ 2400/10672]\n",
      "loss: 1.124520  [ 2500/10672]\n",
      "loss: 1.129662  [ 2600/10672]\n",
      "loss: 1.662966  [ 2700/10672]\n",
      "loss: 1.122164  [ 2800/10672]\n",
      "loss: 1.660938  [ 2900/10672]\n",
      "loss: 1.663806  [ 3000/10672]\n",
      "loss: 1.208563  [ 3100/10672]\n",
      "loss: 1.129431  [ 3200/10672]\n",
      "loss: 1.094869  [ 3300/10672]\n",
      "loss: 1.665546  [ 3400/10672]\n",
      "loss: 1.664754  [ 3500/10672]\n",
      "loss: 1.663904  [ 3600/10672]\n",
      "loss: 1.171974  [ 3700/10672]\n",
      "loss: 1.107355  [ 3800/10672]\n",
      "loss: 1.218351  [ 3900/10672]\n",
      "loss: 1.142279  [ 4000/10672]\n",
      "loss: 1.663053  [ 4100/10672]\n",
      "loss: 1.662549  [ 4200/10672]\n",
      "loss: 1.135361  [ 4300/10672]\n",
      "loss: 1.662647  [ 4400/10672]\n",
      "loss: 1.168722  [ 4500/10672]\n",
      "loss: 1.241916  [ 4600/10672]\n",
      "loss: 1.161692  [ 4700/10672]\n",
      "loss: 1.664531  [ 4800/10672]\n",
      "loss: 1.144591  [ 4900/10672]\n",
      "loss: 1.661877  [ 5000/10672]\n",
      "loss: 1.166297  [ 5100/10672]\n",
      "loss: 1.116535  [ 5200/10672]\n",
      "loss: 1.298084  [ 5300/10672]\n",
      "loss: 1.124474  [ 5400/10672]\n",
      "loss: 1.104735  [ 5500/10672]\n",
      "loss: 1.135971  [ 5600/10672]\n",
      "loss: 1.664673  [ 5700/10672]\n",
      "loss: 1.151252  [ 5800/10672]\n",
      "loss: 1.164938  [ 5900/10672]\n",
      "loss: 1.662786  [ 6000/10672]\n",
      "loss: 1.198381  [ 6100/10672]\n",
      "loss: 1.110046  [ 6200/10672]\n",
      "loss: 1.135528  [ 6300/10672]\n",
      "loss: 1.145349  [ 6400/10672]\n",
      "loss: 1.666531  [ 6500/10672]\n",
      "loss: 1.100867  [ 6600/10672]\n",
      "loss: 1.210507  [ 6700/10672]\n",
      "loss: 1.117201  [ 6800/10672]\n",
      "loss: 1.176248  [ 6900/10672]\n",
      "loss: 1.136716  [ 7000/10672]\n",
      "loss: 1.060113  [ 7100/10672]\n",
      "loss: 1.207680  [ 7200/10672]\n",
      "loss: 1.190587  [ 7300/10672]\n",
      "loss: 1.183303  [ 7400/10672]\n",
      "loss: 1.074843  [ 7500/10672]\n",
      "loss: 1.211312  [ 7600/10672]\n",
      "loss: 1.215768  [ 7700/10672]\n",
      "loss: 1.226404  [ 7800/10672]\n",
      "loss: 1.120878  [ 7900/10672]\n",
      "loss: 1.103386  [ 8000/10672]\n",
      "loss: 1.130885  [ 8100/10672]\n",
      "loss: 1.165044  [ 8200/10672]\n",
      "loss: 1.167215  [ 8300/10672]\n",
      "loss: 1.160122  [ 8400/10672]\n",
      "loss: 1.084488  [ 8500/10672]\n",
      "loss: 1.163111  [ 8600/10672]\n",
      "loss: 1.208135  [ 8700/10672]\n",
      "loss: 1.150369  [ 8800/10672]\n",
      "loss: 1.081213  [ 8900/10672]\n",
      "loss: 1.663375  [ 9000/10672]\n",
      "loss: 1.145655  [ 9100/10672]\n",
      "loss: 1.093727  [ 9200/10672]\n",
      "loss: 1.159263  [ 9300/10672]\n",
      "loss: 1.154253  [ 9400/10672]\n",
      "loss: 1.189027  [ 9500/10672]\n",
      "loss: 1.668213  [ 9600/10672]\n",
      "loss: 1.219342  [ 9700/10672]\n",
      "loss: 1.137297  [ 9800/10672]\n",
      "loss: 1.111161  [ 9900/10672]\n",
      "loss: 1.162669  [10000/10672]\n",
      "loss: 1.112319  [10100/10672]\n",
      "loss: 1.059730  [10200/10672]\n",
      "loss: 1.187158  [10300/10672]\n",
      "loss: 1.215650  [10400/10672]\n",
      "loss: 1.210945  [10500/10672]\n",
      "loss: 1.663339  [10600/10672]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.075629  [    0/10672]\n",
      "loss: 1.114913  [  100/10672]\n",
      "loss: 1.035143  [  200/10672]\n",
      "loss: 1.050750  [  300/10672]\n",
      "loss: 1.247041  [  400/10672]\n",
      "loss: 1.178677  [  500/10672]\n",
      "loss: 1.664499  [  600/10672]\n",
      "loss: 1.066112  [  700/10672]\n",
      "loss: 1.663612  [  800/10672]\n",
      "loss: 1.332257  [  900/10672]\n",
      "loss: 1.664874  [ 1000/10672]\n",
      "loss: 1.254221  [ 1100/10672]\n",
      "loss: 1.196598  [ 1200/10672]\n",
      "loss: 1.123784  [ 1300/10672]\n",
      "loss: 1.142109  [ 1400/10672]\n",
      "loss: 1.084407  [ 1500/10672]\n",
      "loss: 1.158418  [ 1600/10672]\n",
      "loss: 1.073557  [ 1700/10672]\n",
      "loss: 1.228408  [ 1800/10672]\n",
      "loss: 1.164419  [ 1900/10672]\n",
      "loss: 1.197250  [ 2000/10672]\n",
      "loss: 1.191906  [ 2100/10672]\n",
      "loss: 1.174811  [ 2200/10672]\n",
      "loss: 1.224606  [ 2300/10672]\n",
      "loss: 1.135187  [ 2400/10672]\n",
      "loss: 1.124299  [ 2500/10672]\n",
      "loss: 1.129571  [ 2600/10672]\n",
      "loss: 1.663131  [ 2700/10672]\n",
      "loss: 1.121763  [ 2800/10672]\n",
      "loss: 1.661246  [ 2900/10672]\n",
      "loss: 1.664014  [ 3000/10672]\n",
      "loss: 1.208608  [ 3100/10672]\n",
      "loss: 1.126377  [ 3200/10672]\n",
      "loss: 1.093605  [ 3300/10672]\n",
      "loss: 1.665829  [ 3400/10672]\n",
      "loss: 1.664948  [ 3500/10672]\n",
      "loss: 1.664093  [ 3600/10672]\n",
      "loss: 1.170259  [ 3700/10672]\n",
      "loss: 1.106751  [ 3800/10672]\n",
      "loss: 1.218153  [ 3900/10672]\n",
      "loss: 1.139079  [ 4000/10672]\n",
      "loss: 1.663242  [ 4100/10672]\n",
      "loss: 1.662756  [ 4200/10672]\n",
      "loss: 1.135661  [ 4300/10672]\n",
      "loss: 1.662836  [ 4400/10672]\n",
      "loss: 1.166810  [ 4500/10672]\n",
      "loss: 1.242901  [ 4600/10672]\n",
      "loss: 1.159595  [ 4700/10672]\n",
      "loss: 1.664744  [ 4800/10672]\n",
      "loss: 1.145132  [ 4900/10672]\n",
      "loss: 1.662104  [ 5000/10672]\n",
      "loss: 1.167624  [ 5100/10672]\n",
      "loss: 1.115901  [ 5200/10672]\n",
      "loss: 1.301176  [ 5300/10672]\n",
      "loss: 1.124448  [ 5400/10672]\n",
      "loss: 1.100686  [ 5500/10672]\n",
      "loss: 1.136216  [ 5600/10672]\n",
      "loss: 1.664891  [ 5700/10672]\n",
      "loss: 1.148897  [ 5800/10672]\n",
      "loss: 1.166117  [ 5900/10672]\n",
      "loss: 1.662996  [ 6000/10672]\n",
      "loss: 1.197886  [ 6100/10672]\n",
      "loss: 1.108989  [ 6200/10672]\n",
      "loss: 1.132842  [ 6300/10672]\n",
      "loss: 1.146112  [ 6400/10672]\n",
      "loss: 1.666778  [ 6500/10672]\n",
      "loss: 1.100088  [ 6600/10672]\n",
      "loss: 1.210137  [ 6700/10672]\n",
      "loss: 1.116694  [ 6800/10672]\n",
      "loss: 1.174627  [ 6900/10672]\n",
      "loss: 1.137004  [ 7000/10672]\n",
      "loss: 1.057779  [ 7100/10672]\n",
      "loss: 1.207335  [ 7200/10672]\n",
      "loss: 1.192655  [ 7300/10672]\n",
      "loss: 1.182369  [ 7400/10672]\n",
      "loss: 1.072564  [ 7500/10672]\n",
      "loss: 1.210988  [ 7600/10672]\n",
      "loss: 1.215883  [ 7700/10672]\n",
      "loss: 1.226824  [ 7800/10672]\n",
      "loss: 1.120679  [ 7900/10672]\n",
      "loss: 1.102689  [ 8000/10672]\n",
      "loss: 1.130760  [ 8100/10672]\n",
      "loss: 1.162879  [ 8200/10672]\n",
      "loss: 1.165231  [ 8300/10672]\n",
      "loss: 1.161115  [ 8400/10672]\n",
      "loss: 1.083176  [ 8500/10672]\n",
      "loss: 1.164092  [ 8600/10672]\n",
      "loss: 1.207757  [ 8700/10672]\n",
      "loss: 1.148220  [ 8800/10672]\n",
      "loss: 1.079688  [ 8900/10672]\n",
      "loss: 1.663525  [ 9000/10672]\n",
      "loss: 1.142691  [ 9100/10672]\n",
      "loss: 1.092426  [ 9200/10672]\n",
      "loss: 1.160482  [ 9300/10672]\n",
      "loss: 1.155489  [ 9400/10672]\n",
      "loss: 1.187415  [ 9500/10672]\n",
      "loss: 1.668423  [ 9600/10672]\n",
      "loss: 1.218985  [ 9700/10672]\n",
      "loss: 1.137955  [ 9800/10672]\n",
      "loss: 1.110922  [ 9900/10672]\n",
      "loss: 1.164269  [10000/10672]\n",
      "loss: 1.112042  [10100/10672]\n",
      "loss: 1.057657  [10200/10672]\n",
      "loss: 1.185607  [10300/10672]\n",
      "loss: 1.214839  [10400/10672]\n",
      "loss: 1.210038  [10500/10672]\n",
      "loss: 1.663598  [10600/10672]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.074419  [    0/10672]\n",
      "loss: 1.114767  [  100/10672]\n",
      "loss: 1.032231  [  200/10672]\n",
      "loss: 1.048474  [  300/10672]\n",
      "loss: 1.247315  [  400/10672]\n",
      "loss: 1.176606  [  500/10672]\n",
      "loss: 1.664642  [  600/10672]\n",
      "loss: 1.064716  [  700/10672]\n",
      "loss: 1.663779  [  800/10672]\n",
      "loss: 1.335643  [  900/10672]\n",
      "loss: 1.665000  [ 1000/10672]\n",
      "loss: 1.254830  [ 1100/10672]\n",
      "loss: 1.195165  [ 1200/10672]\n",
      "loss: 1.124130  [ 1300/10672]\n",
      "loss: 1.138824  [ 1400/10672]\n",
      "loss: 1.083293  [ 1500/10672]\n",
      "loss: 1.155729  [ 1600/10672]\n",
      "loss: 1.072264  [ 1700/10672]\n",
      "loss: 1.228535  [ 1800/10672]\n",
      "loss: 1.162138  [ 1900/10672]\n",
      "loss: 1.196107  [ 2000/10672]\n",
      "loss: 1.190676  [ 2100/10672]\n",
      "loss: 1.172995  [ 2200/10672]\n",
      "loss: 1.224821  [ 2300/10672]\n",
      "loss: 1.135281  [ 2400/10672]\n",
      "loss: 1.124076  [ 2500/10672]\n",
      "loss: 1.129484  [ 2600/10672]\n",
      "loss: 1.663283  [ 2700/10672]\n",
      "loss: 1.121359  [ 2800/10672]\n",
      "loss: 1.661536  [ 2900/10672]\n",
      "loss: 1.664209  [ 3000/10672]\n",
      "loss: 1.208673  [ 3100/10672]\n",
      "loss: 1.123368  [ 3200/10672]\n",
      "loss: 1.092344  [ 3300/10672]\n",
      "loss: 1.666101  [ 3400/10672]\n",
      "loss: 1.665132  [ 3500/10672]\n",
      "loss: 1.664274  [ 3600/10672]\n",
      "loss: 1.168570  [ 3700/10672]\n",
      "loss: 1.106147  [ 3800/10672]\n",
      "loss: 1.217968  [ 3900/10672]\n",
      "loss: 1.135919  [ 4000/10672]\n",
      "loss: 1.663418  [ 4100/10672]\n",
      "loss: 1.662953  [ 4200/10672]\n",
      "loss: 1.135952  [ 4300/10672]\n",
      "loss: 1.663010  [ 4400/10672]\n",
      "loss: 1.164923  [ 4500/10672]\n",
      "loss: 1.243900  [ 4600/10672]\n",
      "loss: 1.157532  [ 4700/10672]\n",
      "loss: 1.664946  [ 4800/10672]\n",
      "loss: 1.145671  [ 4900/10672]\n",
      "loss: 1.662315  [ 5000/10672]\n",
      "loss: 1.168944  [ 5100/10672]\n",
      "loss: 1.115263  [ 5200/10672]\n",
      "loss: 1.304265  [ 5300/10672]\n",
      "loss: 1.124418  [ 5400/10672]\n",
      "loss: 1.096694  [ 5500/10672]\n",
      "loss: 1.136450  [ 5600/10672]\n",
      "loss: 1.665096  [ 5700/10672]\n",
      "loss: 1.146571  [ 5800/10672]\n",
      "loss: 1.167286  [ 5900/10672]\n",
      "loss: 1.663193  [ 6000/10672]\n",
      "loss: 1.197412  [ 6100/10672]\n",
      "loss: 1.107939  [ 6200/10672]\n",
      "loss: 1.130202  [ 6300/10672]\n",
      "loss: 1.146871  [ 6400/10672]\n",
      "loss: 1.667016  [ 6500/10672]\n",
      "loss: 1.099312  [ 6600/10672]\n",
      "loss: 1.209787  [ 6700/10672]\n",
      "loss: 1.116185  [ 6800/10672]\n",
      "loss: 1.173026  [ 6900/10672]\n",
      "loss: 1.137291  [ 7000/10672]\n",
      "loss: 1.055464  [ 7100/10672]\n",
      "loss: 1.207007  [ 7200/10672]\n",
      "loss: 1.194711  [ 7300/10672]\n",
      "loss: 1.181459  [ 7400/10672]\n",
      "loss: 1.070306  [ 7500/10672]\n",
      "loss: 1.210680  [ 7600/10672]\n",
      "loss: 1.216012  [ 7700/10672]\n",
      "loss: 1.227251  [ 7800/10672]\n",
      "loss: 1.120476  [ 7900/10672]\n",
      "loss: 1.101994  [ 8000/10672]\n",
      "loss: 1.130629  [ 8100/10672]\n",
      "loss: 1.160747  [ 8200/10672]\n",
      "loss: 1.163278  [ 8300/10672]\n",
      "loss: 1.162105  [ 8400/10672]\n",
      "loss: 1.081874  [ 8500/10672]\n",
      "loss: 1.165074  [ 8600/10672]\n",
      "loss: 1.207403  [ 8700/10672]\n",
      "loss: 1.146106  [ 8800/10672]\n",
      "loss: 1.078172  [ 8900/10672]\n",
      "loss: 1.663665  [ 9000/10672]\n",
      "loss: 1.139760  [ 9100/10672]\n",
      "loss: 1.091131  [ 9200/10672]\n",
      "loss: 1.161695  [ 9300/10672]\n",
      "loss: 1.156717  [ 9400/10672]\n",
      "loss: 1.185829  [ 9500/10672]\n",
      "loss: 1.668625  [ 9600/10672]\n",
      "loss: 1.218641  [ 9700/10672]\n",
      "loss: 1.138610  [ 9800/10672]\n",
      "loss: 1.110684  [ 9900/10672]\n",
      "loss: 1.165864  [10000/10672]\n",
      "loss: 1.111761  [10100/10672]\n",
      "loss: 1.055600  [10200/10672]\n",
      "loss: 1.184080  [10300/10672]\n",
      "loss: 1.214048  [10400/10672]\n",
      "loss: 1.209149  [10500/10672]\n",
      "loss: 1.663842  [10600/10672]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.073220  [    0/10672]\n",
      "loss: 1.114614  [  100/10672]\n",
      "loss: 1.029350  [  200/10672]\n",
      "loss: 1.046219  [  300/10672]\n",
      "loss: 1.247598  [  400/10672]\n",
      "loss: 1.174563  [  500/10672]\n",
      "loss: 1.664774  [  600/10672]\n",
      "loss: 1.063326  [  700/10672]\n",
      "loss: 1.663933  [  800/10672]\n",
      "loss: 1.339014  [  900/10672]\n",
      "loss: 1.665116  [ 1000/10672]\n",
      "loss: 1.255447  [ 1100/10672]\n",
      "loss: 1.193749  [ 1200/10672]\n",
      "loss: 1.124469  [ 1300/10672]\n",
      "loss: 1.135580  [ 1400/10672]\n",
      "loss: 1.082187  [ 1500/10672]\n",
      "loss: 1.153069  [ 1600/10672]\n",
      "loss: 1.070977  [ 1700/10672]\n",
      "loss: 1.228675  [ 1800/10672]\n",
      "loss: 1.159885  [ 1900/10672]\n",
      "loss: 1.194982  [ 2000/10672]\n",
      "loss: 1.189469  [ 2100/10672]\n",
      "loss: 1.171206  [ 2200/10672]\n",
      "loss: 1.225049  [ 2300/10672]\n",
      "loss: 1.135372  [ 2400/10672]\n",
      "loss: 1.123850  [ 2500/10672]\n",
      "loss: 1.129400  [ 2600/10672]\n",
      "loss: 1.663424  [ 2700/10672]\n",
      "loss: 1.120954  [ 2800/10672]\n",
      "loss: 1.661810  [ 2900/10672]\n",
      "loss: 1.664392  [ 3000/10672]\n",
      "loss: 1.208755  [ 3100/10672]\n",
      "loss: 1.120401  [ 3200/10672]\n",
      "loss: 1.091089  [ 3300/10672]\n",
      "loss: 1.666363  [ 3400/10672]\n",
      "loss: 1.665305  [ 3500/10672]\n",
      "loss: 1.664447  [ 3600/10672]\n",
      "loss: 1.166902  [ 3700/10672]\n",
      "loss: 1.105546  [ 3800/10672]\n",
      "loss: 1.217794  [ 3900/10672]\n",
      "loss: 1.132794  [ 4000/10672]\n",
      "loss: 1.663581  [ 4100/10672]\n",
      "loss: 1.663142  [ 4200/10672]\n",
      "loss: 1.136239  [ 4300/10672]\n",
      "loss: 1.663172  [ 4400/10672]\n",
      "loss: 1.163059  [ 4500/10672]\n",
      "loss: 1.244907  [ 4600/10672]\n",
      "loss: 1.155499  [ 4700/10672]\n",
      "loss: 1.665137  [ 4800/10672]\n",
      "loss: 1.146210  [ 4900/10672]\n",
      "loss: 1.662511  [ 5000/10672]\n",
      "loss: 1.170261  [ 5100/10672]\n",
      "loss: 1.114621  [ 5200/10672]\n",
      "loss: 1.307348  [ 5300/10672]\n",
      "loss: 1.124386  [ 5400/10672]\n",
      "loss: 1.092757  [ 5500/10672]\n",
      "loss: 1.136677  [ 5600/10672]\n",
      "loss: 1.665291  [ 5700/10672]\n",
      "loss: 1.144270  [ 5800/10672]\n",
      "loss: 1.168448  [ 5900/10672]\n",
      "loss: 1.663376  [ 6000/10672]\n",
      "loss: 1.196956  [ 6100/10672]\n",
      "loss: 1.106898  [ 6200/10672]\n",
      "loss: 1.127600  [ 6300/10672]\n",
      "loss: 1.147631  [ 6400/10672]\n",
      "loss: 1.667245  [ 6500/10672]\n",
      "loss: 1.098543  [ 6600/10672]\n",
      "loss: 1.209453  [ 6700/10672]\n",
      "loss: 1.115676  [ 6800/10672]\n",
      "loss: 1.171442  [ 6900/10672]\n",
      "loss: 1.137576  [ 7000/10672]\n",
      "loss: 1.053171  [ 7100/10672]\n",
      "loss: 1.206692  [ 7200/10672]\n",
      "loss: 1.196756  [ 7300/10672]\n",
      "loss: 1.180569  [ 7400/10672]\n",
      "loss: 1.068069  [ 7500/10672]\n",
      "loss: 1.210384  [ 7600/10672]\n",
      "loss: 1.216150  [ 7700/10672]\n",
      "loss: 1.227684  [ 7800/10672]\n",
      "loss: 1.120269  [ 7900/10672]\n",
      "loss: 1.101303  [ 8000/10672]\n",
      "loss: 1.130495  [ 8100/10672]\n",
      "loss: 1.158643  [ 8200/10672]\n",
      "loss: 1.161352  [ 8300/10672]\n",
      "loss: 1.163092  [ 8400/10672]\n",
      "loss: 1.080584  [ 8500/10672]\n",
      "loss: 1.166058  [ 8600/10672]\n",
      "loss: 1.207073  [ 8700/10672]\n",
      "loss: 1.144025  [ 8800/10672]\n",
      "loss: 1.076666  [ 8900/10672]\n",
      "loss: 1.663796  [ 9000/10672]\n",
      "loss: 1.136860  [ 9100/10672]\n",
      "loss: 1.089841  [ 9200/10672]\n",
      "loss: 1.162904  [ 9300/10672]\n",
      "loss: 1.157937  [ 9400/10672]\n",
      "loss: 1.184267  [ 9500/10672]\n",
      "loss: 1.668822  [ 9600/10672]\n",
      "loss: 1.218307  [ 9700/10672]\n",
      "loss: 1.139262  [ 9800/10672]\n",
      "loss: 1.110447  [ 9900/10672]\n",
      "loss: 1.167452  [10000/10672]\n",
      "loss: 1.111476  [10100/10672]\n",
      "loss: 1.053560  [10200/10672]\n",
      "loss: 1.182576  [10300/10672]\n",
      "loss: 1.213274  [10400/10672]\n",
      "loss: 1.208275  [10500/10672]\n",
      "loss: 1.664073  [10600/10672]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.072035  [    0/10672]\n",
      "loss: 1.114455  [  100/10672]\n",
      "loss: 1.026500  [  200/10672]\n",
      "loss: 1.043984  [  300/10672]\n",
      "loss: 1.247887  [  400/10672]\n",
      "loss: 1.172547  [  500/10672]\n",
      "loss: 1.664897  [  600/10672]\n",
      "loss: 1.061942  [  700/10672]\n",
      "loss: 1.664076  [  800/10672]\n",
      "loss: 1.342370  [  900/10672]\n",
      "loss: 1.665223  [ 1000/10672]\n",
      "loss: 1.256070  [ 1100/10672]\n",
      "loss: 1.192348  [ 1200/10672]\n",
      "loss: 1.124803  [ 1300/10672]\n",
      "loss: 1.132375  [ 1400/10672]\n",
      "loss: 1.081088  [ 1500/10672]\n",
      "loss: 1.150439  [ 1600/10672]\n",
      "loss: 1.069696  [ 1700/10672]\n",
      "loss: 1.228830  [ 1800/10672]\n",
      "loss: 1.157663  [ 1900/10672]\n",
      "loss: 1.193875  [ 2000/10672]\n",
      "loss: 1.188287  [ 2100/10672]\n",
      "loss: 1.169442  [ 2200/10672]\n",
      "loss: 1.225289  [ 2300/10672]\n",
      "loss: 1.135457  [ 2400/10672]\n",
      "loss: 1.123619  [ 2500/10672]\n",
      "loss: 1.129318  [ 2600/10672]\n",
      "loss: 1.663556  [ 2700/10672]\n",
      "loss: 1.120546  [ 2800/10672]\n",
      "loss: 1.662068  [ 2900/10672]\n",
      "loss: 1.664564  [ 3000/10672]\n",
      "loss: 1.208855  [ 3100/10672]\n",
      "loss: 1.117477  [ 3200/10672]\n",
      "loss: 1.089837  [ 3300/10672]\n",
      "loss: 1.666615  [ 3400/10672]\n",
      "loss: 1.665470  [ 3500/10672]\n",
      "loss: 1.664614  [ 3600/10672]\n",
      "loss: 1.165259  [ 3700/10672]\n",
      "loss: 1.104945  [ 3800/10672]\n",
      "loss: 1.217632  [ 3900/10672]\n",
      "loss: 1.129707  [ 4000/10672]\n",
      "loss: 1.663733  [ 4100/10672]\n",
      "loss: 1.663322  [ 4200/10672]\n",
      "loss: 1.136517  [ 4300/10672]\n",
      "loss: 1.663322  [ 4400/10672]\n",
      "loss: 1.161217  [ 4500/10672]\n",
      "loss: 1.245923  [ 4600/10672]\n",
      "loss: 1.153496  [ 4700/10672]\n",
      "loss: 1.665318  [ 4800/10672]\n",
      "loss: 1.146747  [ 4900/10672]\n",
      "loss: 1.662695  [ 5000/10672]\n",
      "loss: 1.171572  [ 5100/10672]\n",
      "loss: 1.113975  [ 5200/10672]\n",
      "loss: 1.310426  [ 5300/10672]\n",
      "loss: 1.124349  [ 5400/10672]\n",
      "loss: 1.088873  [ 5500/10672]\n",
      "loss: 1.136894  [ 5600/10672]\n",
      "loss: 1.665476  [ 5700/10672]\n",
      "loss: 1.141996  [ 5800/10672]\n",
      "loss: 1.169601  [ 5900/10672]\n",
      "loss: 1.663550  [ 6000/10672]\n",
      "loss: 1.196518  [ 6100/10672]\n",
      "loss: 1.105864  [ 6200/10672]\n",
      "loss: 1.125039  [ 6300/10672]\n",
      "loss: 1.148389  [ 6400/10672]\n",
      "loss: 1.667468  [ 6500/10672]\n",
      "loss: 1.097780  [ 6600/10672]\n",
      "loss: 1.209133  [ 6700/10672]\n",
      "loss: 1.115166  [ 6800/10672]\n",
      "loss: 1.169874  [ 6900/10672]\n",
      "loss: 1.137861  [ 7000/10672]\n",
      "loss: 1.050901  [ 7100/10672]\n",
      "loss: 1.206388  [ 7200/10672]\n",
      "loss: 1.198790  [ 7300/10672]\n",
      "loss: 1.179697  [ 7400/10672]\n",
      "loss: 1.065851  [ 7500/10672]\n",
      "loss: 1.210099  [ 7600/10672]\n",
      "loss: 1.216298  [ 7700/10672]\n",
      "loss: 1.228120  [ 7800/10672]\n",
      "loss: 1.120060  [ 7900/10672]\n",
      "loss: 1.100616  [ 8000/10672]\n",
      "loss: 1.130358  [ 8100/10672]\n",
      "loss: 1.156566  [ 8200/10672]\n",
      "loss: 1.159451  [ 8300/10672]\n",
      "loss: 1.164079  [ 8400/10672]\n",
      "loss: 1.079307  [ 8500/10672]\n",
      "loss: 1.167044  [ 8600/10672]\n",
      "loss: 1.206762  [ 8700/10672]\n",
      "loss: 1.141974  [ 8800/10672]\n",
      "loss: 1.075171  [ 8900/10672]\n",
      "loss: 1.663918  [ 9000/10672]\n",
      "loss: 1.133987  [ 9100/10672]\n",
      "loss: 1.088561  [ 9200/10672]\n",
      "loss: 1.164110  [ 9300/10672]\n",
      "loss: 1.159152  [ 9400/10672]\n",
      "loss: 1.182725  [ 9500/10672]\n",
      "loss: 1.669014  [ 9600/10672]\n",
      "loss: 1.217981  [ 9700/10672]\n",
      "loss: 1.139913  [ 9800/10672]\n",
      "loss: 1.110213  [ 9900/10672]\n",
      "loss: 1.169036  [10000/10672]\n",
      "loss: 1.111188  [10100/10672]\n",
      "loss: 1.051537  [10200/10672]\n",
      "loss: 1.181092  [10300/10672]\n",
      "loss: 1.212515  [10400/10672]\n",
      "loss: 1.207415  [10500/10672]\n",
      "loss: 1.664292  [10600/10672]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.070863  [    0/10672]\n",
      "loss: 1.114290  [  100/10672]\n",
      "loss: 1.023683  [  200/10672]\n",
      "loss: 1.041769  [  300/10672]\n",
      "loss: 1.248182  [  400/10672]\n",
      "loss: 1.170555  [  500/10672]\n",
      "loss: 1.665011  [  600/10672]\n",
      "loss: 1.060566  [  700/10672]\n",
      "loss: 1.664208  [  800/10672]\n",
      "loss: 1.345705  [  900/10672]\n",
      "loss: 1.665323  [ 1000/10672]\n",
      "loss: 1.256696  [ 1100/10672]\n",
      "loss: 1.190960  [ 1200/10672]\n",
      "loss: 1.125133  [ 1300/10672]\n",
      "loss: 1.129206  [ 1400/10672]\n",
      "loss: 1.079998  [ 1500/10672]\n",
      "loss: 1.147835  [ 1600/10672]\n",
      "loss: 1.068423  [ 1700/10672]\n",
      "loss: 1.228994  [ 1800/10672]\n",
      "loss: 1.155467  [ 1900/10672]\n",
      "loss: 1.192782  [ 2000/10672]\n",
      "loss: 1.187126  [ 2100/10672]\n",
      "loss: 1.167700  [ 2200/10672]\n",
      "loss: 1.225538  [ 2300/10672]\n",
      "loss: 1.135539  [ 2400/10672]\n",
      "loss: 1.123388  [ 2500/10672]\n",
      "loss: 1.129241  [ 2600/10672]\n",
      "loss: 1.663678  [ 2700/10672]\n",
      "loss: 1.120137  [ 2800/10672]\n",
      "loss: 1.662313  [ 2900/10672]\n",
      "loss: 1.664726  [ 3000/10672]\n",
      "loss: 1.208967  [ 3100/10672]\n",
      "loss: 1.114591  [ 3200/10672]\n",
      "loss: 1.088591  [ 3300/10672]\n",
      "loss: 1.666860  [ 3400/10672]\n",
      "loss: 1.665626  [ 3500/10672]\n",
      "loss: 1.664777  [ 3600/10672]\n",
      "loss: 1.163634  [ 3700/10672]\n",
      "loss: 1.104347  [ 3800/10672]\n",
      "loss: 1.217476  [ 3900/10672]\n",
      "loss: 1.126653  [ 4000/10672]\n",
      "loss: 1.663875  [ 4100/10672]\n",
      "loss: 1.663495  [ 4200/10672]\n",
      "loss: 1.136790  [ 4300/10672]\n",
      "loss: 1.663462  [ 4400/10672]\n",
      "loss: 1.159395  [ 4500/10672]\n",
      "loss: 1.246945  [ 4600/10672]\n",
      "loss: 1.151520  [ 4700/10672]\n",
      "loss: 1.665490  [ 4800/10672]\n",
      "loss: 1.147285  [ 4900/10672]\n",
      "loss: 1.662866  [ 5000/10672]\n",
      "loss: 1.172878  [ 5100/10672]\n",
      "loss: 1.113326  [ 5200/10672]\n",
      "loss: 1.313495  [ 5300/10672]\n",
      "loss: 1.124309  [ 5400/10672]\n",
      "loss: 1.085041  [ 5500/10672]\n",
      "loss: 1.137104  [ 5600/10672]\n",
      "loss: 1.665653  [ 5700/10672]\n",
      "loss: 1.139744  [ 5800/10672]\n",
      "loss: 1.170747  [ 5900/10672]\n",
      "loss: 1.663713  [ 6000/10672]\n",
      "loss: 1.196095  [ 6100/10672]\n",
      "loss: 1.104838  [ 6200/10672]\n",
      "loss: 1.122517  [ 6300/10672]\n",
      "loss: 1.149145  [ 6400/10672]\n",
      "loss: 1.667684  [ 6500/10672]\n",
      "loss: 1.097023  [ 6600/10672]\n",
      "loss: 1.208827  [ 6700/10672]\n",
      "loss: 1.114656  [ 6800/10672]\n",
      "loss: 1.168321  [ 6900/10672]\n",
      "loss: 1.138145  [ 7000/10672]\n",
      "loss: 1.048652  [ 7100/10672]\n",
      "loss: 1.206096  [ 7200/10672]\n",
      "loss: 1.200813  [ 7300/10672]\n",
      "loss: 1.178843  [ 7400/10672]\n",
      "loss: 1.063655  [ 7500/10672]\n",
      "loss: 1.209824  [ 7600/10672]\n",
      "loss: 1.216453  [ 7700/10672]\n",
      "loss: 1.228559  [ 7800/10672]\n",
      "loss: 1.119848  [ 7900/10672]\n",
      "loss: 1.099933  [ 8000/10672]\n",
      "loss: 1.130216  [ 8100/10672]\n",
      "loss: 1.154515  [ 8200/10672]\n",
      "loss: 1.157575  [ 8300/10672]\n",
      "loss: 1.165063  [ 8400/10672]\n",
      "loss: 1.078041  [ 8500/10672]\n",
      "loss: 1.168031  [ 8600/10672]\n",
      "loss: 1.206472  [ 8700/10672]\n",
      "loss: 1.139953  [ 8800/10672]\n",
      "loss: 1.073686  [ 8900/10672]\n",
      "loss: 1.664032  [ 9000/10672]\n",
      "loss: 1.131142  [ 9100/10672]\n",
      "loss: 1.087286  [ 9200/10672]\n",
      "loss: 1.165311  [ 9300/10672]\n",
      "loss: 1.160359  [ 9400/10672]\n",
      "loss: 1.181205  [ 9500/10672]\n",
      "loss: 1.669201  [ 9600/10672]\n",
      "loss: 1.217663  [ 9700/10672]\n",
      "loss: 1.140561  [ 9800/10672]\n",
      "loss: 1.109980  [ 9900/10672]\n",
      "loss: 1.170613  [10000/10672]\n",
      "loss: 1.110895  [10100/10672]\n",
      "loss: 1.049530  [10200/10672]\n",
      "loss: 1.179630  [10300/10672]\n",
      "loss: 1.211772  [10400/10672]\n",
      "loss: 1.206570  [10500/10672]\n",
      "loss: 1.664500  [10600/10672]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.069703  [    0/10672]\n",
      "loss: 1.114118  [  100/10672]\n",
      "loss: 1.020895  [  200/10672]\n",
      "loss: 1.039573  [  300/10672]\n",
      "loss: 1.248482  [  400/10672]\n",
      "loss: 1.168588  [  500/10672]\n",
      "loss: 1.665117  [  600/10672]\n",
      "loss: 1.059195  [  700/10672]\n",
      "loss: 1.664332  [  800/10672]\n",
      "loss: 1.349023  [  900/10672]\n",
      "loss: 1.665416  [ 1000/10672]\n",
      "loss: 1.257326  [ 1100/10672]\n",
      "loss: 1.189586  [ 1200/10672]\n",
      "loss: 1.125456  [ 1300/10672]\n",
      "loss: 1.126076  [ 1400/10672]\n",
      "loss: 1.078914  [ 1500/10672]\n",
      "loss: 1.145258  [ 1600/10672]\n",
      "loss: 1.067156  [ 1700/10672]\n",
      "loss: 1.229170  [ 1800/10672]\n",
      "loss: 1.153298  [ 1900/10672]\n",
      "loss: 1.191704  [ 2000/10672]\n",
      "loss: 1.185987  [ 2100/10672]\n",
      "loss: 1.165983  [ 2200/10672]\n",
      "loss: 1.225797  [ 2300/10672]\n",
      "loss: 1.135615  [ 2400/10672]\n",
      "loss: 1.123152  [ 2500/10672]\n",
      "loss: 1.129165  [ 2600/10672]\n",
      "loss: 1.663793  [ 2700/10672]\n",
      "loss: 1.119724  [ 2800/10672]\n",
      "loss: 1.662546  [ 2900/10672]\n",
      "loss: 1.664880  [ 3000/10672]\n",
      "loss: 1.209096  [ 3100/10672]\n",
      "loss: 1.111746  [ 3200/10672]\n",
      "loss: 1.087348  [ 3300/10672]\n",
      "loss: 1.667098  [ 3400/10672]\n",
      "loss: 1.665775  [ 3500/10672]\n",
      "loss: 1.664934  [ 3600/10672]\n",
      "loss: 1.162031  [ 3700/10672]\n",
      "loss: 1.103747  [ 3800/10672]\n",
      "loss: 1.217331  [ 3900/10672]\n",
      "loss: 1.123633  [ 4000/10672]\n",
      "loss: 1.664008  [ 4100/10672]\n",
      "loss: 1.663662  [ 4200/10672]\n",
      "loss: 1.137055  [ 4300/10672]\n",
      "loss: 1.663593  [ 4400/10672]\n",
      "loss: 1.157593  [ 4500/10672]\n",
      "loss: 1.247973  [ 4600/10672]\n",
      "loss: 1.149572  [ 4700/10672]\n",
      "loss: 1.665655  [ 4800/10672]\n",
      "loss: 1.147819  [ 4900/10672]\n",
      "loss: 1.663027  [ 5000/10672]\n",
      "loss: 1.174178  [ 5100/10672]\n",
      "loss: 1.112672  [ 5200/10672]\n",
      "loss: 1.316556  [ 5300/10672]\n",
      "loss: 1.124264  [ 5400/10672]\n",
      "loss: 1.081262  [ 5500/10672]\n",
      "loss: 1.137303  [ 5600/10672]\n",
      "loss: 1.665823  [ 5700/10672]\n",
      "loss: 1.137516  [ 5800/10672]\n",
      "loss: 1.171884  [ 5900/10672]\n",
      "loss: 1.663867  [ 6000/10672]\n",
      "loss: 1.195689  [ 6100/10672]\n",
      "loss: 1.103819  [ 6200/10672]\n",
      "loss: 1.120031  [ 6300/10672]\n",
      "loss: 1.149898  [ 6400/10672]\n",
      "loss: 1.667894  [ 6500/10672]\n",
      "loss: 1.096269  [ 6600/10672]\n",
      "loss: 1.208536  [ 6700/10672]\n",
      "loss: 1.114144  [ 6800/10672]\n",
      "loss: 1.166783  [ 6900/10672]\n",
      "loss: 1.138426  [ 7000/10672]\n",
      "loss: 1.046423  [ 7100/10672]\n",
      "loss: 1.205815  [ 7200/10672]\n",
      "loss: 1.202821  [ 7300/10672]\n",
      "loss: 1.178007  [ 7400/10672]\n",
      "loss: 1.061478  [ 7500/10672]\n",
      "loss: 1.209559  [ 7600/10672]\n",
      "loss: 1.216616  [ 7700/10672]\n",
      "loss: 1.229000  [ 7800/10672]\n",
      "loss: 1.119631  [ 7900/10672]\n",
      "loss: 1.099253  [ 8000/10672]\n",
      "loss: 1.130069  [ 8100/10672]\n",
      "loss: 1.152491  [ 8200/10672]\n",
      "loss: 1.155723  [ 8300/10672]\n",
      "loss: 1.166044  [ 8400/10672]\n",
      "loss: 1.076786  [ 8500/10672]\n",
      "loss: 1.169018  [ 8600/10672]\n",
      "loss: 1.206200  [ 8700/10672]\n",
      "loss: 1.137961  [ 8800/10672]\n",
      "loss: 1.072212  [ 8900/10672]\n",
      "loss: 1.664140  [ 9000/10672]\n",
      "loss: 1.128324  [ 9100/10672]\n",
      "loss: 1.086019  [ 9200/10672]\n",
      "loss: 1.166507  [ 9300/10672]\n",
      "loss: 1.161558  [ 9400/10672]\n",
      "loss: 1.179704  [ 9500/10672]\n",
      "loss: 1.669384  [ 9600/10672]\n",
      "loss: 1.217352  [ 9700/10672]\n",
      "loss: 1.141206  [ 9800/10672]\n",
      "loss: 1.109748  [ 9900/10672]\n",
      "loss: 1.172183  [10000/10672]\n",
      "loss: 1.110598  [10100/10672]\n",
      "loss: 1.047538  [10200/10672]\n",
      "loss: 1.178187  [10300/10672]\n",
      "loss: 1.211043  [10400/10672]\n",
      "loss: 1.205738  [10500/10672]\n",
      "loss: 1.664698  [10600/10672]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.068553  [    0/10672]\n",
      "loss: 1.113939  [  100/10672]\n",
      "loss: 1.018138  [  200/10672]\n",
      "loss: 1.037396  [  300/10672]\n",
      "loss: 1.248786  [  400/10672]\n",
      "loss: 1.166645  [  500/10672]\n",
      "loss: 1.665216  [  600/10672]\n",
      "loss: 1.057830  [  700/10672]\n",
      "loss: 1.664447  [  800/10672]\n",
      "loss: 1.352322  [  900/10672]\n",
      "loss: 1.665502  [ 1000/10672]\n",
      "loss: 1.257958  [ 1100/10672]\n",
      "loss: 1.188225  [ 1200/10672]\n",
      "loss: 1.125772  [ 1300/10672]\n",
      "loss: 1.122981  [ 1400/10672]\n",
      "loss: 1.077838  [ 1500/10672]\n",
      "loss: 1.142707  [ 1600/10672]\n",
      "loss: 1.065894  [ 1700/10672]\n",
      "loss: 1.229356  [ 1800/10672]\n",
      "loss: 1.151155  [ 1900/10672]\n",
      "loss: 1.190640  [ 2000/10672]\n",
      "loss: 1.184870  [ 2100/10672]\n",
      "loss: 1.164288  [ 2200/10672]\n",
      "loss: 1.226065  [ 2300/10672]\n",
      "loss: 1.135686  [ 2400/10672]\n",
      "loss: 1.122912  [ 2500/10672]\n",
      "loss: 1.129090  [ 2600/10672]\n",
      "loss: 1.663900  [ 2700/10672]\n",
      "loss: 1.119308  [ 2800/10672]\n",
      "loss: 1.662767  [ 2900/10672]\n",
      "loss: 1.665026  [ 3000/10672]\n",
      "loss: 1.209239  [ 3100/10672]\n",
      "loss: 1.108940  [ 3200/10672]\n",
      "loss: 1.086108  [ 3300/10672]\n",
      "loss: 1.667330  [ 3400/10672]\n",
      "loss: 1.665919  [ 3500/10672]\n",
      "loss: 1.665087  [ 3600/10672]\n",
      "loss: 1.160448  [ 3700/10672]\n",
      "loss: 1.103148  [ 3800/10672]\n",
      "loss: 1.217194  [ 3900/10672]\n",
      "loss: 1.120647  [ 4000/10672]\n",
      "loss: 1.664134  [ 4100/10672]\n",
      "loss: 1.663824  [ 4200/10672]\n",
      "loss: 1.137311  [ 4300/10672]\n",
      "loss: 1.663715  [ 4400/10672]\n",
      "loss: 1.155813  [ 4500/10672]\n",
      "loss: 1.249008  [ 4600/10672]\n",
      "loss: 1.147652  [ 4700/10672]\n",
      "loss: 1.665812  [ 4800/10672]\n",
      "loss: 1.148350  [ 4900/10672]\n",
      "loss: 1.663179  [ 5000/10672]\n",
      "loss: 1.175470  [ 5100/10672]\n",
      "loss: 1.112014  [ 5200/10672]\n",
      "loss: 1.319608  [ 5300/10672]\n",
      "loss: 1.124213  [ 5400/10672]\n",
      "loss: 1.077534  [ 5500/10672]\n",
      "loss: 1.137493  [ 5600/10672]\n",
      "loss: 1.665986  [ 5700/10672]\n",
      "loss: 1.135311  [ 5800/10672]\n",
      "loss: 1.173011  [ 5900/10672]\n",
      "loss: 1.664013  [ 6000/10672]\n",
      "loss: 1.195298  [ 6100/10672]\n",
      "loss: 1.102805  [ 6200/10672]\n",
      "loss: 1.117584  [ 6300/10672]\n",
      "loss: 1.150647  [ 6400/10672]\n",
      "loss: 1.668100  [ 6500/10672]\n",
      "loss: 1.095520  [ 6600/10672]\n",
      "loss: 1.208258  [ 6700/10672]\n",
      "loss: 1.113630  [ 6800/10672]\n",
      "loss: 1.165260  [ 6900/10672]\n",
      "loss: 1.138706  [ 7000/10672]\n",
      "loss: 1.044215  [ 7100/10672]\n",
      "loss: 1.205545  [ 7200/10672]\n",
      "loss: 1.204817  [ 7300/10672]\n",
      "loss: 1.177188  [ 7400/10672]\n",
      "loss: 1.059320  [ 7500/10672]\n",
      "loss: 1.209304  [ 7600/10672]\n",
      "loss: 1.216787  [ 7700/10672]\n",
      "loss: 1.229444  [ 7800/10672]\n",
      "loss: 1.119410  [ 7900/10672]\n",
      "loss: 1.098575  [ 8000/10672]\n",
      "loss: 1.129917  [ 8100/10672]\n",
      "loss: 1.150493  [ 8200/10672]\n",
      "loss: 1.153895  [ 8300/10672]\n",
      "loss: 1.167019  [ 8400/10672]\n",
      "loss: 1.075540  [ 8500/10672]\n",
      "loss: 1.170002  [ 8600/10672]\n",
      "loss: 1.205948  [ 8700/10672]\n",
      "loss: 1.135998  [ 8800/10672]\n",
      "loss: 1.070746  [ 8900/10672]\n",
      "loss: 1.664242  [ 9000/10672]\n",
      "loss: 1.125535  [ 9100/10672]\n",
      "loss: 1.084755  [ 9200/10672]\n",
      "loss: 1.167696  [ 9300/10672]\n",
      "loss: 1.162747  [ 9400/10672]\n",
      "loss: 1.178224  [ 9500/10672]\n",
      "loss: 1.669564  [ 9600/10672]\n",
      "loss: 1.217049  [ 9700/10672]\n",
      "loss: 1.141846  [ 9800/10672]\n",
      "loss: 1.109516  [ 9900/10672]\n",
      "loss: 1.173745  [10000/10672]\n",
      "loss: 1.110295  [10100/10672]\n",
      "loss: 1.045563  [10200/10672]\n",
      "loss: 1.176764  [10300/10672]\n",
      "loss: 1.210329  [10400/10672]\n",
      "loss: 1.204918  [10500/10672]\n",
      "loss: 1.664888  [10600/10672]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.067417  [    0/10672]\n",
      "loss: 1.113753  [  100/10672]\n",
      "loss: 1.015411  [  200/10672]\n",
      "loss: 1.035239  [  300/10672]\n",
      "loss: 1.249093  [  400/10672]\n",
      "loss: 1.164724  [  500/10672]\n",
      "loss: 1.665309  [  600/10672]\n",
      "loss: 1.056473  [  700/10672]\n",
      "loss: 1.664555  [  800/10672]\n",
      "loss: 1.355599  [  900/10672]\n",
      "loss: 1.665582  [ 1000/10672]\n",
      "loss: 1.258592  [ 1100/10672]\n",
      "loss: 1.186875  [ 1200/10672]\n",
      "loss: 1.126083  [ 1300/10672]\n",
      "loss: 1.119921  [ 1400/10672]\n",
      "loss: 1.076769  [ 1500/10672]\n",
      "loss: 1.140181  [ 1600/10672]\n",
      "loss: 1.064640  [ 1700/10672]\n",
      "loss: 1.229549  [ 1800/10672]\n",
      "loss: 1.149036  [ 1900/10672]\n",
      "loss: 1.189587  [ 2000/10672]\n",
      "loss: 1.183771  [ 2100/10672]\n",
      "loss: 1.162614  [ 2200/10672]\n",
      "loss: 1.226340  [ 2300/10672]\n",
      "loss: 1.135754  [ 2400/10672]\n",
      "loss: 1.122671  [ 2500/10672]\n",
      "loss: 1.129019  [ 2600/10672]\n",
      "loss: 1.664000  [ 2700/10672]\n",
      "loss: 1.118891  [ 2800/10672]\n",
      "loss: 1.662979  [ 2900/10672]\n",
      "loss: 1.665164  [ 3000/10672]\n",
      "loss: 1.209393  [ 3100/10672]\n",
      "loss: 1.106171  [ 3200/10672]\n",
      "loss: 1.084875  [ 3300/10672]\n",
      "loss: 1.667557  [ 3400/10672]\n",
      "loss: 1.666056  [ 3500/10672]\n",
      "loss: 1.665237  [ 3600/10672]\n",
      "loss: 1.158881  [ 3700/10672]\n",
      "loss: 1.102552  [ 3800/10672]\n",
      "loss: 1.217062  [ 3900/10672]\n",
      "loss: 1.117692  [ 4000/10672]\n",
      "loss: 1.664251  [ 4100/10672]\n",
      "loss: 1.663981  [ 4200/10672]\n",
      "loss: 1.137561  [ 4300/10672]\n",
      "loss: 1.663830  [ 4400/10672]\n",
      "loss: 1.154049  [ 4500/10672]\n",
      "loss: 1.250046  [ 4600/10672]\n",
      "loss: 1.145756  [ 4700/10672]\n",
      "loss: 1.665964  [ 4800/10672]\n",
      "loss: 1.148880  [ 4900/10672]\n",
      "loss: 1.663321  [ 5000/10672]\n",
      "loss: 1.176759  [ 5100/10672]\n",
      "loss: 1.111353  [ 5200/10672]\n",
      "loss: 1.322648  [ 5300/10672]\n",
      "loss: 1.124160  [ 5400/10672]\n",
      "loss: 1.073856  [ 5500/10672]\n",
      "loss: 1.137675  [ 5600/10672]\n",
      "loss: 1.666142  [ 5700/10672]\n",
      "loss: 1.133128  [ 5800/10672]\n",
      "loss: 1.174130  [ 5900/10672]\n",
      "loss: 1.664152  [ 6000/10672]\n",
      "loss: 1.194920  [ 6100/10672]\n",
      "loss: 1.101799  [ 6200/10672]\n",
      "loss: 1.115172  [ 6300/10672]\n",
      "loss: 1.151395  [ 6400/10672]\n",
      "loss: 1.668301  [ 6500/10672]\n",
      "loss: 1.094777  [ 6600/10672]\n",
      "loss: 1.207992  [ 6700/10672]\n",
      "loss: 1.113115  [ 6800/10672]\n",
      "loss: 1.163749  [ 6900/10672]\n",
      "loss: 1.138984  [ 7000/10672]\n",
      "loss: 1.042027  [ 7100/10672]\n",
      "loss: 1.205283  [ 7200/10672]\n",
      "loss: 1.206800  [ 7300/10672]\n",
      "loss: 1.176384  [ 7400/10672]\n",
      "loss: 1.057183  [ 7500/10672]\n",
      "loss: 1.209056  [ 7600/10672]\n",
      "loss: 1.216962  [ 7700/10672]\n",
      "loss: 1.229889  [ 7800/10672]\n",
      "loss: 1.119186  [ 7900/10672]\n",
      "loss: 1.097899  [ 8000/10672]\n",
      "loss: 1.129760  [ 8100/10672]\n",
      "loss: 1.148519  [ 8200/10672]\n",
      "loss: 1.152090  [ 8300/10672]\n",
      "loss: 1.167991  [ 8400/10672]\n",
      "loss: 1.074306  [ 8500/10672]\n",
      "loss: 1.170986  [ 8600/10672]\n",
      "loss: 1.205713  [ 8700/10672]\n",
      "loss: 1.134063  [ 8800/10672]\n",
      "loss: 1.069289  [ 8900/10672]\n",
      "loss: 1.664338  [ 9000/10672]\n",
      "loss: 1.122770  [ 9100/10672]\n",
      "loss: 1.083498  [ 9200/10672]\n",
      "loss: 1.168880  [ 9300/10672]\n",
      "loss: 1.163929  [ 9400/10672]\n",
      "loss: 1.176762  [ 9500/10672]\n",
      "loss: 1.669740  [ 9600/10672]\n",
      "loss: 1.216751  [ 9700/10672]\n",
      "loss: 1.142484  [ 9800/10672]\n",
      "loss: 1.109285  [ 9900/10672]\n",
      "loss: 1.175300  [10000/10672]\n",
      "loss: 1.109989  [10100/10672]\n",
      "loss: 1.043602  [10200/10672]\n",
      "loss: 1.175359  [10300/10672]\n",
      "loss: 1.209628  [10400/10672]\n",
      "loss: 1.204111  [10500/10672]\n",
      "loss: 1.665069  [10600/10672]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.066291  [    0/10672]\n",
      "loss: 1.113561  [  100/10672]\n",
      "loss: 1.012714  [  200/10672]\n",
      "loss: 1.033099  [  300/10672]\n",
      "loss: 1.249404  [  400/10672]\n",
      "loss: 1.162826  [  500/10672]\n",
      "loss: 1.665396  [  600/10672]\n",
      "loss: 1.055120  [  700/10672]\n",
      "loss: 1.664657  [  800/10672]\n",
      "loss: 1.358856  [  900/10672]\n",
      "loss: 1.665658  [ 1000/10672]\n",
      "loss: 1.259228  [ 1100/10672]\n",
      "loss: 1.185537  [ 1200/10672]\n",
      "loss: 1.126387  [ 1300/10672]\n",
      "loss: 1.116897  [ 1400/10672]\n",
      "loss: 1.075706  [ 1500/10672]\n",
      "loss: 1.137681  [ 1600/10672]\n",
      "loss: 1.063390  [ 1700/10672]\n",
      "loss: 1.229753  [ 1800/10672]\n",
      "loss: 1.146942  [ 1900/10672]\n",
      "loss: 1.188548  [ 2000/10672]\n",
      "loss: 1.182692  [ 2100/10672]\n",
      "loss: 1.160961  [ 2200/10672]\n",
      "loss: 1.226622  [ 2300/10672]\n",
      "loss: 1.135814  [ 2400/10672]\n",
      "loss: 1.122425  [ 2500/10672]\n",
      "loss: 1.128948  [ 2600/10672]\n",
      "loss: 1.664094  [ 2700/10672]\n",
      "loss: 1.118470  [ 2800/10672]\n",
      "loss: 1.663182  [ 2900/10672]\n",
      "loss: 1.665296  [ 3000/10672]\n",
      "loss: 1.209559  [ 3100/10672]\n",
      "loss: 1.103440  [ 3200/10672]\n",
      "loss: 1.083645  [ 3300/10672]\n",
      "loss: 1.667778  [ 3400/10672]\n",
      "loss: 1.666188  [ 3500/10672]\n",
      "loss: 1.665383  [ 3600/10672]\n",
      "loss: 1.157333  [ 3700/10672]\n",
      "loss: 1.101955  [ 3800/10672]\n",
      "loss: 1.216937  [ 3900/10672]\n",
      "loss: 1.114769  [ 4000/10672]\n",
      "loss: 1.664363  [ 4100/10672]\n",
      "loss: 1.664135  [ 4200/10672]\n",
      "loss: 1.137802  [ 4300/10672]\n",
      "loss: 1.663937  [ 4400/10672]\n",
      "loss: 1.152304  [ 4500/10672]\n",
      "loss: 1.251088  [ 4600/10672]\n",
      "loss: 1.143885  [ 4700/10672]\n",
      "loss: 1.666110  [ 4800/10672]\n",
      "loss: 1.149407  [ 4900/10672]\n",
      "loss: 1.663455  [ 5000/10672]\n",
      "loss: 1.178041  [ 5100/10672]\n",
      "loss: 1.110687  [ 5200/10672]\n",
      "loss: 1.325678  [ 5300/10672]\n",
      "loss: 1.124102  [ 5400/10672]\n",
      "loss: 1.070227  [ 5500/10672]\n",
      "loss: 1.137848  [ 5600/10672]\n",
      "loss: 1.666294  [ 5700/10672]\n",
      "loss: 1.130965  [ 5800/10672]\n",
      "loss: 1.175240  [ 5900/10672]\n",
      "loss: 1.664285  [ 6000/10672]\n",
      "loss: 1.194554  [ 6100/10672]\n",
      "loss: 1.100798  [ 6200/10672]\n",
      "loss: 1.112795  [ 6300/10672]\n",
      "loss: 1.152140  [ 6400/10672]\n",
      "loss: 1.668498  [ 6500/10672]\n",
      "loss: 1.094038  [ 6600/10672]\n",
      "loss: 1.207736  [ 6700/10672]\n",
      "loss: 1.112601  [ 6800/10672]\n",
      "loss: 1.162250  [ 6900/10672]\n",
      "loss: 1.139261  [ 7000/10672]\n",
      "loss: 1.039861  [ 7100/10672]\n",
      "loss: 1.205029  [ 7200/10672]\n",
      "loss: 1.208769  [ 7300/10672]\n",
      "loss: 1.175594  [ 7400/10672]\n",
      "loss: 1.055064  [ 7500/10672]\n",
      "loss: 1.208815  [ 7600/10672]\n",
      "loss: 1.217143  [ 7700/10672]\n",
      "loss: 1.230333  [ 7800/10672]\n",
      "loss: 1.118958  [ 7900/10672]\n",
      "loss: 1.097227  [ 8000/10672]\n",
      "loss: 1.129598  [ 8100/10672]\n",
      "loss: 1.146568  [ 8200/10672]\n",
      "loss: 1.150306  [ 8300/10672]\n",
      "loss: 1.168960  [ 8400/10672]\n",
      "loss: 1.073082  [ 8500/10672]\n",
      "loss: 1.171968  [ 8600/10672]\n",
      "loss: 1.205494  [ 8700/10672]\n",
      "loss: 1.132154  [ 8800/10672]\n",
      "loss: 1.067843  [ 8900/10672]\n",
      "loss: 1.664429  [ 9000/10672]\n",
      "loss: 1.120030  [ 9100/10672]\n",
      "loss: 1.082248  [ 9200/10672]\n",
      "loss: 1.170058  [ 9300/10672]\n",
      "loss: 1.165104  [ 9400/10672]\n",
      "loss: 1.175318  [ 9500/10672]\n",
      "loss: 1.669913  [ 9600/10672]\n",
      "loss: 1.216457  [ 9700/10672]\n",
      "loss: 1.143119  [ 9800/10672]\n",
      "loss: 1.109056  [ 9900/10672]\n",
      "loss: 1.176847  [10000/10672]\n",
      "loss: 1.109679  [10100/10672]\n",
      "loss: 1.041659  [10200/10672]\n",
      "loss: 1.173971  [10300/10672]\n",
      "loss: 1.208939  [10400/10672]\n",
      "loss: 1.203314  [10500/10672]\n",
      "loss: 1.665243  [10600/10672]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.065178  [    0/10672]\n",
      "loss: 1.113362  [  100/10672]\n",
      "loss: 1.010047  [  200/10672]\n",
      "loss: 1.030979  [  300/10672]\n",
      "loss: 1.249716  [  400/10672]\n",
      "loss: 1.160949  [  500/10672]\n",
      "loss: 1.665478  [  600/10672]\n",
      "loss: 1.053774  [  700/10672]\n",
      "loss: 1.664752  [  800/10672]\n",
      "loss: 1.362089  [  900/10672]\n",
      "loss: 1.665729  [ 1000/10672]\n",
      "loss: 1.259863  [ 1100/10672]\n",
      "loss: 1.184208  [ 1200/10672]\n",
      "loss: 1.126685  [ 1300/10672]\n",
      "loss: 1.113907  [ 1400/10672]\n",
      "loss: 1.074650  [ 1500/10672]\n",
      "loss: 1.135204  [ 1600/10672]\n",
      "loss: 1.062148  [ 1700/10672]\n",
      "loss: 1.229962  [ 1800/10672]\n",
      "loss: 1.144871  [ 1900/10672]\n",
      "loss: 1.187519  [ 2000/10672]\n",
      "loss: 1.181631  [ 2100/10672]\n",
      "loss: 1.159328  [ 2200/10672]\n",
      "loss: 1.226910  [ 2300/10672]\n",
      "loss: 1.135870  [ 2400/10672]\n",
      "loss: 1.122177  [ 2500/10672]\n",
      "loss: 1.128880  [ 2600/10672]\n",
      "loss: 1.664183  [ 2700/10672]\n",
      "loss: 1.118048  [ 2800/10672]\n",
      "loss: 1.663376  [ 2900/10672]\n",
      "loss: 1.665422  [ 3000/10672]\n",
      "loss: 1.209736  [ 3100/10672]\n",
      "loss: 1.100744  [ 3200/10672]\n",
      "loss: 1.082421  [ 3300/10672]\n",
      "loss: 1.667995  [ 3400/10672]\n",
      "loss: 1.666315  [ 3500/10672]\n",
      "loss: 1.665527  [ 3600/10672]\n",
      "loss: 1.155801  [ 3700/10672]\n",
      "loss: 1.101359  [ 3800/10672]\n",
      "loss: 1.216817  [ 3900/10672]\n",
      "loss: 1.111877  [ 4000/10672]\n",
      "loss: 1.664468  [ 4100/10672]\n",
      "loss: 1.664284  [ 4200/10672]\n",
      "loss: 1.138037  [ 4300/10672]\n",
      "loss: 1.664039  [ 4400/10672]\n",
      "loss: 1.150576  [ 4500/10672]\n",
      "loss: 1.252133  [ 4600/10672]\n",
      "loss: 1.142039  [ 4700/10672]\n",
      "loss: 1.666251  [ 4800/10672]\n",
      "loss: 1.149931  [ 4900/10672]\n",
      "loss: 1.663583  [ 5000/10672]\n",
      "loss: 1.179317  [ 5100/10672]\n",
      "loss: 1.110017  [ 5200/10672]\n",
      "loss: 1.328695  [ 5300/10672]\n",
      "loss: 1.124040  [ 5400/10672]\n",
      "loss: 1.066646  [ 5500/10672]\n",
      "loss: 1.138012  [ 5600/10672]\n",
      "loss: 1.666440  [ 5700/10672]\n",
      "loss: 1.128822  [ 5800/10672]\n",
      "loss: 1.176342  [ 5900/10672]\n",
      "loss: 1.664411  [ 6000/10672]\n",
      "loss: 1.194201  [ 6100/10672]\n",
      "loss: 1.099804  [ 6200/10672]\n",
      "loss: 1.110452  [ 6300/10672]\n",
      "loss: 1.152882  [ 6400/10672]\n",
      "loss: 1.668692  [ 6500/10672]\n",
      "loss: 1.093304  [ 6600/10672]\n",
      "loss: 1.207491  [ 6700/10672]\n",
      "loss: 1.112085  [ 6800/10672]\n",
      "loss: 1.160763  [ 6900/10672]\n",
      "loss: 1.139537  [ 7000/10672]\n",
      "loss: 1.037715  [ 7100/10672]\n",
      "loss: 1.204783  [ 7200/10672]\n",
      "loss: 1.210726  [ 7300/10672]\n",
      "loss: 1.174818  [ 7400/10672]\n",
      "loss: 1.052965  [ 7500/10672]\n",
      "loss: 1.208581  [ 7600/10672]\n",
      "loss: 1.217328  [ 7700/10672]\n",
      "loss: 1.230777  [ 7800/10672]\n",
      "loss: 1.118727  [ 7900/10672]\n",
      "loss: 1.096557  [ 8000/10672]\n",
      "loss: 1.129431  [ 8100/10672]\n",
      "loss: 1.144641  [ 8200/10672]\n",
      "loss: 1.148543  [ 8300/10672]\n",
      "loss: 1.169923  [ 8400/10672]\n",
      "loss: 1.071868  [ 8500/10672]\n",
      "loss: 1.172948  [ 8600/10672]\n",
      "loss: 1.205291  [ 8700/10672]\n",
      "loss: 1.130272  [ 8800/10672]\n",
      "loss: 1.066406  [ 8900/10672]\n",
      "loss: 1.664516  [ 9000/10672]\n",
      "loss: 1.117315  [ 9100/10672]\n",
      "loss: 1.081004  [ 9200/10672]\n",
      "loss: 1.171229  [ 9300/10672]\n",
      "loss: 1.166270  [ 9400/10672]\n",
      "loss: 1.173892  [ 9500/10672]\n",
      "loss: 1.670084  [ 9600/10672]\n",
      "loss: 1.216169  [ 9700/10672]\n",
      "loss: 1.143750  [ 9800/10672]\n",
      "loss: 1.108827  [ 9900/10672]\n",
      "loss: 1.178386  [10000/10672]\n",
      "loss: 1.109363  [10100/10672]\n",
      "loss: 1.039730  [10200/10672]\n",
      "loss: 1.172601  [10300/10672]\n",
      "loss: 1.208262  [10400/10672]\n",
      "loss: 1.202528  [10500/10672]\n",
      "loss: 1.665411  [10600/10672]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.064076  [    0/10672]\n",
      "loss: 1.113155  [  100/10672]\n",
      "loss: 1.007410  [  200/10672]\n",
      "loss: 1.028877  [  300/10672]\n",
      "loss: 1.250030  [  400/10672]\n",
      "loss: 1.159092  [  500/10672]\n",
      "loss: 1.665555  [  600/10672]\n",
      "loss: 1.052434  [  700/10672]\n",
      "loss: 1.664841  [  800/10672]\n",
      "loss: 1.365301  [  900/10672]\n",
      "loss: 1.665795  [ 1000/10672]\n",
      "loss: 1.260498  [ 1100/10672]\n",
      "loss: 1.182890  [ 1200/10672]\n",
      "loss: 1.126977  [ 1300/10672]\n",
      "loss: 1.110950  [ 1400/10672]\n",
      "loss: 1.073601  [ 1500/10672]\n",
      "loss: 1.132751  [ 1600/10672]\n",
      "loss: 1.060911  [ 1700/10672]\n",
      "loss: 1.230179  [ 1800/10672]\n",
      "loss: 1.142822  [ 1900/10672]\n",
      "loss: 1.186502  [ 2000/10672]\n",
      "loss: 1.180587  [ 2100/10672]\n",
      "loss: 1.157714  [ 2200/10672]\n",
      "loss: 1.227203  [ 2300/10672]\n",
      "loss: 1.135922  [ 2400/10672]\n",
      "loss: 1.121925  [ 2500/10672]\n",
      "loss: 1.128813  [ 2600/10672]\n",
      "loss: 1.664268  [ 2700/10672]\n",
      "loss: 1.117623  [ 2800/10672]\n",
      "loss: 1.663562  [ 2900/10672]\n",
      "loss: 1.665543  [ 3000/10672]\n",
      "loss: 1.209924  [ 3100/10672]\n",
      "loss: 1.098084  [ 3200/10672]\n",
      "loss: 1.081201  [ 3300/10672]\n",
      "loss: 1.668208  [ 3400/10672]\n",
      "loss: 1.666439  [ 3500/10672]\n",
      "loss: 1.665668  [ 3600/10672]\n",
      "loss: 1.154286  [ 3700/10672]\n",
      "loss: 1.100763  [ 3800/10672]\n",
      "loss: 1.216702  [ 3900/10672]\n",
      "loss: 1.109014  [ 4000/10672]\n",
      "loss: 1.664567  [ 4100/10672]\n",
      "loss: 1.664431  [ 4200/10672]\n",
      "loss: 1.138264  [ 4300/10672]\n",
      "loss: 1.664134  [ 4400/10672]\n",
      "loss: 1.148865  [ 4500/10672]\n",
      "loss: 1.253180  [ 4600/10672]\n",
      "loss: 1.140216  [ 4700/10672]\n",
      "loss: 1.666387  [ 4800/10672]\n",
      "loss: 1.150453  [ 4900/10672]\n",
      "loss: 1.663703  [ 5000/10672]\n",
      "loss: 1.180587  [ 5100/10672]\n",
      "loss: 1.109344  [ 5200/10672]\n",
      "loss: 1.331698  [ 5300/10672]\n",
      "loss: 1.123974  [ 5400/10672]\n",
      "loss: 1.063113  [ 5500/10672]\n",
      "loss: 1.138167  [ 5600/10672]\n",
      "loss: 1.666581  [ 5700/10672]\n",
      "loss: 1.126699  [ 5800/10672]\n",
      "loss: 1.177436  [ 5900/10672]\n",
      "loss: 1.664531  [ 6000/10672]\n",
      "loss: 1.193858  [ 6100/10672]\n",
      "loss: 1.098816  [ 6200/10672]\n",
      "loss: 1.108142  [ 6300/10672]\n",
      "loss: 1.153621  [ 6400/10672]\n",
      "loss: 1.668883  [ 6500/10672]\n",
      "loss: 1.092575  [ 6600/10672]\n",
      "loss: 1.207256  [ 6700/10672]\n",
      "loss: 1.111569  [ 6800/10672]\n",
      "loss: 1.159287  [ 6900/10672]\n",
      "loss: 1.139811  [ 7000/10672]\n",
      "loss: 1.035590  [ 7100/10672]\n",
      "loss: 1.204543  [ 7200/10672]\n",
      "loss: 1.212669  [ 7300/10672]\n",
      "loss: 1.174056  [ 7400/10672]\n",
      "loss: 1.050885  [ 7500/10672]\n",
      "loss: 1.208352  [ 7600/10672]\n",
      "loss: 1.217516  [ 7700/10672]\n",
      "loss: 1.231219  [ 7800/10672]\n",
      "loss: 1.118492  [ 7900/10672]\n",
      "loss: 1.095890  [ 8000/10672]\n",
      "loss: 1.129260  [ 8100/10672]\n",
      "loss: 1.142736  [ 8200/10672]\n",
      "loss: 1.146801  [ 8300/10672]\n",
      "loss: 1.170882  [ 8400/10672]\n",
      "loss: 1.070664  [ 8500/10672]\n",
      "loss: 1.173925  [ 8600/10672]\n",
      "loss: 1.205105  [ 8700/10672]\n",
      "loss: 1.128415  [ 8800/10672]\n",
      "loss: 1.064978  [ 8900/10672]\n",
      "loss: 1.664600  [ 9000/10672]\n",
      "loss: 1.114625  [ 9100/10672]\n",
      "loss: 1.079765  [ 9200/10672]\n",
      "loss: 1.172393  [ 9300/10672]\n",
      "loss: 1.167427  [ 9400/10672]\n",
      "loss: 1.172482  [ 9500/10672]\n",
      "loss: 1.670253  [ 9600/10672]\n",
      "loss: 1.215885  [ 9700/10672]\n",
      "loss: 1.144377  [ 9800/10672]\n",
      "loss: 1.108598  [ 9900/10672]\n",
      "loss: 1.179916  [10000/10672]\n",
      "loss: 1.109044  [10100/10672]\n",
      "loss: 1.037817  [10200/10672]\n",
      "loss: 1.171248  [10300/10672]\n",
      "loss: 1.207597  [10400/10672]\n",
      "loss: 1.201754  [10500/10672]\n",
      "loss: 1.665572  [10600/10672]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.062984  [    0/10672]\n",
      "loss: 1.112942  [  100/10672]\n",
      "loss: 1.004801  [  200/10672]\n",
      "loss: 1.026792  [  300/10672]\n",
      "loss: 1.250346  [  400/10672]\n",
      "loss: 1.157257  [  500/10672]\n",
      "loss: 1.665628  [  600/10672]\n",
      "loss: 1.051099  [  700/10672]\n",
      "loss: 1.664925  [  800/10672]\n",
      "loss: 1.368489  [  900/10672]\n",
      "loss: 1.665857  [ 1000/10672]\n",
      "loss: 1.261133  [ 1100/10672]\n",
      "loss: 1.181582  [ 1200/10672]\n",
      "loss: 1.127262  [ 1300/10672]\n",
      "loss: 1.108026  [ 1400/10672]\n",
      "loss: 1.072557  [ 1500/10672]\n",
      "loss: 1.130322  [ 1600/10672]\n",
      "loss: 1.059680  [ 1700/10672]\n",
      "loss: 1.230404  [ 1800/10672]\n",
      "loss: 1.140797  [ 1900/10672]\n",
      "loss: 1.185495  [ 2000/10672]\n",
      "loss: 1.179563  [ 2100/10672]\n",
      "loss: 1.156121  [ 2200/10672]\n",
      "loss: 1.227503  [ 2300/10672]\n",
      "loss: 1.135966  [ 2400/10672]\n",
      "loss: 1.121669  [ 2500/10672]\n",
      "loss: 1.128746  [ 2600/10672]\n",
      "loss: 1.664347  [ 2700/10672]\n",
      "loss: 1.117194  [ 2800/10672]\n",
      "loss: 1.663742  [ 2900/10672]\n",
      "loss: 1.665658  [ 3000/10672]\n",
      "loss: 1.210123  [ 3100/10672]\n",
      "loss: 1.095460  [ 3200/10672]\n",
      "loss: 1.079985  [ 3300/10672]\n",
      "loss: 1.668417  [ 3400/10672]\n",
      "loss: 1.666558  [ 3500/10672]\n",
      "loss: 1.665807  [ 3600/10672]\n",
      "loss: 1.152789  [ 3700/10672]\n",
      "loss: 1.100166  [ 3800/10672]\n",
      "loss: 1.216595  [ 3900/10672]\n",
      "loss: 1.106183  [ 4000/10672]\n",
      "loss: 1.664661  [ 4100/10672]\n",
      "loss: 1.664574  [ 4200/10672]\n",
      "loss: 1.138483  [ 4300/10672]\n",
      "loss: 1.664224  [ 4400/10672]\n",
      "loss: 1.147171  [ 4500/10672]\n",
      "loss: 1.254231  [ 4600/10672]\n",
      "loss: 1.138417  [ 4700/10672]\n",
      "loss: 1.666519  [ 4800/10672]\n",
      "loss: 1.150970  [ 4900/10672]\n",
      "loss: 1.663817  [ 5000/10672]\n",
      "loss: 1.181850  [ 5100/10672]\n",
      "loss: 1.108665  [ 5200/10672]\n",
      "loss: 1.334689  [ 5300/10672]\n",
      "loss: 1.123901  [ 5400/10672]\n",
      "loss: 1.059629  [ 5500/10672]\n",
      "loss: 1.138312  [ 5600/10672]\n",
      "loss: 1.666719  [ 5700/10672]\n",
      "loss: 1.124597  [ 5800/10672]\n",
      "loss: 1.178519  [ 5900/10672]\n",
      "loss: 1.664647  [ 6000/10672]\n",
      "loss: 1.193529  [ 6100/10672]\n",
      "loss: 1.097831  [ 6200/10672]\n",
      "loss: 1.105867  [ 6300/10672]\n",
      "loss: 1.154355  [ 6400/10672]\n",
      "loss: 1.669071  [ 6500/10672]\n",
      "loss: 1.091849  [ 6600/10672]\n",
      "loss: 1.207033  [ 6700/10672]\n",
      "loss: 1.111049  [ 6800/10672]\n",
      "loss: 1.157824  [ 6900/10672]\n",
      "loss: 1.140081  [ 7000/10672]\n",
      "loss: 1.033484  [ 7100/10672]\n",
      "loss: 1.204312  [ 7200/10672]\n",
      "loss: 1.214596  [ 7300/10672]\n",
      "loss: 1.173308  [ 7400/10672]\n",
      "loss: 1.048821  [ 7500/10672]\n",
      "loss: 1.208132  [ 7600/10672]\n",
      "loss: 1.217710  [ 7700/10672]\n",
      "loss: 1.231663  [ 7800/10672]\n",
      "loss: 1.118252  [ 7900/10672]\n",
      "loss: 1.095224  [ 8000/10672]\n",
      "loss: 1.129081  [ 8100/10672]\n",
      "loss: 1.140855  [ 8200/10672]\n",
      "loss: 1.145081  [ 8300/10672]\n",
      "loss: 1.171835  [ 8400/10672]\n",
      "loss: 1.069469  [ 8500/10672]\n",
      "loss: 1.174897  [ 8600/10672]\n",
      "loss: 1.204933  [ 8700/10672]\n",
      "loss: 1.126584  [ 8800/10672]\n",
      "loss: 1.063559  [ 8900/10672]\n",
      "loss: 1.664679  [ 9000/10672]\n",
      "loss: 1.111959  [ 9100/10672]\n",
      "loss: 1.078532  [ 9200/10672]\n",
      "loss: 1.173551  [ 9300/10672]\n",
      "loss: 1.168575  [ 9400/10672]\n",
      "loss: 1.171091  [ 9500/10672]\n",
      "loss: 1.670420  [ 9600/10672]\n",
      "loss: 1.215606  [ 9700/10672]\n",
      "loss: 1.145000  [ 9800/10672]\n",
      "loss: 1.108369  [ 9900/10672]\n",
      "loss: 1.181437  [10000/10672]\n",
      "loss: 1.108719  [10100/10672]\n",
      "loss: 1.035918  [10200/10672]\n",
      "loss: 1.169912  [10300/10672]\n",
      "loss: 1.206944  [10400/10672]\n",
      "loss: 1.200989  [10500/10672]\n",
      "loss: 1.665727  [10600/10672]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.061903  [    0/10672]\n",
      "loss: 1.112721  [  100/10672]\n",
      "loss: 1.002221  [  200/10672]\n",
      "loss: 1.024726  [  300/10672]\n",
      "loss: 1.250663  [  400/10672]\n",
      "loss: 1.155442  [  500/10672]\n",
      "loss: 1.665697  [  600/10672]\n",
      "loss: 1.049770  [  700/10672]\n",
      "loss: 1.665004  [  800/10672]\n",
      "loss: 1.371654  [  900/10672]\n",
      "loss: 1.665916  [ 1000/10672]\n",
      "loss: 1.261767  [ 1100/10672]\n",
      "loss: 1.180283  [ 1200/10672]\n",
      "loss: 1.127540  [ 1300/10672]\n",
      "loss: 1.105136  [ 1400/10672]\n",
      "loss: 1.071520  [ 1500/10672]\n",
      "loss: 1.127915  [ 1600/10672]\n",
      "loss: 1.058454  [ 1700/10672]\n",
      "loss: 1.230634  [ 1800/10672]\n",
      "loss: 1.138793  [ 1900/10672]\n",
      "loss: 1.184498  [ 2000/10672]\n",
      "loss: 1.178554  [ 2100/10672]\n",
      "loss: 1.154545  [ 2200/10672]\n",
      "loss: 1.227808  [ 2300/10672]\n",
      "loss: 1.136005  [ 2400/10672]\n",
      "loss: 1.121410  [ 2500/10672]\n",
      "loss: 1.128681  [ 2600/10672]\n",
      "loss: 1.664422  [ 2700/10672]\n",
      "loss: 1.116764  [ 2800/10672]\n",
      "loss: 1.663915  [ 2900/10672]\n",
      "loss: 1.665769  [ 3000/10672]\n",
      "loss: 1.210330  [ 3100/10672]\n",
      "loss: 1.092870  [ 3200/10672]\n",
      "loss: 1.078774  [ 3300/10672]\n",
      "loss: 1.668624  [ 3400/10672]\n",
      "loss: 1.666674  [ 3500/10672]\n",
      "loss: 1.665945  [ 3600/10672]\n",
      "loss: 1.151306  [ 3700/10672]\n",
      "loss: 1.099571  [ 3800/10672]\n",
      "loss: 1.216489  [ 3900/10672]\n",
      "loss: 1.103381  [ 4000/10672]\n",
      "loss: 1.664751  [ 4100/10672]\n",
      "loss: 1.664716  [ 4200/10672]\n",
      "loss: 1.138694  [ 4300/10672]\n",
      "loss: 1.664309  [ 4400/10672]\n",
      "loss: 1.145493  [ 4500/10672]\n",
      "loss: 1.255282  [ 4600/10672]\n",
      "loss: 1.136639  [ 4700/10672]\n",
      "loss: 1.666648  [ 4800/10672]\n",
      "loss: 1.151485  [ 4900/10672]\n",
      "loss: 1.663925  [ 5000/10672]\n",
      "loss: 1.183108  [ 5100/10672]\n",
      "loss: 1.107985  [ 5200/10672]\n",
      "loss: 1.337664  [ 5300/10672]\n",
      "loss: 1.123826  [ 5400/10672]\n",
      "loss: 1.056190  [ 5500/10672]\n",
      "loss: 1.138450  [ 5600/10672]\n",
      "loss: 1.666853  [ 5700/10672]\n",
      "loss: 1.122513  [ 5800/10672]\n",
      "loss: 1.179595  [ 5900/10672]\n",
      "loss: 1.664758  [ 6000/10672]\n",
      "loss: 1.193209  [ 6100/10672]\n",
      "loss: 1.096854  [ 6200/10672]\n",
      "loss: 1.103622  [ 6300/10672]\n",
      "loss: 1.155088  [ 6400/10672]\n",
      "loss: 1.669256  [ 6500/10672]\n",
      "loss: 1.091129  [ 6600/10672]\n",
      "loss: 1.206817  [ 6700/10672]\n",
      "loss: 1.110532  [ 6800/10672]\n",
      "loss: 1.156370  [ 6900/10672]\n",
      "loss: 1.140352  [ 7000/10672]\n",
      "loss: 1.031400  [ 7100/10672]\n",
      "loss: 1.204086  [ 7200/10672]\n",
      "loss: 1.216511  [ 7300/10672]\n",
      "loss: 1.172571  [ 7400/10672]\n",
      "loss: 1.046779  [ 7500/10672]\n",
      "loss: 1.207915  [ 7600/10672]\n",
      "loss: 1.217905  [ 7700/10672]\n",
      "loss: 1.232102  [ 7800/10672]\n",
      "loss: 1.118011  [ 7900/10672]\n",
      "loss: 1.094562  [ 8000/10672]\n",
      "loss: 1.128898  [ 8100/10672]\n",
      "loss: 1.138994  [ 8200/10672]\n",
      "loss: 1.143379  [ 8300/10672]\n",
      "loss: 1.172784  [ 8400/10672]\n",
      "loss: 1.068284  [ 8500/10672]\n",
      "loss: 1.175868  [ 8600/10672]\n",
      "loss: 1.204776  [ 8700/10672]\n",
      "loss: 1.124777  [ 8800/10672]\n",
      "loss: 1.062150  [ 8900/10672]\n",
      "loss: 1.664755  [ 9000/10672]\n",
      "loss: 1.109316  [ 9100/10672]\n",
      "loss: 1.077305  [ 9200/10672]\n",
      "loss: 1.174702  [ 9300/10672]\n",
      "loss: 1.169716  [ 9400/10672]\n",
      "loss: 1.169714  [ 9500/10672]\n",
      "loss: 1.670584  [ 9600/10672]\n",
      "loss: 1.215329  [ 9700/10672]\n",
      "loss: 1.145621  [ 9800/10672]\n",
      "loss: 1.108142  [ 9900/10672]\n",
      "loss: 1.182950  [10000/10672]\n",
      "loss: 1.108391  [10100/10672]\n",
      "loss: 1.034035  [10200/10672]\n",
      "loss: 1.168590  [10300/10672]\n",
      "loss: 1.206302  [10400/10672]\n",
      "loss: 1.200233  [10500/10672]\n",
      "loss: 1.665878  [10600/10672]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.060834  [    0/10672]\n",
      "loss: 1.112494  [  100/10672]\n",
      "loss: 0.999671  [  200/10672]\n",
      "loss: 1.022678  [  300/10672]\n",
      "loss: 1.250980  [  400/10672]\n",
      "loss: 1.153646  [  500/10672]\n",
      "loss: 1.665762  [  600/10672]\n",
      "loss: 1.048448  [  700/10672]\n",
      "loss: 1.665079  [  800/10672]\n",
      "loss: 1.374794  [  900/10672]\n",
      "loss: 1.665972  [ 1000/10672]\n",
      "loss: 1.262398  [ 1100/10672]\n",
      "loss: 1.178993  [ 1200/10672]\n",
      "loss: 1.127812  [ 1300/10672]\n",
      "loss: 1.102276  [ 1400/10672]\n",
      "loss: 1.070490  [ 1500/10672]\n",
      "loss: 1.125530  [ 1600/10672]\n",
      "loss: 1.057236  [ 1700/10672]\n",
      "loss: 1.230869  [ 1800/10672]\n",
      "loss: 1.136810  [ 1900/10672]\n",
      "loss: 1.183510  [ 2000/10672]\n",
      "loss: 1.177561  [ 2100/10672]\n",
      "loss: 1.152987  [ 2200/10672]\n",
      "loss: 1.228115  [ 2300/10672]\n",
      "loss: 1.136040  [ 2400/10672]\n",
      "loss: 1.121149  [ 2500/10672]\n",
      "loss: 1.128618  [ 2600/10672]\n",
      "loss: 1.664494  [ 2700/10672]\n",
      "loss: 1.116332  [ 2800/10672]\n",
      "loss: 1.664083  [ 2900/10672]\n",
      "loss: 1.665876  [ 3000/10672]\n",
      "loss: 1.210546  [ 3100/10672]\n",
      "loss: 1.090313  [ 3200/10672]\n",
      "loss: 1.077568  [ 3300/10672]\n",
      "loss: 1.668827  [ 3400/10672]\n",
      "loss: 1.666786  [ 3500/10672]\n",
      "loss: 1.666081  [ 3600/10672]\n",
      "loss: 1.149838  [ 3700/10672]\n",
      "loss: 1.098977  [ 3800/10672]\n",
      "loss: 1.216388  [ 3900/10672]\n",
      "loss: 1.100607  [ 4000/10672]\n",
      "loss: 1.664837  [ 4100/10672]\n",
      "loss: 1.664855  [ 4200/10672]\n",
      "loss: 1.138898  [ 4300/10672]\n",
      "loss: 1.664390  [ 4400/10672]\n",
      "loss: 1.143830  [ 4500/10672]\n",
      "loss: 1.256334  [ 4600/10672]\n",
      "loss: 1.134885  [ 4700/10672]\n",
      "loss: 1.666772  [ 4800/10672]\n",
      "loss: 1.151996  [ 4900/10672]\n",
      "loss: 1.664029  [ 5000/10672]\n",
      "loss: 1.184359  [ 5100/10672]\n",
      "loss: 1.107299  [ 5200/10672]\n",
      "loss: 1.340626  [ 5300/10672]\n",
      "loss: 1.123745  [ 5400/10672]\n",
      "loss: 1.052798  [ 5500/10672]\n",
      "loss: 1.138579  [ 5600/10672]\n",
      "loss: 1.666983  [ 5700/10672]\n",
      "loss: 1.120447  [ 5800/10672]\n",
      "loss: 1.180661  [ 5900/10672]\n",
      "loss: 1.664865  [ 6000/10672]\n",
      "loss: 1.192900  [ 6100/10672]\n",
      "loss: 1.095882  [ 6200/10672]\n",
      "loss: 1.101410  [ 6300/10672]\n",
      "loss: 1.155817  [ 6400/10672]\n",
      "loss: 1.669439  [ 6500/10672]\n",
      "loss: 1.090413  [ 6600/10672]\n",
      "loss: 1.206610  [ 6700/10672]\n",
      "loss: 1.110013  [ 6800/10672]\n",
      "loss: 1.154926  [ 6900/10672]\n",
      "loss: 1.140621  [ 7000/10672]\n",
      "loss: 1.029335  [ 7100/10672]\n",
      "loss: 1.203865  [ 7200/10672]\n",
      "loss: 1.218412  [ 7300/10672]\n",
      "loss: 1.171846  [ 7400/10672]\n",
      "loss: 1.044755  [ 7500/10672]\n",
      "loss: 1.207703  [ 7600/10672]\n",
      "loss: 1.218103  [ 7700/10672]\n",
      "loss: 1.232540  [ 7800/10672]\n",
      "loss: 1.117765  [ 7900/10672]\n",
      "loss: 1.093902  [ 8000/10672]\n",
      "loss: 1.128711  [ 8100/10672]\n",
      "loss: 1.137154  [ 8200/10672]\n",
      "loss: 1.141696  [ 8300/10672]\n",
      "loss: 1.173728  [ 8400/10672]\n",
      "loss: 1.067109  [ 8500/10672]\n",
      "loss: 1.176835  [ 8600/10672]\n",
      "loss: 1.204632  [ 8700/10672]\n",
      "loss: 1.122993  [ 8800/10672]\n",
      "loss: 1.060751  [ 8900/10672]\n",
      "loss: 1.664828  [ 9000/10672]\n",
      "loss: 1.106696  [ 9100/10672]\n",
      "loss: 1.076084  [ 9200/10672]\n",
      "loss: 1.175847  [ 9300/10672]\n",
      "loss: 1.170848  [ 9400/10672]\n",
      "loss: 1.168353  [ 9500/10672]\n",
      "loss: 1.670748  [ 9600/10672]\n",
      "loss: 1.215055  [ 9700/10672]\n",
      "loss: 1.146239  [ 9800/10672]\n",
      "loss: 1.107915  [ 9900/10672]\n",
      "loss: 1.184455  [10000/10672]\n",
      "loss: 1.108059  [10100/10672]\n",
      "loss: 1.032168  [10200/10672]\n",
      "loss: 1.167284  [10300/10672]\n",
      "loss: 1.205668  [10400/10672]\n",
      "loss: 1.199486  [10500/10672]\n",
      "loss: 1.666023  [10600/10672]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.059776  [    0/10672]\n",
      "loss: 1.112261  [  100/10672]\n",
      "loss: 0.997149  [  200/10672]\n",
      "loss: 1.020649  [  300/10672]\n",
      "loss: 1.251297  [  400/10672]\n",
      "loss: 1.151869  [  500/10672]\n",
      "loss: 1.665824  [  600/10672]\n",
      "loss: 1.047132  [  700/10672]\n",
      "loss: 1.665149  [  800/10672]\n",
      "loss: 1.377909  [  900/10672]\n",
      "loss: 1.666024  [ 1000/10672]\n",
      "loss: 1.263027  [ 1100/10672]\n",
      "loss: 1.177710  [ 1200/10672]\n",
      "loss: 1.128079  [ 1300/10672]\n",
      "loss: 1.099448  [ 1400/10672]\n",
      "loss: 1.069467  [ 1500/10672]\n",
      "loss: 1.123167  [ 1600/10672]\n",
      "loss: 1.056023  [ 1700/10672]\n",
      "loss: 1.231108  [ 1800/10672]\n",
      "loss: 1.134848  [ 1900/10672]\n",
      "loss: 1.182530  [ 2000/10672]\n",
      "loss: 1.176584  [ 2100/10672]\n",
      "loss: 1.151446  [ 2200/10672]\n",
      "loss: 1.228426  [ 2300/10672]\n",
      "loss: 1.136071  [ 2400/10672]\n",
      "loss: 1.120885  [ 2500/10672]\n",
      "loss: 1.128557  [ 2600/10672]\n",
      "loss: 1.664561  [ 2700/10672]\n",
      "loss: 1.115898  [ 2800/10672]\n",
      "loss: 1.664244  [ 2900/10672]\n",
      "loss: 1.665979  [ 3000/10672]\n",
      "loss: 1.210769  [ 3100/10672]\n",
      "loss: 1.087789  [ 3200/10672]\n",
      "loss: 1.076368  [ 3300/10672]\n",
      "loss: 1.669028  [ 3400/10672]\n",
      "loss: 1.666896  [ 3500/10672]\n",
      "loss: 1.666215  [ 3600/10672]\n",
      "loss: 1.148383  [ 3700/10672]\n",
      "loss: 1.098384  [ 3800/10672]\n",
      "loss: 1.216290  [ 3900/10672]\n",
      "loss: 1.097861  [ 4000/10672]\n",
      "loss: 1.664918  [ 4100/10672]\n",
      "loss: 1.664992  [ 4200/10672]\n",
      "loss: 1.139095  [ 4300/10672]\n",
      "loss: 1.664466  [ 4400/10672]\n",
      "loss: 1.142182  [ 4500/10672]\n",
      "loss: 1.257387  [ 4600/10672]\n",
      "loss: 1.133150  [ 4700/10672]\n",
      "loss: 1.666894  [ 4800/10672]\n",
      "loss: 1.152504  [ 4900/10672]\n",
      "loss: 1.664127  [ 5000/10672]\n",
      "loss: 1.185605  [ 5100/10672]\n",
      "loss: 1.106611  [ 5200/10672]\n",
      "loss: 1.343570  [ 5300/10672]\n",
      "loss: 1.123662  [ 5400/10672]\n",
      "loss: 1.049450  [ 5500/10672]\n",
      "loss: 1.138700  [ 5600/10672]\n",
      "loss: 1.667110  [ 5700/10672]\n",
      "loss: 1.118399  [ 5800/10672]\n",
      "loss: 1.181720  [ 5900/10672]\n",
      "loss: 1.664967  [ 6000/10672]\n",
      "loss: 1.192599  [ 6100/10672]\n",
      "loss: 1.094916  [ 6200/10672]\n",
      "loss: 1.099227  [ 6300/10672]\n",
      "loss: 1.156544  [ 6400/10672]\n",
      "loss: 1.669621  [ 6500/10672]\n",
      "loss: 1.089702  [ 6600/10672]\n",
      "loss: 1.206412  [ 6700/10672]\n",
      "loss: 1.109494  [ 6800/10672]\n",
      "loss: 1.153491  [ 6900/10672]\n",
      "loss: 1.140889  [ 7000/10672]\n",
      "loss: 1.027291  [ 7100/10672]\n",
      "loss: 1.203650  [ 7200/10672]\n",
      "loss: 1.220299  [ 7300/10672]\n",
      "loss: 1.171132  [ 7400/10672]\n",
      "loss: 1.042749  [ 7500/10672]\n",
      "loss: 1.207495  [ 7600/10672]\n",
      "loss: 1.218302  [ 7700/10672]\n",
      "loss: 1.232976  [ 7800/10672]\n",
      "loss: 1.117516  [ 7900/10672]\n",
      "loss: 1.093244  [ 8000/10672]\n",
      "loss: 1.128517  [ 8100/10672]\n",
      "loss: 1.135336  [ 8200/10672]\n",
      "loss: 1.140033  [ 8300/10672]\n",
      "loss: 1.174667  [ 8400/10672]\n",
      "loss: 1.065943  [ 8500/10672]\n",
      "loss: 1.177799  [ 8600/10672]\n",
      "loss: 1.204503  [ 8700/10672]\n",
      "loss: 1.121234  [ 8800/10672]\n",
      "loss: 1.059360  [ 8900/10672]\n",
      "loss: 1.664899  [ 9000/10672]\n",
      "loss: 1.104098  [ 9100/10672]\n",
      "loss: 1.074869  [ 9200/10672]\n",
      "loss: 1.176985  [ 9300/10672]\n",
      "loss: 1.171973  [ 9400/10672]\n",
      "loss: 1.167007  [ 9500/10672]\n",
      "loss: 1.670909  [ 9600/10672]\n",
      "loss: 1.214783  [ 9700/10672]\n",
      "loss: 1.146852  [ 9800/10672]\n",
      "loss: 1.107689  [ 9900/10672]\n",
      "loss: 1.185951  [10000/10672]\n",
      "loss: 1.107723  [10100/10672]\n",
      "loss: 1.030316  [10200/10672]\n",
      "loss: 1.165992  [10300/10672]\n",
      "loss: 1.205045  [10400/10672]\n",
      "loss: 1.198748  [10500/10672]\n",
      "loss: 1.666164  [10600/10672]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.058729  [    0/10672]\n",
      "loss: 1.112022  [  100/10672]\n",
      "loss: 0.994657  [  200/10672]\n",
      "loss: 1.018637  [  300/10672]\n",
      "loss: 1.251613  [  400/10672]\n",
      "loss: 1.150109  [  500/10672]\n",
      "loss: 1.665882  [  600/10672]\n",
      "loss: 1.045822  [  700/10672]\n",
      "loss: 1.665216  [  800/10672]\n",
      "loss: 1.381000  [  900/10672]\n",
      "loss: 1.666074  [ 1000/10672]\n",
      "loss: 1.263653  [ 1100/10672]\n",
      "loss: 1.176436  [ 1200/10672]\n",
      "loss: 1.128340  [ 1300/10672]\n",
      "loss: 1.096651  [ 1400/10672]\n",
      "loss: 1.068450  [ 1500/10672]\n",
      "loss: 1.120825  [ 1600/10672]\n",
      "loss: 1.054817  [ 1700/10672]\n",
      "loss: 1.231353  [ 1800/10672]\n",
      "loss: 1.132905  [ 1900/10672]\n",
      "loss: 1.181559  [ 2000/10672]\n",
      "loss: 1.175622  [ 2100/10672]\n",
      "loss: 1.149923  [ 2200/10672]\n",
      "loss: 1.228741  [ 2300/10672]\n",
      "loss: 1.136094  [ 2400/10672]\n",
      "loss: 1.120618  [ 2500/10672]\n",
      "loss: 1.128495  [ 2600/10672]\n",
      "loss: 1.664626  [ 2700/10672]\n",
      "loss: 1.115461  [ 2800/10672]\n",
      "loss: 1.664401  [ 2900/10672]\n",
      "loss: 1.666078  [ 3000/10672]\n",
      "loss: 1.211003  [ 3100/10672]\n",
      "loss: 1.085299  [ 3200/10672]\n",
      "loss: 1.075171  [ 3300/10672]\n",
      "loss: 1.669227  [ 3400/10672]\n",
      "loss: 1.667003  [ 3500/10672]\n",
      "loss: 1.666348  [ 3600/10672]\n",
      "loss: 1.146945  [ 3700/10672]\n",
      "loss: 1.097790  [ 3800/10672]\n",
      "loss: 1.216197  [ 3900/10672]\n",
      "loss: 1.095144  [ 4000/10672]\n",
      "loss: 1.664995  [ 4100/10672]\n",
      "loss: 1.665128  [ 4200/10672]\n",
      "loss: 1.139283  [ 4300/10672]\n",
      "loss: 1.664538  [ 4400/10672]\n",
      "loss: 1.140549  [ 4500/10672]\n",
      "loss: 1.258440  [ 4600/10672]\n",
      "loss: 1.131438  [ 4700/10672]\n",
      "loss: 1.667013  [ 4800/10672]\n",
      "loss: 1.153008  [ 4900/10672]\n",
      "loss: 1.664220  [ 5000/10672]\n",
      "loss: 1.186844  [ 5100/10672]\n",
      "loss: 1.105918  [ 5200/10672]\n",
      "loss: 1.346501  [ 5300/10672]\n",
      "loss: 1.123572  [ 5400/10672]\n",
      "loss: 1.046148  [ 5500/10672]\n",
      "loss: 1.138811  [ 5600/10672]\n",
      "loss: 1.667234  [ 5700/10672]\n",
      "loss: 1.116369  [ 5800/10672]\n",
      "loss: 1.182769  [ 5900/10672]\n",
      "loss: 1.665066  [ 6000/10672]\n",
      "loss: 1.192308  [ 6100/10672]\n",
      "loss: 1.093954  [ 6200/10672]\n",
      "loss: 1.097077  [ 6300/10672]\n",
      "loss: 1.157266  [ 6400/10672]\n",
      "loss: 1.669800  [ 6500/10672]\n",
      "loss: 1.088995  [ 6600/10672]\n",
      "loss: 1.206222  [ 6700/10672]\n",
      "loss: 1.108974  [ 6800/10672]\n",
      "loss: 1.152067  [ 6900/10672]\n",
      "loss: 1.141155  [ 7000/10672]\n",
      "loss: 1.025265  [ 7100/10672]\n",
      "loss: 1.203440  [ 7200/10672]\n",
      "loss: 1.222171  [ 7300/10672]\n",
      "loss: 1.170431  [ 7400/10672]\n",
      "loss: 1.040761  [ 7500/10672]\n",
      "loss: 1.207292  [ 7600/10672]\n",
      "loss: 1.218505  [ 7700/10672]\n",
      "loss: 1.233410  [ 7800/10672]\n",
      "loss: 1.117263  [ 7900/10672]\n",
      "loss: 1.092588  [ 8000/10672]\n",
      "loss: 1.128319  [ 8100/10672]\n",
      "loss: 1.133538  [ 8200/10672]\n",
      "loss: 1.138388  [ 8300/10672]\n",
      "loss: 1.175599  [ 8400/10672]\n",
      "loss: 1.064786  [ 8500/10672]\n",
      "loss: 1.178758  [ 8600/10672]\n",
      "loss: 1.204386  [ 8700/10672]\n",
      "loss: 1.119497  [ 8800/10672]\n",
      "loss: 1.057979  [ 8900/10672]\n",
      "loss: 1.664966  [ 9000/10672]\n",
      "loss: 1.101523  [ 9100/10672]\n",
      "loss: 1.073660  [ 9200/10672]\n",
      "loss: 1.178115  [ 9300/10672]\n",
      "loss: 1.173088  [ 9400/10672]\n",
      "loss: 1.165676  [ 9500/10672]\n",
      "loss: 1.671070  [ 9600/10672]\n",
      "loss: 1.214514  [ 9700/10672]\n",
      "loss: 1.147462  [ 9800/10672]\n",
      "loss: 1.107463  [ 9900/10672]\n",
      "loss: 1.187437  [10000/10672]\n",
      "loss: 1.107382  [10100/10672]\n",
      "loss: 1.028479  [10200/10672]\n",
      "loss: 1.164716  [10300/10672]\n",
      "loss: 1.204433  [10400/10672]\n",
      "loss: 1.198019  [10500/10672]\n",
      "loss: 1.666301  [10600/10672]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.057692  [    0/10672]\n",
      "loss: 1.111775  [  100/10672]\n",
      "loss: 0.992192  [  200/10672]\n",
      "loss: 1.016643  [  300/10672]\n",
      "loss: 1.251929  [  400/10672]\n",
      "loss: 1.148369  [  500/10672]\n",
      "loss: 1.665938  [  600/10672]\n",
      "loss: 1.044518  [  700/10672]\n",
      "loss: 1.665279  [  800/10672]\n",
      "loss: 1.384065  [  900/10672]\n",
      "loss: 1.666121  [ 1000/10672]\n",
      "loss: 1.264277  [ 1100/10672]\n",
      "loss: 1.175169  [ 1200/10672]\n",
      "loss: 1.128594  [ 1300/10672]\n",
      "loss: 1.093885  [ 1400/10672]\n",
      "loss: 1.067439  [ 1500/10672]\n",
      "loss: 1.118505  [ 1600/10672]\n",
      "loss: 1.053617  [ 1700/10672]\n",
      "loss: 1.231602  [ 1800/10672]\n",
      "loss: 1.130983  [ 1900/10672]\n",
      "loss: 1.180596  [ 2000/10672]\n",
      "loss: 1.174676  [ 2100/10672]\n",
      "loss: 1.148417  [ 2200/10672]\n",
      "loss: 1.229059  [ 2300/10672]\n",
      "loss: 1.136114  [ 2400/10672]\n",
      "loss: 1.120348  [ 2500/10672]\n",
      "loss: 1.128436  [ 2600/10672]\n",
      "loss: 1.664687  [ 2700/10672]\n",
      "loss: 1.115022  [ 2800/10672]\n",
      "loss: 1.664553  [ 2900/10672]\n",
      "loss: 1.666174  [ 3000/10672]\n",
      "loss: 1.211244  [ 3100/10672]\n",
      "loss: 1.082841  [ 3200/10672]\n",
      "loss: 1.073980  [ 3300/10672]\n",
      "loss: 1.669424  [ 3400/10672]\n",
      "loss: 1.667107  [ 3500/10672]\n",
      "loss: 1.666480  [ 3600/10672]\n",
      "loss: 1.145520  [ 3700/10672]\n",
      "loss: 1.097196  [ 3800/10672]\n",
      "loss: 1.216107  [ 3900/10672]\n",
      "loss: 1.092454  [ 4000/10672]\n",
      "loss: 1.665070  [ 4100/10672]\n",
      "loss: 1.665263  [ 4200/10672]\n",
      "loss: 1.139464  [ 4300/10672]\n",
      "loss: 1.664607  [ 4400/10672]\n",
      "loss: 1.138932  [ 4500/10672]\n",
      "loss: 1.259494  [ 4600/10672]\n",
      "loss: 1.129746  [ 4700/10672]\n",
      "loss: 1.667129  [ 4800/10672]\n",
      "loss: 1.153509  [ 4900/10672]\n",
      "loss: 1.664310  [ 5000/10672]\n",
      "loss: 1.188077  [ 5100/10672]\n",
      "loss: 1.105222  [ 5200/10672]\n",
      "loss: 1.349414  [ 5300/10672]\n",
      "loss: 1.123479  [ 5400/10672]\n",
      "loss: 1.042891  [ 5500/10672]\n",
      "loss: 1.138914  [ 5600/10672]\n",
      "loss: 1.667356  [ 5700/10672]\n",
      "loss: 1.114357  [ 5800/10672]\n",
      "loss: 1.183809  [ 5900/10672]\n",
      "loss: 1.665162  [ 6000/10672]\n",
      "loss: 1.192027  [ 6100/10672]\n",
      "loss: 1.092997  [ 6200/10672]\n",
      "loss: 1.094956  [ 6300/10672]\n",
      "loss: 1.157985  [ 6400/10672]\n",
      "loss: 1.669978  [ 6500/10672]\n",
      "loss: 1.088292  [ 6600/10672]\n",
      "loss: 1.206040  [ 6700/10672]\n",
      "loss: 1.108454  [ 6800/10672]\n",
      "loss: 1.150651  [ 6900/10672]\n",
      "loss: 1.141418  [ 7000/10672]\n",
      "loss: 1.023260  [ 7100/10672]\n",
      "loss: 1.203235  [ 7200/10672]\n",
      "loss: 1.224029  [ 7300/10672]\n",
      "loss: 1.169740  [ 7400/10672]\n",
      "loss: 1.038790  [ 7500/10672]\n",
      "loss: 1.207094  [ 7600/10672]\n",
      "loss: 1.218709  [ 7700/10672]\n",
      "loss: 1.233841  [ 7800/10672]\n",
      "loss: 1.117005  [ 7900/10672]\n",
      "loss: 1.091933  [ 8000/10672]\n",
      "loss: 1.128113  [ 8100/10672]\n",
      "loss: 1.131762  [ 8200/10672]\n",
      "loss: 1.136762  [ 8300/10672]\n",
      "loss: 1.176525  [ 8400/10672]\n",
      "loss: 1.063638  [ 8500/10672]\n",
      "loss: 1.179712  [ 8600/10672]\n",
      "loss: 1.204284  [ 8700/10672]\n",
      "loss: 1.117784  [ 8800/10672]\n",
      "loss: 1.056606  [ 8900/10672]\n",
      "loss: 1.665032  [ 9000/10672]\n",
      "loss: 1.098971  [ 9100/10672]\n",
      "loss: 1.072456  [ 9200/10672]\n",
      "loss: 1.179238  [ 9300/10672]\n",
      "loss: 1.174194  [ 9400/10672]\n",
      "loss: 1.164361  [ 9500/10672]\n",
      "loss: 1.671229  [ 9600/10672]\n",
      "loss: 1.214248  [ 9700/10672]\n",
      "loss: 1.148067  [ 9800/10672]\n",
      "loss: 1.107236  [ 9900/10672]\n",
      "loss: 1.188913  [10000/10672]\n",
      "loss: 1.107035  [10100/10672]\n",
      "loss: 1.026655  [10200/10672]\n",
      "loss: 1.163456  [10300/10672]\n",
      "loss: 1.203831  [10400/10672]\n",
      "loss: 1.197300  [10500/10672]\n",
      "loss: 1.666435  [10600/10672]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.056665  [    0/10672]\n",
      "loss: 1.111520  [  100/10672]\n",
      "loss: 0.989754  [  200/10672]\n",
      "loss: 1.014665  [  300/10672]\n",
      "loss: 1.252246  [  400/10672]\n",
      "loss: 1.146648  [  500/10672]\n",
      "loss: 1.665991  [  600/10672]\n",
      "loss: 1.043218  [  700/10672]\n",
      "loss: 1.665339  [  800/10672]\n",
      "loss: 1.387107  [  900/10672]\n",
      "loss: 1.666165  [ 1000/10672]\n",
      "loss: 1.264900  [ 1100/10672]\n",
      "loss: 1.173912  [ 1200/10672]\n",
      "loss: 1.128840  [ 1300/10672]\n",
      "loss: 1.091151  [ 1400/10672]\n",
      "loss: 1.066433  [ 1500/10672]\n",
      "loss: 1.116208  [ 1600/10672]\n",
      "loss: 1.052420  [ 1700/10672]\n",
      "loss: 1.231857  [ 1800/10672]\n",
      "loss: 1.129083  [ 1900/10672]\n",
      "loss: 1.179643  [ 2000/10672]\n",
      "loss: 1.173746  [ 2100/10672]\n",
      "loss: 1.146930  [ 2200/10672]\n",
      "loss: 1.229382  [ 2300/10672]\n",
      "loss: 1.136125  [ 2400/10672]\n",
      "loss: 1.120073  [ 2500/10672]\n",
      "loss: 1.128376  [ 2600/10672]\n",
      "loss: 1.664746  [ 2700/10672]\n",
      "loss: 1.114580  [ 2800/10672]\n",
      "loss: 1.664701  [ 2900/10672]\n",
      "loss: 1.666267  [ 3000/10672]\n",
      "loss: 1.211492  [ 3100/10672]\n",
      "loss: 1.080416  [ 3200/10672]\n",
      "loss: 1.072794  [ 3300/10672]\n",
      "loss: 1.669618  [ 3400/10672]\n",
      "loss: 1.667209  [ 3500/10672]\n",
      "loss: 1.666611  [ 3600/10672]\n",
      "loss: 1.144109  [ 3700/10672]\n",
      "loss: 1.096603  [ 3800/10672]\n",
      "loss: 1.216020  [ 3900/10672]\n",
      "loss: 1.089793  [ 4000/10672]\n",
      "loss: 1.665141  [ 4100/10672]\n",
      "loss: 1.665396  [ 4200/10672]\n",
      "loss: 1.139638  [ 4300/10672]\n",
      "loss: 1.664672  [ 4400/10672]\n",
      "loss: 1.137329  [ 4500/10672]\n",
      "loss: 1.260547  [ 4600/10672]\n",
      "loss: 1.128075  [ 4700/10672]\n",
      "loss: 1.667242  [ 4800/10672]\n",
      "loss: 1.154006  [ 4900/10672]\n",
      "loss: 1.664395  [ 5000/10672]\n",
      "loss: 1.189303  [ 5100/10672]\n",
      "loss: 1.104523  [ 5200/10672]\n",
      "loss: 1.352311  [ 5300/10672]\n",
      "loss: 1.123381  [ 5400/10672]\n",
      "loss: 1.039677  [ 5500/10672]\n",
      "loss: 1.139010  [ 5600/10672]\n",
      "loss: 1.667475  [ 5700/10672]\n",
      "loss: 1.112362  [ 5800/10672]\n",
      "loss: 1.184841  [ 5900/10672]\n",
      "loss: 1.665255  [ 6000/10672]\n",
      "loss: 1.191753  [ 6100/10672]\n",
      "loss: 1.092047  [ 6200/10672]\n",
      "loss: 1.092864  [ 6300/10672]\n",
      "loss: 1.158702  [ 6400/10672]\n",
      "loss: 1.670154  [ 6500/10672]\n",
      "loss: 1.087594  [ 6600/10672]\n",
      "loss: 1.205865  [ 6700/10672]\n",
      "loss: 1.107934  [ 6800/10672]\n",
      "loss: 1.149244  [ 6900/10672]\n",
      "loss: 1.141682  [ 7000/10672]\n",
      "loss: 1.021275  [ 7100/10672]\n",
      "loss: 1.203034  [ 7200/10672]\n",
      "loss: 1.225874  [ 7300/10672]\n",
      "loss: 1.169059  [ 7400/10672]\n",
      "loss: 1.036839  [ 7500/10672]\n",
      "loss: 1.206898  [ 7600/10672]\n",
      "loss: 1.218914  [ 7700/10672]\n",
      "loss: 1.234268  [ 7800/10672]\n",
      "loss: 1.116747  [ 7900/10672]\n",
      "loss: 1.091283  [ 8000/10672]\n",
      "loss: 1.127904  [ 8100/10672]\n",
      "loss: 1.130003  [ 8200/10672]\n",
      "loss: 1.135153  [ 8300/10672]\n",
      "loss: 1.177448  [ 8400/10672]\n",
      "loss: 1.062499  [ 8500/10672]\n",
      "loss: 1.180663  [ 8600/10672]\n",
      "loss: 1.204192  [ 8700/10672]\n",
      "loss: 1.116091  [ 8800/10672]\n",
      "loss: 1.055244  [ 8900/10672]\n",
      "loss: 1.665095  [ 9000/10672]\n",
      "loss: 1.096439  [ 9100/10672]\n",
      "loss: 1.071259  [ 9200/10672]\n",
      "loss: 1.180354  [ 9300/10672]\n",
      "loss: 1.175293  [ 9400/10672]\n",
      "loss: 1.163059  [ 9500/10672]\n",
      "loss: 1.671387  [ 9600/10672]\n",
      "loss: 1.213983  [ 9700/10672]\n",
      "loss: 1.148670  [ 9800/10672]\n",
      "loss: 1.107011  [ 9900/10672]\n",
      "loss: 1.190382  [10000/10672]\n",
      "loss: 1.106687  [10100/10672]\n",
      "loss: 1.024848  [10200/10672]\n",
      "loss: 1.162207  [10300/10672]\n",
      "loss: 1.203236  [10400/10672]\n",
      "loss: 1.196587  [10500/10672]\n",
      "loss: 1.666564  [10600/10672]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.055650  [    0/10672]\n",
      "loss: 1.111261  [  100/10672]\n",
      "loss: 0.987345  [  200/10672]\n",
      "loss: 1.012706  [  300/10672]\n",
      "loss: 1.252560  [  400/10672]\n",
      "loss: 1.144943  [  500/10672]\n",
      "loss: 1.666041  [  600/10672]\n",
      "loss: 1.041927  [  700/10672]\n",
      "loss: 1.665396  [  800/10672]\n",
      "loss: 1.390121  [  900/10672]\n",
      "loss: 1.666208  [ 1000/10672]\n",
      "loss: 1.265517  [ 1100/10672]\n",
      "loss: 1.172661  [ 1200/10672]\n",
      "loss: 1.129081  [ 1300/10672]\n",
      "loss: 1.088445  [ 1400/10672]\n",
      "loss: 1.065434  [ 1500/10672]\n",
      "loss: 1.113931  [ 1600/10672]\n",
      "loss: 1.051232  [ 1700/10672]\n",
      "loss: 1.232114  [ 1800/10672]\n",
      "loss: 1.127200  [ 1900/10672]\n",
      "loss: 1.178696  [ 2000/10672]\n",
      "loss: 1.172829  [ 2100/10672]\n",
      "loss: 1.145457  [ 2200/10672]\n",
      "loss: 1.229706  [ 2300/10672]\n",
      "loss: 1.136133  [ 2400/10672]\n",
      "loss: 1.119797  [ 2500/10672]\n",
      "loss: 1.128318  [ 2600/10672]\n",
      "loss: 1.664802  [ 2700/10672]\n",
      "loss: 1.114136  [ 2800/10672]\n",
      "loss: 1.664845  [ 2900/10672]\n",
      "loss: 1.666357  [ 3000/10672]\n",
      "loss: 1.211748  [ 3100/10672]\n",
      "loss: 1.078021  [ 3200/10672]\n",
      "loss: 1.071612  [ 3300/10672]\n",
      "loss: 1.669811  [ 3400/10672]\n",
      "loss: 1.667309  [ 3500/10672]\n",
      "loss: 1.666740  [ 3600/10672]\n",
      "loss: 1.142712  [ 3700/10672]\n",
      "loss: 1.096011  [ 3800/10672]\n",
      "loss: 1.215935  [ 3900/10672]\n",
      "loss: 1.087158  [ 4000/10672]\n",
      "loss: 1.665209  [ 4100/10672]\n",
      "loss: 1.665528  [ 4200/10672]\n",
      "loss: 1.139804  [ 4300/10672]\n",
      "loss: 1.664735  [ 4400/10672]\n",
      "loss: 1.135739  [ 4500/10672]\n",
      "loss: 1.261600  [ 4600/10672]\n",
      "loss: 1.126424  [ 4700/10672]\n",
      "loss: 1.667354  [ 4800/10672]\n",
      "loss: 1.154498  [ 4900/10672]\n",
      "loss: 1.664477  [ 5000/10672]\n",
      "loss: 1.190524  [ 5100/10672]\n",
      "loss: 1.103820  [ 5200/10672]\n",
      "loss: 1.355191  [ 5300/10672]\n",
      "loss: 1.123279  [ 5400/10672]\n",
      "loss: 1.036507  [ 5500/10672]\n",
      "loss: 1.139096  [ 5600/10672]\n",
      "loss: 1.667592  [ 5700/10672]\n",
      "loss: 1.110384  [ 5800/10672]\n",
      "loss: 1.185864  [ 5900/10672]\n",
      "loss: 1.665345  [ 6000/10672]\n",
      "loss: 1.191489  [ 6100/10672]\n",
      "loss: 1.091100  [ 6200/10672]\n",
      "loss: 1.090802  [ 6300/10672]\n",
      "loss: 1.159414  [ 6400/10672]\n",
      "loss: 1.670329  [ 6500/10672]\n",
      "loss: 1.086899  [ 6600/10672]\n",
      "loss: 1.205698  [ 6700/10672]\n",
      "loss: 1.107412  [ 6800/10672]\n",
      "loss: 1.147847  [ 6900/10672]\n",
      "loss: 1.141943  [ 7000/10672]\n",
      "loss: 1.019309  [ 7100/10672]\n",
      "loss: 1.202839  [ 7200/10672]\n",
      "loss: 1.227702  [ 7300/10672]\n",
      "loss: 1.168390  [ 7400/10672]\n",
      "loss: 1.034904  [ 7500/10672]\n",
      "loss: 1.206707  [ 7600/10672]\n",
      "loss: 1.219121  [ 7700/10672]\n",
      "loss: 1.234693  [ 7800/10672]\n",
      "loss: 1.116483  [ 7900/10672]\n",
      "loss: 1.090633  [ 8000/10672]\n",
      "loss: 1.127689  [ 8100/10672]\n",
      "loss: 1.128266  [ 8200/10672]\n",
      "loss: 1.133562  [ 8300/10672]\n",
      "loss: 1.178363  [ 8400/10672]\n",
      "loss: 1.061369  [ 8500/10672]\n",
      "loss: 1.181609  [ 8600/10672]\n",
      "loss: 1.204113  [ 8700/10672]\n",
      "loss: 1.114421  [ 8800/10672]\n",
      "loss: 1.053889  [ 8900/10672]\n",
      "loss: 1.665157  [ 9000/10672]\n",
      "loss: 1.093931  [ 9100/10672]\n",
      "loss: 1.070068  [ 9200/10672]\n",
      "loss: 1.181464  [ 9300/10672]\n",
      "loss: 1.176384  [ 9400/10672]\n",
      "loss: 1.161771  [ 9500/10672]\n",
      "loss: 1.671544  [ 9600/10672]\n",
      "loss: 1.213719  [ 9700/10672]\n",
      "loss: 1.149270  [ 9800/10672]\n",
      "loss: 1.106788  [ 9900/10672]\n",
      "loss: 1.191842  [10000/10672]\n",
      "loss: 1.106335  [10100/10672]\n",
      "loss: 1.023056  [10200/10672]\n",
      "loss: 1.160972  [10300/10672]\n",
      "loss: 1.202650  [10400/10672]\n",
      "loss: 1.195881  [10500/10672]\n",
      "loss: 1.666690  [10600/10672]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.054646  [    0/10672]\n",
      "loss: 1.110996  [  100/10672]\n",
      "loss: 0.984965  [  200/10672]\n",
      "loss: 1.010765  [  300/10672]\n",
      "loss: 1.252874  [  400/10672]\n",
      "loss: 1.143255  [  500/10672]\n",
      "loss: 1.666090  [  600/10672]\n",
      "loss: 1.040641  [  700/10672]\n",
      "loss: 1.665451  [  800/10672]\n",
      "loss: 1.393109  [  900/10672]\n",
      "loss: 1.666248  [ 1000/10672]\n",
      "loss: 1.266132  [ 1100/10672]\n",
      "loss: 1.171416  [ 1200/10672]\n",
      "loss: 1.129317  [ 1300/10672]\n",
      "loss: 1.085769  [ 1400/10672]\n",
      "loss: 1.064442  [ 1500/10672]\n",
      "loss: 1.111673  [ 1600/10672]\n",
      "loss: 1.050049  [ 1700/10672]\n",
      "loss: 1.232375  [ 1800/10672]\n",
      "loss: 1.125336  [ 1900/10672]\n",
      "loss: 1.177757  [ 2000/10672]\n",
      "loss: 1.171926  [ 2100/10672]\n",
      "loss: 1.143999  [ 2200/10672]\n",
      "loss: 1.230031  [ 2300/10672]\n",
      "loss: 1.136137  [ 2400/10672]\n",
      "loss: 1.119519  [ 2500/10672]\n",
      "loss: 1.128261  [ 2600/10672]\n",
      "loss: 1.664856  [ 2700/10672]\n",
      "loss: 1.113691  [ 2800/10672]\n",
      "loss: 1.664985  [ 2900/10672]\n",
      "loss: 1.666444  [ 3000/10672]\n",
      "loss: 1.212009  [ 3100/10672]\n",
      "loss: 1.075656  [ 3200/10672]\n",
      "loss: 1.070437  [ 3300/10672]\n",
      "loss: 1.670003  [ 3400/10672]\n",
      "loss: 1.667408  [ 3500/10672]\n",
      "loss: 1.666870  [ 3600/10672]\n",
      "loss: 1.141326  [ 3700/10672]\n",
      "loss: 1.095420  [ 3800/10672]\n",
      "loss: 1.215852  [ 3900/10672]\n",
      "loss: 1.084549  [ 4000/10672]\n",
      "loss: 1.665274  [ 4100/10672]\n",
      "loss: 1.665659  [ 4200/10672]\n",
      "loss: 1.139963  [ 4300/10672]\n",
      "loss: 1.664794  [ 4400/10672]\n",
      "loss: 1.134163  [ 4500/10672]\n",
      "loss: 1.262652  [ 4600/10672]\n",
      "loss: 1.124792  [ 4700/10672]\n",
      "loss: 1.667463  [ 4800/10672]\n",
      "loss: 1.154988  [ 4900/10672]\n",
      "loss: 1.664556  [ 5000/10672]\n",
      "loss: 1.191738  [ 5100/10672]\n",
      "loss: 1.103114  [ 5200/10672]\n",
      "loss: 1.358054  [ 5300/10672]\n",
      "loss: 1.123173  [ 5400/10672]\n",
      "loss: 1.033379  [ 5500/10672]\n",
      "loss: 1.139176  [ 5600/10672]\n",
      "loss: 1.667707  [ 5700/10672]\n",
      "loss: 1.108423  [ 5800/10672]\n",
      "loss: 1.186879  [ 5900/10672]\n",
      "loss: 1.665432  [ 6000/10672]\n",
      "loss: 1.191232  [ 6100/10672]\n",
      "loss: 1.090159  [ 6200/10672]\n",
      "loss: 1.088768  [ 6300/10672]\n",
      "loss: 1.160124  [ 6400/10672]\n",
      "loss: 1.670503  [ 6500/10672]\n",
      "loss: 1.086210  [ 6600/10672]\n",
      "loss: 1.205537  [ 6700/10672]\n",
      "loss: 1.106892  [ 6800/10672]\n",
      "loss: 1.146458  [ 6900/10672]\n",
      "loss: 1.142202  [ 7000/10672]\n",
      "loss: 1.017362  [ 7100/10672]\n",
      "loss: 1.202647  [ 7200/10672]\n",
      "loss: 1.229519  [ 7300/10672]\n",
      "loss: 1.167729  [ 7400/10672]\n",
      "loss: 1.032988  [ 7500/10672]\n",
      "loss: 1.206518  [ 7600/10672]\n",
      "loss: 1.219327  [ 7700/10672]\n",
      "loss: 1.235114  [ 7800/10672]\n",
      "loss: 1.116217  [ 7900/10672]\n",
      "loss: 1.089986  [ 8000/10672]\n",
      "loss: 1.127470  [ 8100/10672]\n",
      "loss: 1.126547  [ 8200/10672]\n",
      "loss: 1.131988  [ 8300/10672]\n",
      "loss: 1.179273  [ 8400/10672]\n",
      "loss: 1.060248  [ 8500/10672]\n",
      "loss: 1.182550  [ 8600/10672]\n",
      "loss: 1.204046  [ 8700/10672]\n",
      "loss: 1.112773  [ 8800/10672]\n",
      "loss: 1.052544  [ 8900/10672]\n",
      "loss: 1.665216  [ 9000/10672]\n",
      "loss: 1.091443  [ 9100/10672]\n",
      "loss: 1.068882  [ 9200/10672]\n",
      "loss: 1.182565  [ 9300/10672]\n",
      "loss: 1.177465  [ 9400/10672]\n",
      "loss: 1.160498  [ 9500/10672]\n",
      "loss: 1.671700  [ 9600/10672]\n",
      "loss: 1.213457  [ 9700/10672]\n",
      "loss: 1.149865  [ 9800/10672]\n",
      "loss: 1.106563  [ 9900/10672]\n",
      "loss: 1.193291  [10000/10672]\n",
      "loss: 1.105978  [10100/10672]\n",
      "loss: 1.021277  [10200/10672]\n",
      "loss: 1.159751  [10300/10672]\n",
      "loss: 1.202073  [10400/10672]\n",
      "loss: 1.195185  [10500/10672]\n",
      "loss: 1.666814  [10600/10672]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.053651  [    0/10672]\n",
      "loss: 1.110723  [  100/10672]\n",
      "loss: 0.982610  [  200/10672]\n",
      "loss: 1.008840  [  300/10672]\n",
      "loss: 1.253186  [  400/10672]\n",
      "loss: 1.141586  [  500/10672]\n",
      "loss: 1.666136  [  600/10672]\n",
      "loss: 1.039361  [  700/10672]\n",
      "loss: 1.665502  [  800/10672]\n",
      "loss: 1.396072  [  900/10672]\n",
      "loss: 1.666286  [ 1000/10672]\n",
      "loss: 1.266742  [ 1100/10672]\n",
      "loss: 1.170179  [ 1200/10672]\n",
      "loss: 1.129546  [ 1300/10672]\n",
      "loss: 1.083123  [ 1400/10672]\n",
      "loss: 1.063455  [ 1500/10672]\n",
      "loss: 1.109436  [ 1600/10672]\n",
      "loss: 1.048872  [ 1700/10672]\n",
      "loss: 1.232639  [ 1800/10672]\n",
      "loss: 1.123492  [ 1900/10672]\n",
      "loss: 1.176825  [ 2000/10672]\n",
      "loss: 1.171037  [ 2100/10672]\n",
      "loss: 1.142558  [ 2200/10672]\n",
      "loss: 1.230359  [ 2300/10672]\n",
      "loss: 1.136134  [ 2400/10672]\n",
      "loss: 1.119237  [ 2500/10672]\n",
      "loss: 1.128205  [ 2600/10672]\n",
      "loss: 1.664907  [ 2700/10672]\n",
      "loss: 1.113244  [ 2800/10672]\n",
      "loss: 1.665122  [ 2900/10672]\n",
      "loss: 1.666528  [ 3000/10672]\n",
      "loss: 1.212278  [ 3100/10672]\n",
      "loss: 1.073323  [ 3200/10672]\n",
      "loss: 1.069266  [ 3300/10672]\n",
      "loss: 1.670193  [ 3400/10672]\n",
      "loss: 1.667504  [ 3500/10672]\n",
      "loss: 1.666998  [ 3600/10672]\n",
      "loss: 1.139954  [ 3700/10672]\n",
      "loss: 1.094829  [ 3800/10672]\n",
      "loss: 1.215772  [ 3900/10672]\n",
      "loss: 1.081968  [ 4000/10672]\n",
      "loss: 1.665337  [ 4100/10672]\n",
      "loss: 1.665790  [ 4200/10672]\n",
      "loss: 1.140115  [ 4300/10672]\n",
      "loss: 1.664851  [ 4400/10672]\n",
      "loss: 1.132600  [ 4500/10672]\n",
      "loss: 1.263703  [ 4600/10672]\n",
      "loss: 1.123179  [ 4700/10672]\n",
      "loss: 1.667571  [ 4800/10672]\n",
      "loss: 1.155474  [ 4900/10672]\n",
      "loss: 1.664631  [ 5000/10672]\n",
      "loss: 1.192946  [ 5100/10672]\n",
      "loss: 1.102405  [ 5200/10672]\n",
      "loss: 1.360899  [ 5300/10672]\n",
      "loss: 1.123062  [ 5400/10672]\n",
      "loss: 1.030294  [ 5500/10672]\n",
      "loss: 1.139246  [ 5600/10672]\n",
      "loss: 1.667820  [ 5700/10672]\n",
      "loss: 1.106478  [ 5800/10672]\n",
      "loss: 1.187885  [ 5900/10672]\n",
      "loss: 1.665517  [ 6000/10672]\n",
      "loss: 1.190984  [ 6100/10672]\n",
      "loss: 1.089222  [ 6200/10672]\n",
      "loss: 1.086762  [ 6300/10672]\n",
      "loss: 1.160829  [ 6400/10672]\n",
      "loss: 1.670676  [ 6500/10672]\n",
      "loss: 1.085524  [ 6600/10672]\n",
      "loss: 1.205384  [ 6700/10672]\n",
      "loss: 1.106369  [ 6800/10672]\n",
      "loss: 1.145078  [ 6900/10672]\n",
      "loss: 1.142460  [ 7000/10672]\n",
      "loss: 1.015434  [ 7100/10672]\n",
      "loss: 1.202459  [ 7200/10672]\n",
      "loss: 1.231320  [ 7300/10672]\n",
      "loss: 1.167078  [ 7400/10672]\n",
      "loss: 1.031090  [ 7500/10672]\n",
      "loss: 1.206333  [ 7600/10672]\n",
      "loss: 1.219536  [ 7700/10672]\n",
      "loss: 1.235532  [ 7800/10672]\n",
      "loss: 1.115947  [ 7900/10672]\n",
      "loss: 1.089341  [ 8000/10672]\n",
      "loss: 1.127244  [ 8100/10672]\n",
      "loss: 1.124848  [ 8200/10672]\n",
      "loss: 1.130431  [ 8300/10672]\n",
      "loss: 1.180177  [ 8400/10672]\n",
      "loss: 1.059135  [ 8500/10672]\n",
      "loss: 1.183487  [ 8600/10672]\n",
      "loss: 1.203991  [ 8700/10672]\n",
      "loss: 1.111147  [ 8800/10672]\n",
      "loss: 1.051208  [ 8900/10672]\n",
      "loss: 1.665275  [ 9000/10672]\n",
      "loss: 1.088978  [ 9100/10672]\n",
      "loss: 1.067701  [ 9200/10672]\n",
      "loss: 1.183659  [ 9300/10672]\n",
      "loss: 1.178539  [ 9400/10672]\n",
      "loss: 1.159238  [ 9500/10672]\n",
      "loss: 1.671855  [ 9600/10672]\n",
      "loss: 1.213198  [ 9700/10672]\n",
      "loss: 1.150456  [ 9800/10672]\n",
      "loss: 1.106339  [ 9900/10672]\n",
      "loss: 1.194730  [10000/10672]\n",
      "loss: 1.105616  [10100/10672]\n",
      "loss: 1.019513  [10200/10672]\n",
      "loss: 1.158545  [10300/10672]\n",
      "loss: 1.201506  [10400/10672]\n",
      "loss: 1.194495  [10500/10672]\n",
      "loss: 1.666934  [10600/10672]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.052666  [    0/10672]\n",
      "loss: 1.110444  [  100/10672]\n",
      "loss: 0.980283  [  200/10672]\n",
      "loss: 1.006932  [  300/10672]\n",
      "loss: 1.253498  [  400/10672]\n",
      "loss: 1.139933  [  500/10672]\n",
      "loss: 1.666179  [  600/10672]\n",
      "loss: 1.038087  [  700/10672]\n",
      "loss: 1.665551  [  800/10672]\n",
      "loss: 1.399009  [  900/10672]\n",
      "loss: 1.666323  [ 1000/10672]\n",
      "loss: 1.267350  [ 1100/10672]\n",
      "loss: 1.168949  [ 1200/10672]\n",
      "loss: 1.129768  [ 1300/10672]\n",
      "loss: 1.080507  [ 1400/10672]\n",
      "loss: 1.062474  [ 1500/10672]\n",
      "loss: 1.107220  [ 1600/10672]\n",
      "loss: 1.047701  [ 1700/10672]\n",
      "loss: 1.232907  [ 1800/10672]\n",
      "loss: 1.121666  [ 1900/10672]\n",
      "loss: 1.175900  [ 2000/10672]\n",
      "loss: 1.170161  [ 2100/10672]\n",
      "loss: 1.141133  [ 2200/10672]\n",
      "loss: 1.230690  [ 2300/10672]\n",
      "loss: 1.136126  [ 2400/10672]\n",
      "loss: 1.118953  [ 2500/10672]\n",
      "loss: 1.128150  [ 2600/10672]\n",
      "loss: 1.664956  [ 2700/10672]\n",
      "loss: 1.112795  [ 2800/10672]\n",
      "loss: 1.665256  [ 2900/10672]\n",
      "loss: 1.666610  [ 3000/10672]\n",
      "loss: 1.212552  [ 3100/10672]\n",
      "loss: 1.071019  [ 3200/10672]\n",
      "loss: 1.068101  [ 3300/10672]\n",
      "loss: 1.670382  [ 3400/10672]\n",
      "loss: 1.667599  [ 3500/10672]\n",
      "loss: 1.667125  [ 3600/10672]\n",
      "loss: 1.138594  [ 3700/10672]\n",
      "loss: 1.094239  [ 3800/10672]\n",
      "loss: 1.215693  [ 3900/10672]\n",
      "loss: 1.079411  [ 4000/10672]\n",
      "loss: 1.665397  [ 4100/10672]\n",
      "loss: 1.665920  [ 4200/10672]\n",
      "loss: 1.140260  [ 4300/10672]\n",
      "loss: 1.664905  [ 4400/10672]\n",
      "loss: 1.131050  [ 4500/10672]\n",
      "loss: 1.264752  [ 4600/10672]\n",
      "loss: 1.121585  [ 4700/10672]\n",
      "loss: 1.667676  [ 4800/10672]\n",
      "loss: 1.155957  [ 4900/10672]\n",
      "loss: 1.664703  [ 5000/10672]\n",
      "loss: 1.194149  [ 5100/10672]\n",
      "loss: 1.101694  [ 5200/10672]\n",
      "loss: 1.363726  [ 5300/10672]\n",
      "loss: 1.122949  [ 5400/10672]\n",
      "loss: 1.027250  [ 5500/10672]\n",
      "loss: 1.139310  [ 5600/10672]\n",
      "loss: 1.667931  [ 5700/10672]\n",
      "loss: 1.104548  [ 5800/10672]\n",
      "loss: 1.188884  [ 5900/10672]\n",
      "loss: 1.665599  [ 6000/10672]\n",
      "loss: 1.190742  [ 6100/10672]\n",
      "loss: 1.088292  [ 6200/10672]\n",
      "loss: 1.084783  [ 6300/10672]\n",
      "loss: 1.161533  [ 6400/10672]\n",
      "loss: 1.670848  [ 6500/10672]\n",
      "loss: 1.084844  [ 6600/10672]\n",
      "loss: 1.205236  [ 6700/10672]\n",
      "loss: 1.105850  [ 6800/10672]\n",
      "loss: 1.143704  [ 6900/10672]\n",
      "loss: 1.142718  [ 7000/10672]\n",
      "loss: 1.013527  [ 7100/10672]\n",
      "loss: 1.202273  [ 7200/10672]\n",
      "loss: 1.233108  [ 7300/10672]\n",
      "loss: 1.166436  [ 7400/10672]\n",
      "loss: 1.029209  [ 7500/10672]\n",
      "loss: 1.206149  [ 7600/10672]\n",
      "loss: 1.219743  [ 7700/10672]\n",
      "loss: 1.235945  [ 7800/10672]\n",
      "loss: 1.115676  [ 7900/10672]\n",
      "loss: 1.088699  [ 8000/10672]\n",
      "loss: 1.127015  [ 8100/10672]\n",
      "loss: 1.123166  [ 8200/10672]\n",
      "loss: 1.128890  [ 8300/10672]\n",
      "loss: 1.181077  [ 8400/10672]\n",
      "loss: 1.058032  [ 8500/10672]\n",
      "loss: 1.184419  [ 8600/10672]\n",
      "loss: 1.203946  [ 8700/10672]\n",
      "loss: 1.109540  [ 8800/10672]\n",
      "loss: 1.049881  [ 8900/10672]\n",
      "loss: 1.665331  [ 9000/10672]\n",
      "loss: 1.086532  [ 9100/10672]\n",
      "loss: 1.066527  [ 9200/10672]\n",
      "loss: 1.184747  [ 9300/10672]\n",
      "loss: 1.179605  [ 9400/10672]\n",
      "loss: 1.157991  [ 9500/10672]\n",
      "loss: 1.672009  [ 9600/10672]\n",
      "loss: 1.212938  [ 9700/10672]\n",
      "loss: 1.151045  [ 9800/10672]\n",
      "loss: 1.106117  [ 9900/10672]\n",
      "loss: 1.196162  [10000/10672]\n",
      "loss: 1.105252  [10100/10672]\n",
      "loss: 1.017764  [10200/10672]\n",
      "loss: 1.157350  [10300/10672]\n",
      "loss: 1.200945  [10400/10672]\n",
      "loss: 1.193813  [10500/10672]\n",
      "loss: 1.667052  [10600/10672]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.051692  [    0/10672]\n",
      "loss: 1.110160  [  100/10672]\n",
      "loss: 0.977984  [  200/10672]\n",
      "loss: 1.005041  [  300/10672]\n",
      "loss: 1.253807  [  400/10672]\n",
      "loss: 1.138296  [  500/10672]\n",
      "loss: 1.666221  [  600/10672]\n",
      "loss: 1.036819  [  700/10672]\n",
      "loss: 1.665598  [  800/10672]\n",
      "loss: 1.401919  [  900/10672]\n",
      "loss: 1.666357  [ 1000/10672]\n",
      "loss: 1.267953  [ 1100/10672]\n",
      "loss: 1.167725  [ 1200/10672]\n",
      "loss: 1.129986  [ 1300/10672]\n",
      "loss: 1.077918  [ 1400/10672]\n",
      "loss: 1.061500  [ 1500/10672]\n",
      "loss: 1.105023  [ 1600/10672]\n",
      "loss: 1.046536  [ 1700/10672]\n",
      "loss: 1.233177  [ 1800/10672]\n",
      "loss: 1.119857  [ 1900/10672]\n",
      "loss: 1.174981  [ 2000/10672]\n",
      "loss: 1.169299  [ 2100/10672]\n",
      "loss: 1.139722  [ 2200/10672]\n",
      "loss: 1.231021  [ 2300/10672]\n",
      "loss: 1.136114  [ 2400/10672]\n",
      "loss: 1.118666  [ 2500/10672]\n",
      "loss: 1.128096  [ 2600/10672]\n",
      "loss: 1.665004  [ 2700/10672]\n",
      "loss: 1.112344  [ 2800/10672]\n",
      "loss: 1.665386  [ 2900/10672]\n",
      "loss: 1.666691  [ 3000/10672]\n",
      "loss: 1.212833  [ 3100/10672]\n",
      "loss: 1.068745  [ 3200/10672]\n",
      "loss: 1.066941  [ 3300/10672]\n",
      "loss: 1.670570  [ 3400/10672]\n",
      "loss: 1.667692  [ 3500/10672]\n",
      "loss: 1.667253  [ 3600/10672]\n",
      "loss: 1.137248  [ 3700/10672]\n",
      "loss: 1.093650  [ 3800/10672]\n",
      "loss: 1.215617  [ 3900/10672]\n",
      "loss: 1.076882  [ 4000/10672]\n",
      "loss: 1.665455  [ 4100/10672]\n",
      "loss: 1.666049  [ 4200/10672]\n",
      "loss: 1.140397  [ 4300/10672]\n",
      "loss: 1.664956  [ 4400/10672]\n",
      "loss: 1.129515  [ 4500/10672]\n",
      "loss: 1.265801  [ 4600/10672]\n",
      "loss: 1.120010  [ 4700/10672]\n",
      "loss: 1.667780  [ 4800/10672]\n",
      "loss: 1.156435  [ 4900/10672]\n",
      "loss: 1.664772  [ 5000/10672]\n",
      "loss: 1.195345  [ 5100/10672]\n",
      "loss: 1.100979  [ 5200/10672]\n",
      "loss: 1.366536  [ 5300/10672]\n",
      "loss: 1.122830  [ 5400/10672]\n",
      "loss: 1.024248  [ 5500/10672]\n",
      "loss: 1.139366  [ 5600/10672]\n",
      "loss: 1.668041  [ 5700/10672]\n",
      "loss: 1.102635  [ 5800/10672]\n",
      "loss: 1.189873  [ 5900/10672]\n",
      "loss: 1.665679  [ 6000/10672]\n",
      "loss: 1.190508  [ 6100/10672]\n",
      "loss: 1.087365  [ 6200/10672]\n",
      "loss: 1.082833  [ 6300/10672]\n",
      "loss: 1.162232  [ 6400/10672]\n",
      "loss: 1.671020  [ 6500/10672]\n",
      "loss: 1.084167  [ 6600/10672]\n",
      "loss: 1.205095  [ 6700/10672]\n",
      "loss: 1.105328  [ 6800/10672]\n",
      "loss: 1.142340  [ 6900/10672]\n",
      "loss: 1.142973  [ 7000/10672]\n",
      "loss: 1.011637  [ 7100/10672]\n",
      "loss: 1.202093  [ 7200/10672]\n",
      "loss: 1.234880  [ 7300/10672]\n",
      "loss: 1.165804  [ 7400/10672]\n",
      "loss: 1.027346  [ 7500/10672]\n",
      "loss: 1.205969  [ 7600/10672]\n",
      "loss: 1.219952  [ 7700/10672]\n",
      "loss: 1.236355  [ 7800/10672]\n",
      "loss: 1.115400  [ 7900/10672]\n",
      "loss: 1.088058  [ 8000/10672]\n",
      "loss: 1.126780  [ 8100/10672]\n",
      "loss: 1.121503  [ 8200/10672]\n",
      "loss: 1.127366  [ 8300/10672]\n",
      "loss: 1.181969  [ 8400/10672]\n",
      "loss: 1.056937  [ 8500/10672]\n",
      "loss: 1.185347  [ 8600/10672]\n",
      "loss: 1.203912  [ 8700/10672]\n",
      "loss: 1.107953  [ 8800/10672]\n",
      "loss: 1.048564  [ 8900/10672]\n",
      "loss: 1.665386  [ 9000/10672]\n",
      "loss: 1.084107  [ 9100/10672]\n",
      "loss: 1.065360  [ 9200/10672]\n",
      "loss: 1.185827  [ 9300/10672]\n",
      "loss: 1.180663  [ 9400/10672]\n",
      "loss: 1.156756  [ 9500/10672]\n",
      "loss: 1.672163  [ 9600/10672]\n",
      "loss: 1.212679  [ 9700/10672]\n",
      "loss: 1.151631  [ 9800/10672]\n",
      "loss: 1.105894  [ 9900/10672]\n",
      "loss: 1.197583  [10000/10672]\n",
      "loss: 1.104885  [10100/10672]\n",
      "loss: 1.016031  [10200/10672]\n",
      "loss: 1.156167  [10300/10672]\n",
      "loss: 1.200393  [10400/10672]\n",
      "loss: 1.193137  [10500/10672]\n",
      "loss: 1.667167  [10600/10672]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.050728  [    0/10672]\n",
      "loss: 1.109869  [  100/10672]\n",
      "loss: 0.975711  [  200/10672]\n",
      "loss: 1.003168  [  300/10672]\n",
      "loss: 1.254114  [  400/10672]\n",
      "loss: 1.136675  [  500/10672]\n",
      "loss: 1.666262  [  600/10672]\n",
      "loss: 1.035558  [  700/10672]\n",
      "loss: 1.665642  [  800/10672]\n",
      "loss: 1.404803  [  900/10672]\n",
      "loss: 1.666391  [ 1000/10672]\n",
      "loss: 1.268551  [ 1100/10672]\n",
      "loss: 1.166507  [ 1200/10672]\n",
      "loss: 1.130198  [ 1300/10672]\n",
      "loss: 1.075357  [ 1400/10672]\n",
      "loss: 1.060532  [ 1500/10672]\n",
      "loss: 1.102846  [ 1600/10672]\n",
      "loss: 1.045378  [ 1700/10672]\n",
      "loss: 1.233449  [ 1800/10672]\n",
      "loss: 1.118066  [ 1900/10672]\n",
      "loss: 1.174068  [ 2000/10672]\n",
      "loss: 1.168448  [ 2100/10672]\n",
      "loss: 1.138325  [ 2200/10672]\n",
      "loss: 1.231353  [ 2300/10672]\n",
      "loss: 1.136097  [ 2400/10672]\n",
      "loss: 1.118377  [ 2500/10672]\n",
      "loss: 1.128043  [ 2600/10672]\n",
      "loss: 1.665049  [ 2700/10672]\n",
      "loss: 1.111892  [ 2800/10672]\n",
      "loss: 1.665514  [ 2900/10672]\n",
      "loss: 1.666769  [ 3000/10672]\n",
      "loss: 1.213119  [ 3100/10672]\n",
      "loss: 1.066499  [ 3200/10672]\n",
      "loss: 1.065787  [ 3300/10672]\n",
      "loss: 1.670757  [ 3400/10672]\n",
      "loss: 1.667784  [ 3500/10672]\n",
      "loss: 1.667379  [ 3600/10672]\n",
      "loss: 1.135912  [ 3700/10672]\n",
      "loss: 1.093061  [ 3800/10672]\n",
      "loss: 1.215543  [ 3900/10672]\n",
      "loss: 1.074378  [ 4000/10672]\n",
      "loss: 1.665512  [ 4100/10672]\n",
      "loss: 1.666178  [ 4200/10672]\n",
      "loss: 1.140527  [ 4300/10672]\n",
      "loss: 1.665006  [ 4400/10672]\n",
      "loss: 1.127992  [ 4500/10672]\n",
      "loss: 1.266848  [ 4600/10672]\n",
      "loss: 1.118453  [ 4700/10672]\n",
      "loss: 1.667883  [ 4800/10672]\n",
      "loss: 1.156909  [ 4900/10672]\n",
      "loss: 1.664839  [ 5000/10672]\n",
      "loss: 1.196535  [ 5100/10672]\n",
      "loss: 1.100261  [ 5200/10672]\n",
      "loss: 1.369327  [ 5300/10672]\n",
      "loss: 1.122708  [ 5400/10672]\n",
      "loss: 1.021286  [ 5500/10672]\n",
      "loss: 1.139413  [ 5600/10672]\n",
      "loss: 1.668149  [ 5700/10672]\n",
      "loss: 1.100738  [ 5800/10672]\n",
      "loss: 1.190855  [ 5900/10672]\n",
      "loss: 1.665758  [ 6000/10672]\n",
      "loss: 1.190281  [ 6100/10672]\n",
      "loss: 1.086443  [ 6200/10672]\n",
      "loss: 1.080908  [ 6300/10672]\n",
      "loss: 1.162928  [ 6400/10672]\n",
      "loss: 1.671190  [ 6500/10672]\n",
      "loss: 1.083495  [ 6600/10672]\n",
      "loss: 1.204960  [ 6700/10672]\n",
      "loss: 1.104807  [ 6800/10672]\n",
      "loss: 1.140983  [ 6900/10672]\n",
      "loss: 1.143227  [ 7000/10672]\n",
      "loss: 1.009768  [ 7100/10672]\n",
      "loss: 1.201914  [ 7200/10672]\n",
      "loss: 1.236640  [ 7300/10672]\n",
      "loss: 1.165180  [ 7400/10672]\n",
      "loss: 1.025500  [ 7500/10672]\n",
      "loss: 1.205792  [ 7600/10672]\n",
      "loss: 1.220161  [ 7700/10672]\n",
      "loss: 1.236761  [ 7800/10672]\n",
      "loss: 1.115122  [ 7900/10672]\n",
      "loss: 1.087419  [ 8000/10672]\n",
      "loss: 1.126540  [ 8100/10672]\n",
      "loss: 1.119859  [ 8200/10672]\n",
      "loss: 1.125859  [ 8300/10672]\n",
      "loss: 1.182856  [ 8400/10672]\n",
      "loss: 1.055850  [ 8500/10672]\n",
      "loss: 1.186268  [ 8600/10672]\n",
      "loss: 1.203890  [ 8700/10672]\n",
      "loss: 1.106387  [ 8800/10672]\n",
      "loss: 1.047255  [ 8900/10672]\n",
      "loss: 1.665440  [ 9000/10672]\n",
      "loss: 1.081704  [ 9100/10672]\n",
      "loss: 1.064197  [ 9200/10672]\n",
      "loss: 1.186899  [ 9300/10672]\n",
      "loss: 1.181712  [ 9400/10672]\n",
      "loss: 1.155536  [ 9500/10672]\n",
      "loss: 1.672316  [ 9600/10672]\n",
      "loss: 1.212422  [ 9700/10672]\n",
      "loss: 1.152212  [ 9800/10672]\n",
      "loss: 1.105672  [ 9900/10672]\n",
      "loss: 1.198996  [10000/10672]\n",
      "loss: 1.104514  [10100/10672]\n",
      "loss: 1.014311  [10200/10672]\n",
      "loss: 1.154998  [10300/10672]\n",
      "loss: 1.199848  [10400/10672]\n",
      "loss: 1.192468  [10500/10672]\n",
      "loss: 1.667279  [10600/10672]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.049774  [    0/10672]\n",
      "loss: 1.109573  [  100/10672]\n",
      "loss: 0.973464  [  200/10672]\n",
      "loss: 1.001311  [  300/10672]\n",
      "loss: 1.254421  [  400/10672]\n",
      "loss: 1.135071  [  500/10672]\n",
      "loss: 1.666300  [  600/10672]\n",
      "loss: 1.034303  [  700/10672]\n",
      "loss: 1.665685  [  800/10672]\n",
      "loss: 1.407661  [  900/10672]\n",
      "loss: 1.666422  [ 1000/10672]\n",
      "loss: 1.269146  [ 1100/10672]\n",
      "loss: 1.165297  [ 1200/10672]\n",
      "loss: 1.130403  [ 1300/10672]\n",
      "loss: 1.072825  [ 1400/10672]\n",
      "loss: 1.059570  [ 1500/10672]\n",
      "loss: 1.100688  [ 1600/10672]\n",
      "loss: 1.044225  [ 1700/10672]\n",
      "loss: 1.233724  [ 1800/10672]\n",
      "loss: 1.116294  [ 1900/10672]\n",
      "loss: 1.173163  [ 2000/10672]\n",
      "loss: 1.167612  [ 2100/10672]\n",
      "loss: 1.136944  [ 2200/10672]\n",
      "loss: 1.231686  [ 2300/10672]\n",
      "loss: 1.136074  [ 2400/10672]\n",
      "loss: 1.118086  [ 2500/10672]\n",
      "loss: 1.127991  [ 2600/10672]\n",
      "loss: 1.665092  [ 2700/10672]\n",
      "loss: 1.111438  [ 2800/10672]\n",
      "loss: 1.665640  [ 2900/10672]\n",
      "loss: 1.666845  [ 3000/10672]\n",
      "loss: 1.213411  [ 3100/10672]\n",
      "loss: 1.064282  [ 3200/10672]\n",
      "loss: 1.064638  [ 3300/10672]\n",
      "loss: 1.670943  [ 3400/10672]\n",
      "loss: 1.667874  [ 3500/10672]\n",
      "loss: 1.667505  [ 3600/10672]\n",
      "loss: 1.134588  [ 3700/10672]\n",
      "loss: 1.092473  [ 3800/10672]\n",
      "loss: 1.215469  [ 3900/10672]\n",
      "loss: 1.071898  [ 4000/10672]\n",
      "loss: 1.665566  [ 4100/10672]\n",
      "loss: 1.666307  [ 4200/10672]\n",
      "loss: 1.140651  [ 4300/10672]\n",
      "loss: 1.665053  [ 4400/10672]\n",
      "loss: 1.126480  [ 4500/10672]\n",
      "loss: 1.267892  [ 4600/10672]\n",
      "loss: 1.116913  [ 4700/10672]\n",
      "loss: 1.667984  [ 4800/10672]\n",
      "loss: 1.157380  [ 4900/10672]\n",
      "loss: 1.664903  [ 5000/10672]\n",
      "loss: 1.197720  [ 5100/10672]\n",
      "loss: 1.099542  [ 5200/10672]\n",
      "loss: 1.372098  [ 5300/10672]\n",
      "loss: 1.122583  [ 5400/10672]\n",
      "loss: 1.018365  [ 5500/10672]\n",
      "loss: 1.139454  [ 5600/10672]\n",
      "loss: 1.668255  [ 5700/10672]\n",
      "loss: 1.098855  [ 5800/10672]\n",
      "loss: 1.191829  [ 5900/10672]\n",
      "loss: 1.665834  [ 6000/10672]\n",
      "loss: 1.190060  [ 6100/10672]\n",
      "loss: 1.085527  [ 6200/10672]\n",
      "loss: 1.079010  [ 6300/10672]\n",
      "loss: 1.163622  [ 6400/10672]\n",
      "loss: 1.671359  [ 6500/10672]\n",
      "loss: 1.082828  [ 6600/10672]\n",
      "loss: 1.204830  [ 6700/10672]\n",
      "loss: 1.104287  [ 6800/10672]\n",
      "loss: 1.139632  [ 6900/10672]\n",
      "loss: 1.143481  [ 7000/10672]\n",
      "loss: 1.007917  [ 7100/10672]\n",
      "loss: 1.201739  [ 7200/10672]\n",
      "loss: 1.238385  [ 7300/10672]\n",
      "loss: 1.164564  [ 7400/10672]\n",
      "loss: 1.023671  [ 7500/10672]\n",
      "loss: 1.205616  [ 7600/10672]\n",
      "loss: 1.220369  [ 7700/10672]\n",
      "loss: 1.237163  [ 7800/10672]\n",
      "loss: 1.114840  [ 7900/10672]\n",
      "loss: 1.086783  [ 8000/10672]\n",
      "loss: 1.126295  [ 8100/10672]\n",
      "loss: 1.118232  [ 8200/10672]\n",
      "loss: 1.124366  [ 8300/10672]\n",
      "loss: 1.183737  [ 8400/10672]\n",
      "loss: 1.054772  [ 8500/10672]\n",
      "loss: 1.187186  [ 8600/10672]\n",
      "loss: 1.203877  [ 8700/10672]\n",
      "loss: 1.104840  [ 8800/10672]\n",
      "loss: 1.045955  [ 8900/10672]\n",
      "loss: 1.665492  [ 9000/10672]\n",
      "loss: 1.079321  [ 9100/10672]\n",
      "loss: 1.063040  [ 9200/10672]\n",
      "loss: 1.187965  [ 9300/10672]\n",
      "loss: 1.182753  [ 9400/10672]\n",
      "loss: 1.154327  [ 9500/10672]\n",
      "loss: 1.672468  [ 9600/10672]\n",
      "loss: 1.212165  [ 9700/10672]\n",
      "loss: 1.152791  [ 9800/10672]\n",
      "loss: 1.105451  [ 9900/10672]\n",
      "loss: 1.200398  [10000/10672]\n",
      "loss: 1.104138  [10100/10672]\n",
      "loss: 1.012605  [10200/10672]\n",
      "loss: 1.153841  [10300/10672]\n",
      "loss: 1.199311  [10400/10672]\n",
      "loss: 1.191806  [10500/10672]\n",
      "loss: 1.667390  [10600/10672]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.048830  [    0/10672]\n",
      "loss: 1.109271  [  100/10672]\n",
      "loss: 0.971244  [  200/10672]\n",
      "loss: 0.999471  [  300/10672]\n",
      "loss: 1.254724  [  400/10672]\n",
      "loss: 1.133482  [  500/10672]\n",
      "loss: 1.666337  [  600/10672]\n",
      "loss: 1.033054  [  700/10672]\n",
      "loss: 1.665725  [  800/10672]\n",
      "loss: 1.410491  [  900/10672]\n",
      "loss: 1.666453  [ 1000/10672]\n",
      "loss: 1.269736  [ 1100/10672]\n",
      "loss: 1.164091  [ 1200/10672]\n",
      "loss: 1.130603  [ 1300/10672]\n",
      "loss: 1.070321  [ 1400/10672]\n",
      "loss: 1.058614  [ 1500/10672]\n",
      "loss: 1.098549  [ 1600/10672]\n",
      "loss: 1.043079  [ 1700/10672]\n",
      "loss: 1.234001  [ 1800/10672]\n",
      "loss: 1.114537  [ 1900/10672]\n",
      "loss: 1.172263  [ 2000/10672]\n",
      "loss: 1.166787  [ 2100/10672]\n",
      "loss: 1.135577  [ 2200/10672]\n",
      "loss: 1.232021  [ 2300/10672]\n",
      "loss: 1.136047  [ 2400/10672]\n",
      "loss: 1.117793  [ 2500/10672]\n",
      "loss: 1.127940  [ 2600/10672]\n",
      "loss: 1.665135  [ 2700/10672]\n",
      "loss: 1.110982  [ 2800/10672]\n",
      "loss: 1.665762  [ 2900/10672]\n",
      "loss: 1.666919  [ 3000/10672]\n",
      "loss: 1.213707  [ 3100/10672]\n",
      "loss: 1.062093  [ 3200/10672]\n",
      "loss: 1.063494  [ 3300/10672]\n",
      "loss: 1.671128  [ 3400/10672]\n",
      "loss: 1.667964  [ 3500/10672]\n",
      "loss: 1.667631  [ 3600/10672]\n",
      "loss: 1.133276  [ 3700/10672]\n",
      "loss: 1.091887  [ 3800/10672]\n",
      "loss: 1.215397  [ 3900/10672]\n",
      "loss: 1.069443  [ 4000/10672]\n",
      "loss: 1.665618  [ 4100/10672]\n",
      "loss: 1.666435  [ 4200/10672]\n",
      "loss: 1.140769  [ 4300/10672]\n",
      "loss: 1.665098  [ 4400/10672]\n",
      "loss: 1.124981  [ 4500/10672]\n",
      "loss: 1.268934  [ 4600/10672]\n",
      "loss: 1.115391  [ 4700/10672]\n",
      "loss: 1.668084  [ 4800/10672]\n",
      "loss: 1.157849  [ 4900/10672]\n",
      "loss: 1.664965  [ 5000/10672]\n",
      "loss: 1.198899  [ 5100/10672]\n",
      "loss: 1.098820  [ 5200/10672]\n",
      "loss: 1.374851  [ 5300/10672]\n",
      "loss: 1.122454  [ 5400/10672]\n",
      "loss: 1.015482  [ 5500/10672]\n",
      "loss: 1.139489  [ 5600/10672]\n",
      "loss: 1.668360  [ 5700/10672]\n",
      "loss: 1.096987  [ 5800/10672]\n",
      "loss: 1.192795  [ 5900/10672]\n",
      "loss: 1.665909  [ 6000/10672]\n",
      "loss: 1.189845  [ 6100/10672]\n",
      "loss: 1.084616  [ 6200/10672]\n",
      "loss: 1.077137  [ 6300/10672]\n",
      "loss: 1.164313  [ 6400/10672]\n",
      "loss: 1.671528  [ 6500/10672]\n",
      "loss: 1.082166  [ 6600/10672]\n",
      "loss: 1.204705  [ 6700/10672]\n",
      "loss: 1.103767  [ 6800/10672]\n",
      "loss: 1.138289  [ 6900/10672]\n",
      "loss: 1.143734  [ 7000/10672]\n",
      "loss: 1.006085  [ 7100/10672]\n",
      "loss: 1.201566  [ 7200/10672]\n",
      "loss: 1.240117  [ 7300/10672]\n",
      "loss: 1.163956  [ 7400/10672]\n",
      "loss: 1.021860  [ 7500/10672]\n",
      "loss: 1.205442  [ 7600/10672]\n",
      "loss: 1.220577  [ 7700/10672]\n",
      "loss: 1.237560  [ 7800/10672]\n",
      "loss: 1.114557  [ 7900/10672]\n",
      "loss: 1.086149  [ 8000/10672]\n",
      "loss: 1.126047  [ 8100/10672]\n",
      "loss: 1.116622  [ 8200/10672]\n",
      "loss: 1.122890  [ 8300/10672]\n",
      "loss: 1.184613  [ 8400/10672]\n",
      "loss: 1.053703  [ 8500/10672]\n",
      "loss: 1.188098  [ 8600/10672]\n",
      "loss: 1.203875  [ 8700/10672]\n",
      "loss: 1.103313  [ 8800/10672]\n",
      "loss: 1.044665  [ 8900/10672]\n",
      "loss: 1.665544  [ 9000/10672]\n",
      "loss: 1.076958  [ 9100/10672]\n",
      "loss: 1.061890  [ 9200/10672]\n",
      "loss: 1.189023  [ 9300/10672]\n",
      "loss: 1.183787  [ 9400/10672]\n",
      "loss: 1.153130  [ 9500/10672]\n",
      "loss: 1.672620  [ 9600/10672]\n",
      "loss: 1.211909  [ 9700/10672]\n",
      "loss: 1.153366  [ 9800/10672]\n",
      "loss: 1.105231  [ 9900/10672]\n",
      "loss: 1.201792  [10000/10672]\n",
      "loss: 1.103761  [10100/10672]\n",
      "loss: 1.010915  [10200/10672]\n",
      "loss: 1.152695  [10300/10672]\n",
      "loss: 1.198781  [10400/10672]\n",
      "loss: 1.191150  [10500/10672]\n",
      "loss: 1.667499  [10600/10672]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.047896  [    0/10672]\n",
      "loss: 1.108963  [  100/10672]\n",
      "loss: 0.969050  [  200/10672]\n",
      "loss: 0.997648  [  300/10672]\n",
      "loss: 1.255025  [  400/10672]\n",
      "loss: 1.131909  [  500/10672]\n",
      "loss: 1.666373  [  600/10672]\n",
      "loss: 1.031811  [  700/10672]\n",
      "loss: 1.665764  [  800/10672]\n",
      "loss: 1.413296  [  900/10672]\n",
      "loss: 1.666482  [ 1000/10672]\n",
      "loss: 1.270321  [ 1100/10672]\n",
      "loss: 1.162892  [ 1200/10672]\n",
      "loss: 1.130797  [ 1300/10672]\n",
      "loss: 1.067844  [ 1400/10672]\n",
      "loss: 1.057663  [ 1500/10672]\n",
      "loss: 1.096430  [ 1600/10672]\n",
      "loss: 1.041937  [ 1700/10672]\n",
      "loss: 1.234281  [ 1800/10672]\n",
      "loss: 1.112799  [ 1900/10672]\n",
      "loss: 1.171370  [ 2000/10672]\n",
      "loss: 1.165974  [ 2100/10672]\n",
      "loss: 1.134224  [ 2200/10672]\n",
      "loss: 1.232357  [ 2300/10672]\n",
      "loss: 1.136015  [ 2400/10672]\n",
      "loss: 1.117497  [ 2500/10672]\n",
      "loss: 1.127890  [ 2600/10672]\n",
      "loss: 1.665175  [ 2700/10672]\n",
      "loss: 1.110526  [ 2800/10672]\n",
      "loss: 1.665883  [ 2900/10672]\n",
      "loss: 1.666992  [ 3000/10672]\n",
      "loss: 1.214010  [ 3100/10672]\n",
      "loss: 1.059932  [ 3200/10672]\n",
      "loss: 1.062356  [ 3300/10672]\n",
      "loss: 1.671312  [ 3400/10672]\n",
      "loss: 1.668052  [ 3500/10672]\n",
      "loss: 1.667756  [ 3600/10672]\n",
      "loss: 1.131976  [ 3700/10672]\n",
      "loss: 1.091301  [ 3800/10672]\n",
      "loss: 1.215327  [ 3900/10672]\n",
      "loss: 1.067015  [ 4000/10672]\n",
      "loss: 1.665668  [ 4100/10672]\n",
      "loss: 1.666563  [ 4200/10672]\n",
      "loss: 1.140878  [ 4300/10672]\n",
      "loss: 1.665142  [ 4400/10672]\n",
      "loss: 1.123495  [ 4500/10672]\n",
      "loss: 1.269976  [ 4600/10672]\n",
      "loss: 1.113888  [ 4700/10672]\n",
      "loss: 1.668183  [ 4800/10672]\n",
      "loss: 1.158312  [ 4900/10672]\n",
      "loss: 1.665025  [ 5000/10672]\n",
      "loss: 1.200071  [ 5100/10672]\n",
      "loss: 1.098094  [ 5200/10672]\n",
      "loss: 1.377586  [ 5300/10672]\n",
      "loss: 1.122320  [ 5400/10672]\n",
      "loss: 1.012640  [ 5500/10672]\n",
      "loss: 1.139514  [ 5600/10672]\n",
      "loss: 1.668465  [ 5700/10672]\n",
      "loss: 1.095136  [ 5800/10672]\n",
      "loss: 1.193752  [ 5900/10672]\n",
      "loss: 1.665982  [ 6000/10672]\n",
      "loss: 1.189638  [ 6100/10672]\n",
      "loss: 1.083708  [ 6200/10672]\n",
      "loss: 1.075290  [ 6300/10672]\n",
      "loss: 1.164999  [ 6400/10672]\n",
      "loss: 1.671697  [ 6500/10672]\n",
      "loss: 1.081507  [ 6600/10672]\n",
      "loss: 1.204587  [ 6700/10672]\n",
      "loss: 1.103247  [ 6800/10672]\n",
      "loss: 1.136954  [ 6900/10672]\n",
      "loss: 1.143984  [ 7000/10672]\n",
      "loss: 1.004271  [ 7100/10672]\n",
      "loss: 1.201396  [ 7200/10672]\n",
      "loss: 1.241834  [ 7300/10672]\n",
      "loss: 1.163358  [ 7400/10672]\n",
      "loss: 1.020065  [ 7500/10672]\n",
      "loss: 1.205271  [ 7600/10672]\n",
      "loss: 1.220785  [ 7700/10672]\n",
      "loss: 1.237953  [ 7800/10672]\n",
      "loss: 1.114270  [ 7900/10672]\n",
      "loss: 1.085517  [ 8000/10672]\n",
      "loss: 1.125792  [ 8100/10672]\n",
      "loss: 1.115030  [ 8200/10672]\n",
      "loss: 1.121428  [ 8300/10672]\n",
      "loss: 1.185483  [ 8400/10672]\n",
      "loss: 1.052642  [ 8500/10672]\n",
      "loss: 1.189005  [ 8600/10672]\n",
      "loss: 1.203882  [ 8700/10672]\n",
      "loss: 1.101805  [ 8800/10672]\n",
      "loss: 1.043383  [ 8900/10672]\n",
      "loss: 1.665594  [ 9000/10672]\n",
      "loss: 1.074615  [ 9100/10672]\n",
      "loss: 1.060745  [ 9200/10672]\n",
      "loss: 1.190074  [ 9300/10672]\n",
      "loss: 1.184813  [ 9400/10672]\n",
      "loss: 1.151947  [ 9500/10672]\n",
      "loss: 1.672771  [ 9600/10672]\n",
      "loss: 1.211653  [ 9700/10672]\n",
      "loss: 1.153938  [ 9800/10672]\n",
      "loss: 1.105011  [ 9900/10672]\n",
      "loss: 1.203176  [10000/10672]\n",
      "loss: 1.103379  [10100/10672]\n",
      "loss: 1.009238  [10200/10672]\n",
      "loss: 1.151561  [10300/10672]\n",
      "loss: 1.198258  [10400/10672]\n",
      "loss: 1.190501  [10500/10672]\n",
      "loss: 1.667605  [10600/10672]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.046972  [    0/10672]\n",
      "loss: 1.108650  [  100/10672]\n",
      "loss: 0.966882  [  200/10672]\n",
      "loss: 0.995841  [  300/10672]\n",
      "loss: 1.255324  [  400/10672]\n",
      "loss: 1.130351  [  500/10672]\n",
      "loss: 1.666407  [  600/10672]\n",
      "loss: 1.030575  [  700/10672]\n",
      "loss: 1.665801  [  800/10672]\n",
      "loss: 1.416073  [  900/10672]\n",
      "loss: 1.666510  [ 1000/10672]\n",
      "loss: 1.270901  [ 1100/10672]\n",
      "loss: 1.161698  [ 1200/10672]\n",
      "loss: 1.130987  [ 1300/10672]\n",
      "loss: 1.065394  [ 1400/10672]\n",
      "loss: 1.056720  [ 1500/10672]\n",
      "loss: 1.094329  [ 1600/10672]\n",
      "loss: 1.040804  [ 1700/10672]\n",
      "loss: 1.234561  [ 1800/10672]\n",
      "loss: 1.111076  [ 1900/10672]\n",
      "loss: 1.170481  [ 2000/10672]\n",
      "loss: 1.165173  [ 2100/10672]\n",
      "loss: 1.132884  [ 2200/10672]\n",
      "loss: 1.232692  [ 2300/10672]\n",
      "loss: 1.135979  [ 2400/10672]\n",
      "loss: 1.117199  [ 2500/10672]\n",
      "loss: 1.127841  [ 2600/10672]\n",
      "loss: 1.665214  [ 2700/10672]\n",
      "loss: 1.110068  [ 2800/10672]\n",
      "loss: 1.666001  [ 2900/10672]\n",
      "loss: 1.667063  [ 3000/10672]\n",
      "loss: 1.214316  [ 3100/10672]\n",
      "loss: 1.057798  [ 3200/10672]\n",
      "loss: 1.061224  [ 3300/10672]\n",
      "loss: 1.671496  [ 3400/10672]\n",
      "loss: 1.668140  [ 3500/10672]\n",
      "loss: 1.667880  [ 3600/10672]\n",
      "loss: 1.130686  [ 3700/10672]\n",
      "loss: 1.090716  [ 3800/10672]\n",
      "loss: 1.215257  [ 3900/10672]\n",
      "loss: 1.064609  [ 4000/10672]\n",
      "loss: 1.665717  [ 4100/10672]\n",
      "loss: 1.666691  [ 4200/10672]\n",
      "loss: 1.140982  [ 4300/10672]\n",
      "loss: 1.665184  [ 4400/10672]\n",
      "loss: 1.122020  [ 4500/10672]\n",
      "loss: 1.271014  [ 4600/10672]\n",
      "loss: 1.112400  [ 4700/10672]\n",
      "loss: 1.668280  [ 4800/10672]\n",
      "loss: 1.158772  [ 4900/10672]\n",
      "loss: 1.665082  [ 5000/10672]\n",
      "loss: 1.201238  [ 5100/10672]\n",
      "loss: 1.097367  [ 5200/10672]\n",
      "loss: 1.380301  [ 5300/10672]\n",
      "loss: 1.122183  [ 5400/10672]\n",
      "loss: 1.009836  [ 5500/10672]\n",
      "loss: 1.139534  [ 5600/10672]\n",
      "loss: 1.668568  [ 5700/10672]\n",
      "loss: 1.093299  [ 5800/10672]\n",
      "loss: 1.194702  [ 5900/10672]\n",
      "loss: 1.666054  [ 6000/10672]\n",
      "loss: 1.189436  [ 6100/10672]\n",
      "loss: 1.082806  [ 6200/10672]\n",
      "loss: 1.073468  [ 6300/10672]\n",
      "loss: 1.165684  [ 6400/10672]\n",
      "loss: 1.671865  [ 6500/10672]\n",
      "loss: 1.080853  [ 6600/10672]\n",
      "loss: 1.204474  [ 6700/10672]\n",
      "loss: 1.102727  [ 6800/10672]\n",
      "loss: 1.135627  [ 6900/10672]\n",
      "loss: 1.144233  [ 7000/10672]\n",
      "loss: 1.002475  [ 7100/10672]\n",
      "loss: 1.201229  [ 7200/10672]\n",
      "loss: 1.243537  [ 7300/10672]\n",
      "loss: 1.162767  [ 7400/10672]\n",
      "loss: 1.018286  [ 7500/10672]\n",
      "loss: 1.205100  [ 7600/10672]\n",
      "loss: 1.220992  [ 7700/10672]\n",
      "loss: 1.238342  [ 7800/10672]\n",
      "loss: 1.113981  [ 7900/10672]\n",
      "loss: 1.084888  [ 8000/10672]\n",
      "loss: 1.125534  [ 8100/10672]\n",
      "loss: 1.113456  [ 8200/10672]\n",
      "loss: 1.119983  [ 8300/10672]\n",
      "loss: 1.186346  [ 8400/10672]\n",
      "loss: 1.051589  [ 8500/10672]\n",
      "loss: 1.189906  [ 8600/10672]\n",
      "loss: 1.203900  [ 8700/10672]\n",
      "loss: 1.100316  [ 8800/10672]\n",
      "loss: 1.042110  [ 8900/10672]\n",
      "loss: 1.665644  [ 9000/10672]\n",
      "loss: 1.072293  [ 9100/10672]\n",
      "loss: 1.059607  [ 9200/10672]\n",
      "loss: 1.191118  [ 9300/10672]\n",
      "loss: 1.185831  [ 9400/10672]\n",
      "loss: 1.150774  [ 9500/10672]\n",
      "loss: 1.672922  [ 9600/10672]\n",
      "loss: 1.211397  [ 9700/10672]\n",
      "loss: 1.154507  [ 9800/10672]\n",
      "loss: 1.104793  [ 9900/10672]\n",
      "loss: 1.204552  [10000/10672]\n",
      "loss: 1.102996  [10100/10672]\n",
      "loss: 1.007576  [10200/10672]\n",
      "loss: 1.150439  [10300/10672]\n",
      "loss: 1.197742  [10400/10672]\n",
      "loss: 1.189857  [10500/10672]\n",
      "loss: 1.667710  [10600/10672]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.046058  [    0/10672]\n",
      "loss: 1.108332  [  100/10672]\n",
      "loss: 0.964741  [  200/10672]\n",
      "loss: 0.994051  [  300/10672]\n",
      "loss: 1.255621  [  400/10672]\n",
      "loss: 1.128808  [  500/10672]\n",
      "loss: 1.666440  [  600/10672]\n",
      "loss: 1.029346  [  700/10672]\n",
      "loss: 1.665836  [  800/10672]\n",
      "loss: 1.418824  [  900/10672]\n",
      "loss: 1.666536  [ 1000/10672]\n",
      "loss: 1.271477  [ 1100/10672]\n",
      "loss: 1.160509  [ 1200/10672]\n",
      "loss: 1.131171  [ 1300/10672]\n",
      "loss: 1.062970  [ 1400/10672]\n",
      "loss: 1.055782  [ 1500/10672]\n",
      "loss: 1.092246  [ 1600/10672]\n",
      "loss: 1.039676  [ 1700/10672]\n",
      "loss: 1.234843  [ 1800/10672]\n",
      "loss: 1.109370  [ 1900/10672]\n",
      "loss: 1.169598  [ 2000/10672]\n",
      "loss: 1.164383  [ 2100/10672]\n",
      "loss: 1.131558  [ 2200/10672]\n",
      "loss: 1.233028  [ 2300/10672]\n",
      "loss: 1.135938  [ 2400/10672]\n",
      "loss: 1.116901  [ 2500/10672]\n",
      "loss: 1.127793  [ 2600/10672]\n",
      "loss: 1.665252  [ 2700/10672]\n",
      "loss: 1.109609  [ 2800/10672]\n",
      "loss: 1.666118  [ 2900/10672]\n",
      "loss: 1.667132  [ 3000/10672]\n",
      "loss: 1.214627  [ 3100/10672]\n",
      "loss: 1.055691  [ 3200/10672]\n",
      "loss: 1.060097  [ 3300/10672]\n",
      "loss: 1.671678  [ 3400/10672]\n",
      "loss: 1.668226  [ 3500/10672]\n",
      "loss: 1.668004  [ 3600/10672]\n",
      "loss: 1.129408  [ 3700/10672]\n",
      "loss: 1.090131  [ 3800/10672]\n",
      "loss: 1.215189  [ 3900/10672]\n",
      "loss: 1.062229  [ 4000/10672]\n",
      "loss: 1.665765  [ 4100/10672]\n",
      "loss: 1.666819  [ 4200/10672]\n",
      "loss: 1.141079  [ 4300/10672]\n",
      "loss: 1.665224  [ 4400/10672]\n",
      "loss: 1.120557  [ 4500/10672]\n",
      "loss: 1.272050  [ 4600/10672]\n",
      "loss: 1.110930  [ 4700/10672]\n",
      "loss: 1.668377  [ 4800/10672]\n",
      "loss: 1.159228  [ 4900/10672]\n",
      "loss: 1.665138  [ 5000/10672]\n",
      "loss: 1.202399  [ 5100/10672]\n",
      "loss: 1.096638  [ 5200/10672]\n",
      "loss: 1.382998  [ 5300/10672]\n",
      "loss: 1.122042  [ 5400/10672]\n",
      "loss: 1.007071  [ 5500/10672]\n",
      "loss: 1.139545  [ 5600/10672]\n",
      "loss: 1.668669  [ 5700/10672]\n",
      "loss: 1.091477  [ 5800/10672]\n",
      "loss: 1.195643  [ 5900/10672]\n",
      "loss: 1.666124  [ 6000/10672]\n",
      "loss: 1.189241  [ 6100/10672]\n",
      "loss: 1.081909  [ 6200/10672]\n",
      "loss: 1.071671  [ 6300/10672]\n",
      "loss: 1.166364  [ 6400/10672]\n",
      "loss: 1.672032  [ 6500/10672]\n",
      "loss: 1.080203  [ 6600/10672]\n",
      "loss: 1.204366  [ 6700/10672]\n",
      "loss: 1.102208  [ 6800/10672]\n",
      "loss: 1.134305  [ 6900/10672]\n",
      "loss: 1.144483  [ 7000/10672]\n",
      "loss: 1.000699  [ 7100/10672]\n",
      "loss: 1.201064  [ 7200/10672]\n",
      "loss: 1.245228  [ 7300/10672]\n",
      "loss: 1.162182  [ 7400/10672]\n",
      "loss: 1.016526  [ 7500/10672]\n",
      "loss: 1.204932  [ 7600/10672]\n",
      "loss: 1.221198  [ 7700/10672]\n",
      "loss: 1.238725  [ 7800/10672]\n",
      "loss: 1.113690  [ 7900/10672]\n",
      "loss: 1.084260  [ 8000/10672]\n",
      "loss: 1.125271  [ 8100/10672]\n",
      "loss: 1.111897  [ 8200/10672]\n",
      "loss: 1.118551  [ 8300/10672]\n",
      "loss: 1.187203  [ 8400/10672]\n",
      "loss: 1.050544  [ 8500/10672]\n",
      "loss: 1.190803  [ 8600/10672]\n",
      "loss: 1.203926  [ 8700/10672]\n",
      "loss: 1.098845  [ 8800/10672]\n",
      "loss: 1.040846  [ 8900/10672]\n",
      "loss: 1.665692  [ 9000/10672]\n",
      "loss: 1.069990  [ 9100/10672]\n",
      "loss: 1.058474  [ 9200/10672]\n",
      "loss: 1.192154  [ 9300/10672]\n",
      "loss: 1.186841  [ 9400/10672]\n",
      "loss: 1.149614  [ 9500/10672]\n",
      "loss: 1.673072  [ 9600/10672]\n",
      "loss: 1.211141  [ 9700/10672]\n",
      "loss: 1.155073  [ 9800/10672]\n",
      "loss: 1.104575  [ 9900/10672]\n",
      "loss: 1.205918  [10000/10672]\n",
      "loss: 1.102608  [10100/10672]\n",
      "loss: 1.005928  [10200/10672]\n",
      "loss: 1.149329  [10300/10672]\n",
      "loss: 1.197233  [10400/10672]\n",
      "loss: 1.189220  [10500/10672]\n",
      "loss: 1.667813  [10600/10672]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.045152  [    0/10672]\n",
      "loss: 1.108007  [  100/10672]\n",
      "loss: 0.962623  [  200/10672]\n",
      "loss: 0.992277  [  300/10672]\n",
      "loss: 1.255915  [  400/10672]\n",
      "loss: 1.127280  [  500/10672]\n",
      "loss: 1.666472  [  600/10672]\n",
      "loss: 1.028122  [  700/10672]\n",
      "loss: 1.665870  [  800/10672]\n",
      "loss: 1.421549  [  900/10672]\n",
      "loss: 1.666562  [ 1000/10672]\n",
      "loss: 1.272048  [ 1100/10672]\n",
      "loss: 1.159326  [ 1200/10672]\n",
      "loss: 1.131349  [ 1300/10672]\n",
      "loss: 1.060574  [ 1400/10672]\n",
      "loss: 1.054850  [ 1500/10672]\n",
      "loss: 1.090182  [ 1600/10672]\n",
      "loss: 1.038554  [ 1700/10672]\n",
      "loss: 1.235128  [ 1800/10672]\n",
      "loss: 1.107682  [ 1900/10672]\n",
      "loss: 1.168721  [ 2000/10672]\n",
      "loss: 1.163605  [ 2100/10672]\n",
      "loss: 1.130246  [ 2200/10672]\n",
      "loss: 1.233365  [ 2300/10672]\n",
      "loss: 1.135892  [ 2400/10672]\n",
      "loss: 1.116599  [ 2500/10672]\n",
      "loss: 1.127746  [ 2600/10672]\n",
      "loss: 1.665288  [ 2700/10672]\n",
      "loss: 1.109149  [ 2800/10672]\n",
      "loss: 1.666232  [ 2900/10672]\n",
      "loss: 1.667200  [ 3000/10672]\n",
      "loss: 1.214942  [ 3100/10672]\n",
      "loss: 1.053610  [ 3200/10672]\n",
      "loss: 1.058977  [ 3300/10672]\n",
      "loss: 1.671861  [ 3400/10672]\n",
      "loss: 1.668312  [ 3500/10672]\n",
      "loss: 1.668128  [ 3600/10672]\n",
      "loss: 1.128139  [ 3700/10672]\n",
      "loss: 1.089549  [ 3800/10672]\n",
      "loss: 1.215121  [ 3900/10672]\n",
      "loss: 1.059871  [ 4000/10672]\n",
      "loss: 1.665811  [ 4100/10672]\n",
      "loss: 1.666947  [ 4200/10672]\n",
      "loss: 1.141170  [ 4300/10672]\n",
      "loss: 1.665262  [ 4400/10672]\n",
      "loss: 1.119105  [ 4500/10672]\n",
      "loss: 1.273082  [ 4600/10672]\n",
      "loss: 1.109475  [ 4700/10672]\n",
      "loss: 1.668473  [ 4800/10672]\n",
      "loss: 1.159683  [ 4900/10672]\n",
      "loss: 1.665192  [ 5000/10672]\n",
      "loss: 1.203555  [ 5100/10672]\n",
      "loss: 1.095907  [ 5200/10672]\n",
      "loss: 1.385673  [ 5300/10672]\n",
      "loss: 1.121900  [ 5400/10672]\n",
      "loss: 1.004341  [ 5500/10672]\n",
      "loss: 1.139552  [ 5600/10672]\n",
      "loss: 1.668770  [ 5700/10672]\n",
      "loss: 1.089668  [ 5800/10672]\n",
      "loss: 1.196579  [ 5900/10672]\n",
      "loss: 1.666193  [ 6000/10672]\n",
      "loss: 1.189050  [ 6100/10672]\n",
      "loss: 1.081017  [ 6200/10672]\n",
      "loss: 1.069897  [ 6300/10672]\n",
      "loss: 1.167044  [ 6400/10672]\n",
      "loss: 1.672199  [ 6500/10672]\n",
      "loss: 1.079559  [ 6600/10672]\n",
      "loss: 1.204261  [ 6700/10672]\n",
      "loss: 1.101691  [ 6800/10672]\n",
      "loss: 1.132990  [ 6900/10672]\n",
      "loss: 1.144731  [ 7000/10672]\n",
      "loss: 0.998940  [ 7100/10672]\n",
      "loss: 1.200900  [ 7200/10672]\n",
      "loss: 1.246905  [ 7300/10672]\n",
      "loss: 1.161605  [ 7400/10672]\n",
      "loss: 1.014782  [ 7500/10672]\n",
      "loss: 1.204764  [ 7600/10672]\n",
      "loss: 1.221404  [ 7700/10672]\n",
      "loss: 1.239104  [ 7800/10672]\n",
      "loss: 1.113396  [ 7900/10672]\n",
      "loss: 1.083636  [ 8000/10672]\n",
      "loss: 1.125004  [ 8100/10672]\n",
      "loss: 1.110355  [ 8200/10672]\n",
      "loss: 1.117134  [ 8300/10672]\n",
      "loss: 1.188057  [ 8400/10672]\n",
      "loss: 1.049508  [ 8500/10672]\n",
      "loss: 1.191697  [ 8600/10672]\n",
      "loss: 1.203960  [ 8700/10672]\n",
      "loss: 1.097391  [ 8800/10672]\n",
      "loss: 1.039593  [ 8900/10672]\n",
      "loss: 1.665740  [ 9000/10672]\n",
      "loss: 1.067705  [ 9100/10672]\n",
      "loss: 1.057348  [ 9200/10672]\n",
      "loss: 1.193185  [ 9300/10672]\n",
      "loss: 1.187846  [ 9400/10672]\n",
      "loss: 1.148463  [ 9500/10672]\n",
      "loss: 1.673221  [ 9600/10672]\n",
      "loss: 1.210884  [ 9700/10672]\n",
      "loss: 1.155637  [ 9800/10672]\n",
      "loss: 1.104359  [ 9900/10672]\n",
      "loss: 1.207276  [10000/10672]\n",
      "loss: 1.102219  [10100/10672]\n",
      "loss: 1.004295  [10200/10672]\n",
      "loss: 1.148228  [10300/10672]\n",
      "loss: 1.196729  [10400/10672]\n",
      "loss: 1.188587  [10500/10672]\n",
      "loss: 1.667914  [10600/10672]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.044258  [    0/10672]\n",
      "loss: 1.107679  [  100/10672]\n",
      "loss: 0.960532  [  200/10672]\n",
      "loss: 0.990520  [  300/10672]\n",
      "loss: 1.256206  [  400/10672]\n",
      "loss: 1.125766  [  500/10672]\n",
      "loss: 1.666502  [  600/10672]\n",
      "loss: 1.026906  [  700/10672]\n",
      "loss: 1.665903  [  800/10672]\n",
      "loss: 1.424246  [  900/10672]\n",
      "loss: 1.666586  [ 1000/10672]\n",
      "loss: 1.272612  [ 1100/10672]\n",
      "loss: 1.158148  [ 1200/10672]\n",
      "loss: 1.131523  [ 1300/10672]\n",
      "loss: 1.058203  [ 1400/10672]\n",
      "loss: 1.053924  [ 1500/10672]\n",
      "loss: 1.088136  [ 1600/10672]\n",
      "loss: 1.037439  [ 1700/10672]\n",
      "loss: 1.235412  [ 1800/10672]\n",
      "loss: 1.106007  [ 1900/10672]\n",
      "loss: 1.167848  [ 2000/10672]\n",
      "loss: 1.162838  [ 2100/10672]\n",
      "loss: 1.128946  [ 2200/10672]\n",
      "loss: 1.233701  [ 2300/10672]\n",
      "loss: 1.135842  [ 2400/10672]\n",
      "loss: 1.116295  [ 2500/10672]\n",
      "loss: 1.127701  [ 2600/10672]\n",
      "loss: 1.665324  [ 2700/10672]\n",
      "loss: 1.108688  [ 2800/10672]\n",
      "loss: 1.666345  [ 2900/10672]\n",
      "loss: 1.667266  [ 3000/10672]\n",
      "loss: 1.215261  [ 3100/10672]\n",
      "loss: 1.051556  [ 3200/10672]\n",
      "loss: 1.057862  [ 3300/10672]\n",
      "loss: 1.672043  [ 3400/10672]\n",
      "loss: 1.668396  [ 3500/10672]\n",
      "loss: 1.668252  [ 3600/10672]\n",
      "loss: 1.126881  [ 3700/10672]\n",
      "loss: 1.088968  [ 3800/10672]\n",
      "loss: 1.215054  [ 3900/10672]\n",
      "loss: 1.057537  [ 4000/10672]\n",
      "loss: 1.665855  [ 4100/10672]\n",
      "loss: 1.667074  [ 4200/10672]\n",
      "loss: 1.141255  [ 4300/10672]\n",
      "loss: 1.665299  [ 4400/10672]\n",
      "loss: 1.117665  [ 4500/10672]\n",
      "loss: 1.274113  [ 4600/10672]\n",
      "loss: 1.108038  [ 4700/10672]\n",
      "loss: 1.668568  [ 4800/10672]\n",
      "loss: 1.160132  [ 4900/10672]\n",
      "loss: 1.665244  [ 5000/10672]\n",
      "loss: 1.204704  [ 5100/10672]\n",
      "loss: 1.095173  [ 5200/10672]\n",
      "loss: 1.388332  [ 5300/10672]\n",
      "loss: 1.121752  [ 5400/10672]\n",
      "loss: 1.001651  [ 5500/10672]\n",
      "loss: 1.139550  [ 5600/10672]\n",
      "loss: 1.668870  [ 5700/10672]\n",
      "loss: 1.087875  [ 5800/10672]\n",
      "loss: 1.197504  [ 5900/10672]\n",
      "loss: 1.666261  [ 6000/10672]\n",
      "loss: 1.188866  [ 6100/10672]\n",
      "loss: 1.080129  [ 6200/10672]\n",
      "loss: 1.068148  [ 6300/10672]\n",
      "loss: 1.167718  [ 6400/10672]\n",
      "loss: 1.672366  [ 6500/10672]\n",
      "loss: 1.078917  [ 6600/10672]\n",
      "loss: 1.204164  [ 6700/10672]\n",
      "loss: 1.101171  [ 6800/10672]\n",
      "loss: 1.131683  [ 6900/10672]\n",
      "loss: 1.144977  [ 7000/10672]\n",
      "loss: 0.997199  [ 7100/10672]\n",
      "loss: 1.200741  [ 7200/10672]\n",
      "loss: 1.248567  [ 7300/10672]\n",
      "loss: 1.161037  [ 7400/10672]\n",
      "loss: 1.013053  [ 7500/10672]\n",
      "loss: 1.204600  [ 7600/10672]\n",
      "loss: 1.221609  [ 7700/10672]\n",
      "loss: 1.239480  [ 7800/10672]\n",
      "loss: 1.113099  [ 7900/10672]\n",
      "loss: 1.083013  [ 8000/10672]\n",
      "loss: 1.124732  [ 8100/10672]\n",
      "loss: 1.108830  [ 8200/10672]\n",
      "loss: 1.115732  [ 8300/10672]\n",
      "loss: 1.188903  [ 8400/10672]\n",
      "loss: 1.048480  [ 8500/10672]\n",
      "loss: 1.192583  [ 8600/10672]\n",
      "loss: 1.204004  [ 8700/10672]\n",
      "loss: 1.095956  [ 8800/10672]\n",
      "loss: 1.038347  [ 8900/10672]\n",
      "loss: 1.665787  [ 9000/10672]\n",
      "loss: 1.065441  [ 9100/10672]\n",
      "loss: 1.056227  [ 9200/10672]\n",
      "loss: 1.194208  [ 9300/10672]\n",
      "loss: 1.188841  [ 9400/10672]\n",
      "loss: 1.147326  [ 9500/10672]\n",
      "loss: 1.673371  [ 9600/10672]\n",
      "loss: 1.210629  [ 9700/10672]\n",
      "loss: 1.156196  [ 9800/10672]\n",
      "loss: 1.104142  [ 9900/10672]\n",
      "loss: 1.208624  [10000/10672]\n",
      "loss: 1.101826  [10100/10672]\n",
      "loss: 1.002675  [10200/10672]\n",
      "loss: 1.147139  [10300/10672]\n",
      "loss: 1.196233  [10400/10672]\n",
      "loss: 1.187962  [10500/10672]\n",
      "loss: 1.668014  [10600/10672]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.043373  [    0/10672]\n",
      "loss: 1.107345  [  100/10672]\n",
      "loss: 0.958466  [  200/10672]\n",
      "loss: 0.988779  [  300/10672]\n",
      "loss: 1.256495  [  400/10672]\n",
      "loss: 1.124267  [  500/10672]\n",
      "loss: 1.666532  [  600/10672]\n",
      "loss: 1.025695  [  700/10672]\n",
      "loss: 1.665934  [  800/10672]\n",
      "loss: 1.426918  [  900/10672]\n",
      "loss: 1.666610  [ 1000/10672]\n",
      "loss: 1.273172  [ 1100/10672]\n",
      "loss: 1.156975  [ 1200/10672]\n",
      "loss: 1.131691  [ 1300/10672]\n",
      "loss: 1.055857  [ 1400/10672]\n",
      "loss: 1.053005  [ 1500/10672]\n",
      "loss: 1.086108  [ 1600/10672]\n",
      "loss: 1.036329  [ 1700/10672]\n",
      "loss: 1.235699  [ 1800/10672]\n",
      "loss: 1.104349  [ 1900/10672]\n",
      "loss: 1.166981  [ 2000/10672]\n",
      "loss: 1.162082  [ 2100/10672]\n",
      "loss: 1.127659  [ 2200/10672]\n",
      "loss: 1.234037  [ 2300/10672]\n",
      "loss: 1.135788  [ 2400/10672]\n",
      "loss: 1.115991  [ 2500/10672]\n",
      "loss: 1.127656  [ 2600/10672]\n",
      "loss: 1.665358  [ 2700/10672]\n",
      "loss: 1.108226  [ 2800/10672]\n",
      "loss: 1.666456  [ 2900/10672]\n",
      "loss: 1.667332  [ 3000/10672]\n",
      "loss: 1.215585  [ 3100/10672]\n",
      "loss: 1.049527  [ 3200/10672]\n",
      "loss: 1.056752  [ 3300/10672]\n",
      "loss: 1.672224  [ 3400/10672]\n",
      "loss: 1.668480  [ 3500/10672]\n",
      "loss: 1.668375  [ 3600/10672]\n",
      "loss: 1.125634  [ 3700/10672]\n",
      "loss: 1.088386  [ 3800/10672]\n",
      "loss: 1.214988  [ 3900/10672]\n",
      "loss: 1.055227  [ 4000/10672]\n",
      "loss: 1.665899  [ 4100/10672]\n",
      "loss: 1.667202  [ 4200/10672]\n",
      "loss: 1.141333  [ 4300/10672]\n",
      "loss: 1.665335  [ 4400/10672]\n",
      "loss: 1.116236  [ 4500/10672]\n",
      "loss: 1.275141  [ 4600/10672]\n",
      "loss: 1.106616  [ 4700/10672]\n",
      "loss: 1.668662  [ 4800/10672]\n",
      "loss: 1.160578  [ 4900/10672]\n",
      "loss: 1.665294  [ 5000/10672]\n",
      "loss: 1.205848  [ 5100/10672]\n",
      "loss: 1.094439  [ 5200/10672]\n",
      "loss: 1.390969  [ 5300/10672]\n",
      "loss: 1.121602  [ 5400/10672]\n",
      "loss: 0.998996  [ 5500/10672]\n",
      "loss: 1.139542  [ 5600/10672]\n",
      "loss: 1.668969  [ 5700/10672]\n",
      "loss: 1.086094  [ 5800/10672]\n",
      "loss: 1.198425  [ 5900/10672]\n",
      "loss: 1.666328  [ 6000/10672]\n",
      "loss: 1.188686  [ 6100/10672]\n",
      "loss: 1.079247  [ 6200/10672]\n",
      "loss: 1.066422  [ 6300/10672]\n",
      "loss: 1.168391  [ 6400/10672]\n",
      "loss: 1.672532  [ 6500/10672]\n",
      "loss: 1.078282  [ 6600/10672]\n",
      "loss: 1.204068  [ 6700/10672]\n",
      "loss: 1.100655  [ 6800/10672]\n",
      "loss: 1.130381  [ 6900/10672]\n",
      "loss: 1.145223  [ 7000/10672]\n",
      "loss: 0.995476  [ 7100/10672]\n",
      "loss: 1.200581  [ 7200/10672]\n",
      "loss: 1.250217  [ 7300/10672]\n",
      "loss: 1.160474  [ 7400/10672]\n",
      "loss: 1.011342  [ 7500/10672]\n",
      "loss: 1.204435  [ 7600/10672]\n",
      "loss: 1.221813  [ 7700/10672]\n",
      "loss: 1.239850  [ 7800/10672]\n",
      "loss: 1.112800  [ 7900/10672]\n",
      "loss: 1.082392  [ 8000/10672]\n",
      "loss: 1.124456  [ 8100/10672]\n",
      "loss: 1.107322  [ 8200/10672]\n",
      "loss: 1.114345  [ 8300/10672]\n",
      "loss: 1.189743  [ 8400/10672]\n",
      "loss: 1.047459  [ 8500/10672]\n",
      "loss: 1.193464  [ 8600/10672]\n",
      "loss: 1.204059  [ 8700/10672]\n",
      "loss: 1.094539  [ 8800/10672]\n",
      "loss: 1.037108  [ 8900/10672]\n",
      "loss: 1.665833  [ 9000/10672]\n",
      "loss: 1.063197  [ 9100/10672]\n",
      "loss: 1.055111  [ 9200/10672]\n",
      "loss: 1.195221  [ 9300/10672]\n",
      "loss: 1.189827  [ 9400/10672]\n",
      "loss: 1.146201  [ 9500/10672]\n",
      "loss: 1.673520  [ 9600/10672]\n",
      "loss: 1.210375  [ 9700/10672]\n",
      "loss: 1.156750  [ 9800/10672]\n",
      "loss: 1.103925  [ 9900/10672]\n",
      "loss: 1.209960  [10000/10672]\n",
      "loss: 1.101429  [10100/10672]\n",
      "loss: 1.001068  [10200/10672]\n",
      "loss: 1.146062  [10300/10672]\n",
      "loss: 1.195745  [10400/10672]\n",
      "loss: 1.187343  [10500/10672]\n",
      "loss: 1.668112  [10600/10672]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.042495  [    0/10672]\n",
      "loss: 1.107004  [  100/10672]\n",
      "loss: 0.956423  [  200/10672]\n",
      "loss: 0.987052  [  300/10672]\n",
      "loss: 1.256783  [  400/10672]\n",
      "loss: 1.122784  [  500/10672]\n",
      "loss: 1.666561  [  600/10672]\n",
      "loss: 1.024489  [  700/10672]\n",
      "loss: 1.665963  [  800/10672]\n",
      "loss: 1.429565  [  900/10672]\n",
      "loss: 1.666632  [ 1000/10672]\n",
      "loss: 1.273730  [ 1100/10672]\n",
      "loss: 1.155810  [ 1200/10672]\n",
      "loss: 1.131852  [ 1300/10672]\n",
      "loss: 1.053540  [ 1400/10672]\n",
      "loss: 1.052088  [ 1500/10672]\n",
      "loss: 1.084100  [ 1600/10672]\n",
      "loss: 1.035223  [ 1700/10672]\n",
      "loss: 1.235989  [ 1800/10672]\n",
      "loss: 1.102709  [ 1900/10672]\n",
      "loss: 1.166122  [ 2000/10672]\n",
      "loss: 1.161339  [ 2100/10672]\n",
      "loss: 1.126389  [ 2200/10672]\n",
      "loss: 1.234378  [ 2300/10672]\n",
      "loss: 1.135725  [ 2400/10672]\n",
      "loss: 1.115681  [ 2500/10672]\n",
      "loss: 1.127609  [ 2600/10672]\n",
      "loss: 1.665391  [ 2700/10672]\n",
      "loss: 1.107759  [ 2800/10672]\n",
      "loss: 1.666566  [ 2900/10672]\n",
      "loss: 1.667396  [ 3000/10672]\n",
      "loss: 1.215916  [ 3100/10672]\n",
      "loss: 1.047527  [ 3200/10672]\n",
      "loss: 1.055645  [ 3300/10672]\n",
      "loss: 1.672405  [ 3400/10672]\n",
      "loss: 1.668564  [ 3500/10672]\n",
      "loss: 1.668497  [ 3600/10672]\n",
      "loss: 1.124401  [ 3700/10672]\n",
      "loss: 1.087803  [ 3800/10672]\n",
      "loss: 1.214927  [ 3900/10672]\n",
      "loss: 1.052944  [ 4000/10672]\n",
      "loss: 1.665941  [ 4100/10672]\n",
      "loss: 1.667330  [ 4200/10672]\n",
      "loss: 1.141401  [ 4300/10672]\n",
      "loss: 1.665369  [ 4400/10672]\n",
      "loss: 1.114822  [ 4500/10672]\n",
      "loss: 1.276169  [ 4600/10672]\n",
      "loss: 1.105213  [ 4700/10672]\n",
      "loss: 1.668756  [ 4800/10672]\n",
      "loss: 1.161017  [ 4900/10672]\n",
      "loss: 1.665343  [ 5000/10672]\n",
      "loss: 1.206983  [ 5100/10672]\n",
      "loss: 1.093699  [ 5200/10672]\n",
      "loss: 1.393590  [ 5300/10672]\n",
      "loss: 1.121445  [ 5400/10672]\n",
      "loss: 0.996380  [ 5500/10672]\n",
      "loss: 1.139525  [ 5600/10672]\n",
      "loss: 1.669067  [ 5700/10672]\n",
      "loss: 1.084332  [ 5800/10672]\n",
      "loss: 1.199333  [ 5900/10672]\n",
      "loss: 1.666393  [ 6000/10672]\n",
      "loss: 1.188516  [ 6100/10672]\n",
      "loss: 1.078366  [ 6200/10672]\n",
      "loss: 1.064721  [ 6300/10672]\n",
      "loss: 1.169057  [ 6400/10672]\n",
      "loss: 1.672698  [ 6500/10672]\n",
      "loss: 1.077646  [ 6600/10672]\n",
      "loss: 1.203982  [ 6700/10672]\n",
      "loss: 1.100136  [ 6800/10672]\n",
      "loss: 1.129090  [ 6900/10672]\n",
      "loss: 1.145465  [ 7000/10672]\n",
      "loss: 0.993769  [ 7100/10672]\n",
      "loss: 1.200428  [ 7200/10672]\n",
      "loss: 1.251851  [ 7300/10672]\n",
      "loss: 1.159921  [ 7400/10672]\n",
      "loss: 1.009645  [ 7500/10672]\n",
      "loss: 1.204275  [ 7600/10672]\n",
      "loss: 1.222019  [ 7700/10672]\n",
      "loss: 1.240218  [ 7800/10672]\n",
      "loss: 1.112497  [ 7900/10672]\n",
      "loss: 1.081771  [ 8000/10672]\n",
      "loss: 1.124172  [ 8100/10672]\n",
      "loss: 1.105832  [ 8200/10672]\n",
      "loss: 1.112974  [ 8300/10672]\n",
      "loss: 1.190575  [ 8400/10672]\n",
      "loss: 1.046444  [ 8500/10672]\n",
      "loss: 1.194336  [ 8600/10672]\n",
      "loss: 1.204123  [ 8700/10672]\n",
      "loss: 1.093141  [ 8800/10672]\n",
      "loss: 1.035879  [ 8900/10672]\n",
      "loss: 1.665878  [ 9000/10672]\n",
      "loss: 1.060973  [ 9100/10672]\n",
      "loss: 1.054001  [ 9200/10672]\n",
      "loss: 1.196229  [ 9300/10672]\n",
      "loss: 1.190807  [ 9400/10672]\n",
      "loss: 1.145087  [ 9500/10672]\n",
      "loss: 1.673668  [ 9600/10672]\n",
      "loss: 1.210120  [ 9700/10672]\n",
      "loss: 1.157304  [ 9800/10672]\n",
      "loss: 1.103711  [ 9900/10672]\n",
      "loss: 1.211290  [10000/10672]\n",
      "loss: 1.101031  [10100/10672]\n",
      "loss: 0.999476  [10200/10672]\n",
      "loss: 1.144995  [10300/10672]\n",
      "loss: 1.195261  [10400/10672]\n",
      "loss: 1.186728  [10500/10672]\n",
      "loss: 1.668209  [10600/10672]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.041629  [    0/10672]\n",
      "loss: 1.106661  [  100/10672]\n",
      "loss: 0.954406  [  200/10672]\n",
      "loss: 0.985343  [  300/10672]\n",
      "loss: 1.257066  [  400/10672]\n",
      "loss: 1.121312  [  500/10672]\n",
      "loss: 1.666588  [  600/10672]\n",
      "loss: 1.023292  [  700/10672]\n",
      "loss: 1.665992  [  800/10672]\n",
      "loss: 1.432182  [  900/10672]\n",
      "loss: 1.666654  [ 1000/10672]\n",
      "loss: 1.274279  [ 1100/10672]\n",
      "loss: 1.154647  [ 1200/10672]\n",
      "loss: 1.132011  [ 1300/10672]\n",
      "loss: 1.051246  [ 1400/10672]\n",
      "loss: 1.051181  [ 1500/10672]\n",
      "loss: 1.082106  [ 1600/10672]\n",
      "loss: 1.034127  [ 1700/10672]\n",
      "loss: 1.236276  [ 1800/10672]\n",
      "loss: 1.101081  [ 1900/10672]\n",
      "loss: 1.165263  [ 2000/10672]\n",
      "loss: 1.160603  [ 2100/10672]\n",
      "loss: 1.125126  [ 2200/10672]\n",
      "loss: 1.234713  [ 2300/10672]\n",
      "loss: 1.135664  [ 2400/10672]\n",
      "loss: 1.115375  [ 2500/10672]\n",
      "loss: 1.127568  [ 2600/10672]\n",
      "loss: 1.665423  [ 2700/10672]\n",
      "loss: 1.107297  [ 2800/10672]\n",
      "loss: 1.666674  [ 2900/10672]\n",
      "loss: 1.667459  [ 3000/10672]\n",
      "loss: 1.216246  [ 3100/10672]\n",
      "loss: 1.045546  [ 3200/10672]\n",
      "loss: 1.054549  [ 3300/10672]\n",
      "loss: 1.672585  [ 3400/10672]\n",
      "loss: 1.668646  [ 3500/10672]\n",
      "loss: 1.668620  [ 3600/10672]\n",
      "loss: 1.123173  [ 3700/10672]\n",
      "loss: 1.087226  [ 3800/10672]\n",
      "loss: 1.214861  [ 3900/10672]\n",
      "loss: 1.050678  [ 4000/10672]\n",
      "loss: 1.665982  [ 4100/10672]\n",
      "loss: 1.667457  [ 4200/10672]\n",
      "loss: 1.141469  [ 4300/10672]\n",
      "loss: 1.665402  [ 4400/10672]\n",
      "loss: 1.113412  [ 4500/10672]\n",
      "loss: 1.277189  [ 4600/10672]\n",
      "loss: 1.103821  [ 4700/10672]\n",
      "loss: 1.668848  [ 4800/10672]\n",
      "loss: 1.161458  [ 4900/10672]\n",
      "loss: 1.665390  [ 5000/10672]\n",
      "loss: 1.208119  [ 5100/10672]\n",
      "loss: 1.092963  [ 5200/10672]\n",
      "loss: 1.396186  [ 5300/10672]\n",
      "loss: 1.121291  [ 5400/10672]\n",
      "loss: 0.993796  [ 5500/10672]\n",
      "loss: 1.139507  [ 5600/10672]\n",
      "loss: 1.669165  [ 5700/10672]\n",
      "loss: 1.082578  [ 5800/10672]\n",
      "loss: 1.200240  [ 5900/10672]\n",
      "loss: 1.666458  [ 6000/10672]\n",
      "loss: 1.188344  [ 6100/10672]\n",
      "loss: 1.077495  [ 6200/10672]\n",
      "loss: 1.063040  [ 6300/10672]\n",
      "loss: 1.169726  [ 6400/10672]\n",
      "loss: 1.672863  [ 6500/10672]\n",
      "loss: 1.077021  [ 6600/10672]\n",
      "loss: 1.203894  [ 6700/10672]\n",
      "loss: 1.099623  [ 6800/10672]\n",
      "loss: 1.127799  [ 6900/10672]\n",
      "loss: 1.145711  [ 7000/10672]\n",
      "loss: 0.992083  [ 7100/10672]\n",
      "loss: 1.200270  [ 7200/10672]\n",
      "loss: 1.253476  [ 7300/10672]\n",
      "loss: 1.159370  [ 7400/10672]\n",
      "loss: 1.007967  [ 7500/10672]\n",
      "loss: 1.204110  [ 7600/10672]\n",
      "loss: 1.222218  [ 7700/10672]\n",
      "loss: 1.240577  [ 7800/10672]\n",
      "loss: 1.112196  [ 7900/10672]\n",
      "loss: 1.081158  [ 8000/10672]\n",
      "loss: 1.123891  [ 8100/10672]\n",
      "loss: 1.104352  [ 8200/10672]\n",
      "loss: 1.111612  [ 8300/10672]\n",
      "loss: 1.191406  [ 8400/10672]\n",
      "loss: 1.045442  [ 8500/10672]\n",
      "loss: 1.195210  [ 8600/10672]\n",
      "loss: 1.204191  [ 8700/10672]\n",
      "loss: 1.091756  [ 8800/10672]\n",
      "loss: 1.034661  [ 8900/10672]\n",
      "loss: 1.665923  [ 9000/10672]\n",
      "loss: 1.058765  [ 9100/10672]\n",
      "loss: 1.052900  [ 9200/10672]\n",
      "loss: 1.197233  [ 9300/10672]\n",
      "loss: 1.191782  [ 9400/10672]\n",
      "loss: 1.143981  [ 9500/10672]\n",
      "loss: 1.673816  [ 9600/10672]\n",
      "loss: 1.209863  [ 9700/10672]\n",
      "loss: 1.157856  [ 9800/10672]\n",
      "loss: 1.103498  [ 9900/10672]\n",
      "loss: 1.212611  [10000/10672]\n",
      "loss: 1.100630  [10100/10672]\n",
      "loss: 0.997898  [10200/10672]\n",
      "loss: 1.143938  [10300/10672]\n",
      "loss: 1.194783  [10400/10672]\n",
      "loss: 1.186119  [10500/10672]\n",
      "loss: 1.668304  [10600/10672]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.040771  [    0/10672]\n",
      "loss: 1.106312  [  100/10672]\n",
      "loss: 0.952412  [  200/10672]\n",
      "loss: 0.983649  [  300/10672]\n",
      "loss: 1.257346  [  400/10672]\n",
      "loss: 1.119856  [  500/10672]\n",
      "loss: 1.666614  [  600/10672]\n",
      "loss: 1.022101  [  700/10672]\n",
      "loss: 1.666019  [  800/10672]\n",
      "loss: 1.434774  [  900/10672]\n",
      "loss: 1.666675  [ 1000/10672]\n",
      "loss: 1.274824  [ 1100/10672]\n",
      "loss: 1.153489  [ 1200/10672]\n",
      "loss: 1.132165  [ 1300/10672]\n",
      "loss: 1.048977  [ 1400/10672]\n",
      "loss: 1.050280  [ 1500/10672]\n",
      "loss: 1.080131  [ 1600/10672]\n",
      "loss: 1.033037  [ 1700/10672]\n",
      "loss: 1.236565  [ 1800/10672]\n",
      "loss: 1.099469  [ 1900/10672]\n",
      "loss: 1.164410  [ 2000/10672]\n",
      "loss: 1.159878  [ 2100/10672]\n",
      "loss: 1.123877  [ 2200/10672]\n",
      "loss: 1.235048  [ 2300/10672]\n",
      "loss: 1.135596  [ 2400/10672]\n",
      "loss: 1.115065  [ 2500/10672]\n",
      "loss: 1.127526  [ 2600/10672]\n",
      "loss: 1.665454  [ 2700/10672]\n",
      "loss: 1.106832  [ 2800/10672]\n",
      "loss: 1.666781  [ 2900/10672]\n",
      "loss: 1.667520  [ 3000/10672]\n",
      "loss: 1.216581  [ 3100/10672]\n",
      "loss: 1.043592  [ 3200/10672]\n",
      "loss: 1.053457  [ 3300/10672]\n",
      "loss: 1.672765  [ 3400/10672]\n",
      "loss: 1.668728  [ 3500/10672]\n",
      "loss: 1.668742  [ 3600/10672]\n",
      "loss: 1.121956  [ 3700/10672]\n",
      "loss: 1.086649  [ 3800/10672]\n",
      "loss: 1.214797  [ 3900/10672]\n",
      "loss: 1.048437  [ 4000/10672]\n",
      "loss: 1.666022  [ 4100/10672]\n",
      "loss: 1.667585  [ 4200/10672]\n",
      "loss: 1.141529  [ 4300/10672]\n",
      "loss: 1.665434  [ 4400/10672]\n",
      "loss: 1.112016  [ 4500/10672]\n",
      "loss: 1.278207  [ 4600/10672]\n",
      "loss: 1.102446  [ 4700/10672]\n",
      "loss: 1.668941  [ 4800/10672]\n",
      "loss: 1.161894  [ 4900/10672]\n",
      "loss: 1.665436  [ 5000/10672]\n",
      "loss: 1.209246  [ 5100/10672]\n",
      "loss: 1.092223  [ 5200/10672]\n",
      "loss: 1.398765  [ 5300/10672]\n",
      "loss: 1.121132  [ 5400/10672]\n",
      "loss: 0.991249  [ 5500/10672]\n",
      "loss: 1.139481  [ 5600/10672]\n",
      "loss: 1.669261  [ 5700/10672]\n",
      "loss: 1.080839  [ 5800/10672]\n",
      "loss: 1.201137  [ 5900/10672]\n",
      "loss: 1.666522  [ 6000/10672]\n",
      "loss: 1.188181  [ 6100/10672]\n",
      "loss: 1.076626  [ 6200/10672]\n",
      "loss: 1.061382  [ 6300/10672]\n",
      "loss: 1.170389  [ 6400/10672]\n",
      "loss: 1.673029  [ 6500/10672]\n",
      "loss: 1.076397  [ 6600/10672]\n",
      "loss: 1.203813  [ 6700/10672]\n",
      "loss: 1.099108  [ 6800/10672]\n",
      "loss: 1.126518  [ 6900/10672]\n",
      "loss: 1.145954  [ 7000/10672]\n",
      "loss: 0.990413  [ 7100/10672]\n",
      "loss: 1.200117  [ 7200/10672]\n",
      "loss: 1.255086  [ 7300/10672]\n",
      "loss: 1.158827  [ 7400/10672]\n",
      "loss: 1.006304  [ 7500/10672]\n",
      "loss: 1.203949  [ 7600/10672]\n",
      "loss: 1.222419  [ 7700/10672]\n",
      "loss: 1.240932  [ 7800/10672]\n",
      "loss: 1.111891  [ 7900/10672]\n",
      "loss: 1.080544  [ 8000/10672]\n",
      "loss: 1.123603  [ 8100/10672]\n",
      "loss: 1.102890  [ 8200/10672]\n",
      "loss: 1.110266  [ 8300/10672]\n",
      "loss: 1.192230  [ 8400/10672]\n",
      "loss: 1.044445  [ 8500/10672]\n",
      "loss: 1.196075  [ 8600/10672]\n",
      "loss: 1.204269  [ 8700/10672]\n",
      "loss: 1.090390  [ 8800/10672]\n",
      "loss: 1.033450  [ 8900/10672]\n",
      "loss: 1.665967  [ 9000/10672]\n",
      "loss: 1.056577  [ 9100/10672]\n",
      "loss: 1.051803  [ 9200/10672]\n",
      "loss: 1.198227  [ 9300/10672]\n",
      "loss: 1.192747  [ 9400/10672]\n",
      "loss: 1.142887  [ 9500/10672]\n",
      "loss: 1.673963  [ 9600/10672]\n",
      "loss: 1.209607  [ 9700/10672]\n",
      "loss: 1.158403  [ 9800/10672]\n",
      "loss: 1.103285  [ 9900/10672]\n",
      "loss: 1.213923  [10000/10672]\n",
      "loss: 1.100227  [10100/10672]\n",
      "loss: 0.996334  [10200/10672]\n",
      "loss: 1.142891  [10300/10672]\n",
      "loss: 1.194311  [10400/10672]\n",
      "loss: 1.185514  [10500/10672]\n",
      "loss: 1.668398  [10600/10672]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.039923  [    0/10672]\n",
      "loss: 1.105960  [  100/10672]\n",
      "loss: 0.950444  [  200/10672]\n",
      "loss: 0.981972  [  300/10672]\n",
      "loss: 1.257624  [  400/10672]\n",
      "loss: 1.118412  [  500/10672]\n",
      "loss: 1.666640  [  600/10672]\n",
      "loss: 1.020917  [  700/10672]\n",
      "loss: 1.666046  [  800/10672]\n",
      "loss: 1.437341  [  900/10672]\n",
      "loss: 1.666695  [ 1000/10672]\n",
      "loss: 1.275362  [ 1100/10672]\n",
      "loss: 1.152336  [ 1200/10672]\n",
      "loss: 1.132314  [ 1300/10672]\n",
      "loss: 1.046732  [ 1400/10672]\n",
      "loss: 1.049384  [ 1500/10672]\n",
      "loss: 1.078172  [ 1600/10672]\n",
      "loss: 1.031953  [ 1700/10672]\n",
      "loss: 1.236854  [ 1800/10672]\n",
      "loss: 1.097871  [ 1900/10672]\n",
      "loss: 1.163562  [ 2000/10672]\n",
      "loss: 1.159163  [ 2100/10672]\n",
      "loss: 1.122640  [ 2200/10672]\n",
      "loss: 1.235383  [ 2300/10672]\n",
      "loss: 1.135525  [ 2400/10672]\n",
      "loss: 1.114753  [ 2500/10672]\n",
      "loss: 1.127486  [ 2600/10672]\n",
      "loss: 1.665484  [ 2700/10672]\n",
      "loss: 1.106366  [ 2800/10672]\n",
      "loss: 1.666886  [ 2900/10672]\n",
      "loss: 1.667581  [ 3000/10672]\n",
      "loss: 1.216919  [ 3100/10672]\n",
      "loss: 1.041662  [ 3200/10672]\n",
      "loss: 1.052371  [ 3300/10672]\n",
      "loss: 1.672945  [ 3400/10672]\n",
      "loss: 1.668810  [ 3500/10672]\n",
      "loss: 1.668863  [ 3600/10672]\n",
      "loss: 1.120749  [ 3700/10672]\n",
      "loss: 1.086073  [ 3800/10672]\n",
      "loss: 1.214733  [ 3900/10672]\n",
      "loss: 1.046218  [ 4000/10672]\n",
      "loss: 1.666061  [ 4100/10672]\n",
      "loss: 1.667713  [ 4200/10672]\n",
      "loss: 1.141583  [ 4300/10672]\n",
      "loss: 1.665465  [ 4400/10672]\n",
      "loss: 1.110631  [ 4500/10672]\n",
      "loss: 1.279223  [ 4600/10672]\n",
      "loss: 1.101086  [ 4700/10672]\n",
      "loss: 1.669032  [ 4800/10672]\n",
      "loss: 1.162326  [ 4900/10672]\n",
      "loss: 1.665481  [ 5000/10672]\n",
      "loss: 1.210366  [ 5100/10672]\n",
      "loss: 1.091481  [ 5200/10672]\n",
      "loss: 1.401324  [ 5300/10672]\n",
      "loss: 1.120968  [ 5400/10672]\n",
      "loss: 0.988737  [ 5500/10672]\n",
      "loss: 1.139446  [ 5600/10672]\n",
      "loss: 1.669357  [ 5700/10672]\n",
      "loss: 1.079116  [ 5800/10672]\n",
      "loss: 1.202025  [ 5900/10672]\n",
      "loss: 1.666584  [ 6000/10672]\n",
      "loss: 1.188023  [ 6100/10672]\n",
      "loss: 1.075761  [ 6200/10672]\n",
      "loss: 1.059747  [ 6300/10672]\n",
      "loss: 1.171049  [ 6400/10672]\n",
      "loss: 1.673194  [ 6500/10672]\n",
      "loss: 1.075777  [ 6600/10672]\n",
      "loss: 1.203737  [ 6700/10672]\n",
      "loss: 1.098592  [ 6800/10672]\n",
      "loss: 1.125243  [ 6900/10672]\n",
      "loss: 1.146194  [ 7000/10672]\n",
      "loss: 0.988759  [ 7100/10672]\n",
      "loss: 1.199966  [ 7200/10672]\n",
      "loss: 1.256682  [ 7300/10672]\n",
      "loss: 1.158292  [ 7400/10672]\n",
      "loss: 1.004657  [ 7500/10672]\n",
      "loss: 1.203790  [ 7600/10672]\n",
      "loss: 1.222619  [ 7700/10672]\n",
      "loss: 1.241285  [ 7800/10672]\n",
      "loss: 1.111583  [ 7900/10672]\n",
      "loss: 1.079931  [ 8000/10672]\n",
      "loss: 1.123309  [ 8100/10672]\n",
      "loss: 1.101445  [ 8200/10672]\n",
      "loss: 1.108934  [ 8300/10672]\n",
      "loss: 1.193047  [ 8400/10672]\n",
      "loss: 1.043457  [ 8500/10672]\n",
      "loss: 1.196935  [ 8600/10672]\n",
      "loss: 1.204356  [ 8700/10672]\n",
      "loss: 1.089040  [ 8800/10672]\n",
      "loss: 1.032248  [ 8900/10672]\n",
      "loss: 1.666011  [ 9000/10672]\n",
      "loss: 1.054408  [ 9100/10672]\n",
      "loss: 1.050712  [ 9200/10672]\n",
      "loss: 1.199215  [ 9300/10672]\n",
      "loss: 1.193706  [ 9400/10672]\n",
      "loss: 1.141804  [ 9500/10672]\n",
      "loss: 1.674110  [ 9600/10672]\n",
      "loss: 1.209351  [ 9700/10672]\n",
      "loss: 1.158947  [ 9800/10672]\n",
      "loss: 1.103073  [ 9900/10672]\n",
      "loss: 1.215224  [10000/10672]\n",
      "loss: 1.099821  [10100/10672]\n",
      "loss: 0.994782  [10200/10672]\n",
      "loss: 1.141856  [10300/10672]\n",
      "loss: 1.193845  [10400/10672]\n",
      "loss: 1.184916  [10500/10672]\n",
      "loss: 1.668491  [10600/10672]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.039084  [    0/10672]\n",
      "loss: 1.105601  [  100/10672]\n",
      "loss: 0.948498  [  200/10672]\n",
      "loss: 0.980309  [  300/10672]\n",
      "loss: 1.257899  [  400/10672]\n",
      "loss: 1.116983  [  500/10672]\n",
      "loss: 1.666665  [  600/10672]\n",
      "loss: 1.019738  [  700/10672]\n",
      "loss: 1.666071  [  800/10672]\n",
      "loss: 1.439881  [  900/10672]\n",
      "loss: 1.666714  [ 1000/10672]\n",
      "loss: 1.275897  [ 1100/10672]\n",
      "loss: 1.151189  [ 1200/10672]\n",
      "loss: 1.132458  [ 1300/10672]\n",
      "loss: 1.044513  [ 1400/10672]\n",
      "loss: 1.048494  [ 1500/10672]\n",
      "loss: 1.076232  [ 1600/10672]\n",
      "loss: 1.030873  [ 1700/10672]\n",
      "loss: 1.237145  [ 1800/10672]\n",
      "loss: 1.096289  [ 1900/10672]\n",
      "loss: 1.162719  [ 2000/10672]\n",
      "loss: 1.158458  [ 2100/10672]\n",
      "loss: 1.121415  [ 2200/10672]\n",
      "loss: 1.235717  [ 2300/10672]\n",
      "loss: 1.135451  [ 2400/10672]\n",
      "loss: 1.114442  [ 2500/10672]\n",
      "loss: 1.127448  [ 2600/10672]\n",
      "loss: 1.665513  [ 2700/10672]\n",
      "loss: 1.105901  [ 2800/10672]\n",
      "loss: 1.666991  [ 2900/10672]\n",
      "loss: 1.667640  [ 3000/10672]\n",
      "loss: 1.217260  [ 3100/10672]\n",
      "loss: 1.039756  [ 3200/10672]\n",
      "loss: 1.051290  [ 3300/10672]\n",
      "loss: 1.673124  [ 3400/10672]\n",
      "loss: 1.668890  [ 3500/10672]\n",
      "loss: 1.668985  [ 3600/10672]\n",
      "loss: 1.119552  [ 3700/10672]\n",
      "loss: 1.085498  [ 3800/10672]\n",
      "loss: 1.214670  [ 3900/10672]\n",
      "loss: 1.044022  [ 4000/10672]\n",
      "loss: 1.666099  [ 4100/10672]\n",
      "loss: 1.667841  [ 4200/10672]\n",
      "loss: 1.141632  [ 4300/10672]\n",
      "loss: 1.665494  [ 4400/10672]\n",
      "loss: 1.109255  [ 4500/10672]\n",
      "loss: 1.280235  [ 4600/10672]\n",
      "loss: 1.099741  [ 4700/10672]\n",
      "loss: 1.669123  [ 4800/10672]\n",
      "loss: 1.162755  [ 4900/10672]\n",
      "loss: 1.665524  [ 5000/10672]\n",
      "loss: 1.211483  [ 5100/10672]\n",
      "loss: 1.090739  [ 5200/10672]\n",
      "loss: 1.403863  [ 5300/10672]\n",
      "loss: 1.120803  [ 5400/10672]\n",
      "loss: 0.986258  [ 5500/10672]\n",
      "loss: 1.139408  [ 5600/10672]\n",
      "loss: 1.669452  [ 5700/10672]\n",
      "loss: 1.077403  [ 5800/10672]\n",
      "loss: 1.202908  [ 5900/10672]\n",
      "loss: 1.666646  [ 6000/10672]\n",
      "loss: 1.187868  [ 6100/10672]\n",
      "loss: 1.074902  [ 6200/10672]\n",
      "loss: 1.058133  [ 6300/10672]\n",
      "loss: 1.171708  [ 6400/10672]\n",
      "loss: 1.673358  [ 6500/10672]\n",
      "loss: 1.075163  [ 6600/10672]\n",
      "loss: 1.203663  [ 6700/10672]\n",
      "loss: 1.098080  [ 6800/10672]\n",
      "loss: 1.123974  [ 6900/10672]\n",
      "loss: 1.146435  [ 7000/10672]\n",
      "loss: 0.987123  [ 7100/10672]\n",
      "loss: 1.199816  [ 7200/10672]\n",
      "loss: 1.258265  [ 7300/10672]\n",
      "loss: 1.157763  [ 7400/10672]\n",
      "loss: 1.003024  [ 7500/10672]\n",
      "loss: 1.203632  [ 7600/10672]\n",
      "loss: 1.222818  [ 7700/10672]\n",
      "loss: 1.241632  [ 7800/10672]\n",
      "loss: 1.111273  [ 7900/10672]\n",
      "loss: 1.079321  [ 8000/10672]\n",
      "loss: 1.123013  [ 8100/10672]\n",
      "loss: 1.100015  [ 8200/10672]\n",
      "loss: 1.107615  [ 8300/10672]\n",
      "loss: 1.193859  [ 8400/10672]\n",
      "loss: 1.042475  [ 8500/10672]\n",
      "loss: 1.197789  [ 8600/10672]\n",
      "loss: 1.204451  [ 8700/10672]\n",
      "loss: 1.087708  [ 8800/10672]\n",
      "loss: 1.031054  [ 8900/10672]\n",
      "loss: 1.666054  [ 9000/10672]\n",
      "loss: 1.052259  [ 9100/10672]\n",
      "loss: 1.049626  [ 9200/10672]\n",
      "loss: 1.200194  [ 9300/10672]\n",
      "loss: 1.194656  [ 9400/10672]\n",
      "loss: 1.140733  [ 9500/10672]\n",
      "loss: 1.674257  [ 9600/10672]\n",
      "loss: 1.209095  [ 9700/10672]\n",
      "loss: 1.159488  [ 9800/10672]\n",
      "loss: 1.102861  [ 9900/10672]\n",
      "loss: 1.216517  [10000/10672]\n",
      "loss: 1.099412  [10100/10672]\n",
      "loss: 0.993244  [10200/10672]\n",
      "loss: 1.140830  [10300/10672]\n",
      "loss: 1.193385  [10400/10672]\n",
      "loss: 1.184322  [10500/10672]\n",
      "loss: 1.668583  [10600/10672]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.038253  [    0/10672]\n",
      "loss: 1.105238  [  100/10672]\n",
      "loss: 0.946575  [  200/10672]\n",
      "loss: 0.978661  [  300/10672]\n",
      "loss: 1.258172  [  400/10672]\n",
      "loss: 1.115568  [  500/10672]\n",
      "loss: 1.666689  [  600/10672]\n",
      "loss: 1.018565  [  700/10672]\n",
      "loss: 1.666095  [  800/10672]\n",
      "loss: 1.442396  [  900/10672]\n",
      "loss: 1.666733  [ 1000/10672]\n",
      "loss: 1.276427  [ 1100/10672]\n",
      "loss: 1.150047  [ 1200/10672]\n",
      "loss: 1.132596  [ 1300/10672]\n",
      "loss: 1.042319  [ 1400/10672]\n",
      "loss: 1.047608  [ 1500/10672]\n",
      "loss: 1.074309  [ 1600/10672]\n",
      "loss: 1.029800  [ 1700/10672]\n",
      "loss: 1.237437  [ 1800/10672]\n",
      "loss: 1.094722  [ 1900/10672]\n",
      "loss: 1.161881  [ 2000/10672]\n",
      "loss: 1.157764  [ 2100/10672]\n",
      "loss: 1.120203  [ 2200/10672]\n",
      "loss: 1.236054  [ 2300/10672]\n",
      "loss: 1.135370  [ 2400/10672]\n",
      "loss: 1.114126  [ 2500/10672]\n",
      "loss: 1.127408  [ 2600/10672]\n",
      "loss: 1.665542  [ 2700/10672]\n",
      "loss: 1.105433  [ 2800/10672]\n",
      "loss: 1.667094  [ 2900/10672]\n",
      "loss: 1.667699  [ 3000/10672]\n",
      "loss: 1.217607  [ 3100/10672]\n",
      "loss: 1.037875  [ 3200/10672]\n",
      "loss: 1.050214  [ 3300/10672]\n",
      "loss: 1.673303  [ 3400/10672]\n",
      "loss: 1.668971  [ 3500/10672]\n",
      "loss: 1.669106  [ 3600/10672]\n",
      "loss: 1.118365  [ 3700/10672]\n",
      "loss: 1.084923  [ 3800/10672]\n",
      "loss: 1.214608  [ 3900/10672]\n",
      "loss: 1.041848  [ 4000/10672]\n",
      "loss: 1.666136  [ 4100/10672]\n",
      "loss: 1.667969  [ 4200/10672]\n",
      "loss: 1.141674  [ 4300/10672]\n",
      "loss: 1.665523  [ 4400/10672]\n",
      "loss: 1.107891  [ 4500/10672]\n",
      "loss: 1.281245  [ 4600/10672]\n",
      "loss: 1.098411  [ 4700/10672]\n",
      "loss: 1.669214  [ 4800/10672]\n",
      "loss: 1.163179  [ 4900/10672]\n",
      "loss: 1.665566  [ 5000/10672]\n",
      "loss: 1.212592  [ 5100/10672]\n",
      "loss: 1.089994  [ 5200/10672]\n",
      "loss: 1.406383  [ 5300/10672]\n",
      "loss: 1.120633  [ 5400/10672]\n",
      "loss: 0.983815  [ 5500/10672]\n",
      "loss: 1.139362  [ 5600/10672]\n",
      "loss: 1.669547  [ 5700/10672]\n",
      "loss: 1.075707  [ 5800/10672]\n",
      "loss: 1.203782  [ 5900/10672]\n",
      "loss: 1.666708  [ 6000/10672]\n",
      "loss: 1.187720  [ 6100/10672]\n",
      "loss: 1.074046  [ 6200/10672]\n",
      "loss: 1.056542  [ 6300/10672]\n",
      "loss: 1.172362  [ 6400/10672]\n",
      "loss: 1.673523  [ 6500/10672]\n",
      "loss: 1.074552  [ 6600/10672]\n",
      "loss: 1.203594  [ 6700/10672]\n",
      "loss: 1.097567  [ 6800/10672]\n",
      "loss: 1.122712  [ 6900/10672]\n",
      "loss: 1.146675  [ 7000/10672]\n",
      "loss: 0.985504  [ 7100/10672]\n",
      "loss: 1.199669  [ 7200/10672]\n",
      "loss: 1.259836  [ 7300/10672]\n",
      "loss: 1.157240  [ 7400/10672]\n",
      "loss: 1.001408  [ 7500/10672]\n",
      "loss: 1.203474  [ 7600/10672]\n",
      "loss: 1.223015  [ 7700/10672]\n",
      "loss: 1.241973  [ 7800/10672]\n",
      "loss: 1.110962  [ 7900/10672]\n",
      "loss: 1.078715  [ 8000/10672]\n",
      "loss: 1.122713  [ 8100/10672]\n",
      "loss: 1.098598  [ 8200/10672]\n",
      "loss: 1.106309  [ 8300/10672]\n",
      "loss: 1.194666  [ 8400/10672]\n",
      "loss: 1.041502  [ 8500/10672]\n",
      "loss: 1.198639  [ 8600/10672]\n",
      "loss: 1.204552  [ 8700/10672]\n",
      "loss: 1.086391  [ 8800/10672]\n",
      "loss: 1.029869  [ 8900/10672]\n",
      "loss: 1.666096  [ 9000/10672]\n",
      "loss: 1.050127  [ 9100/10672]\n",
      "loss: 1.048547  [ 9200/10672]\n",
      "loss: 1.201169  [ 9300/10672]\n",
      "loss: 1.195601  [ 9400/10672]\n",
      "loss: 1.139670  [ 9500/10672]\n",
      "loss: 1.674404  [ 9600/10672]\n",
      "loss: 1.208838  [ 9700/10672]\n",
      "loss: 1.160028  [ 9800/10672]\n",
      "loss: 1.102651  [ 9900/10672]\n",
      "loss: 1.217802  [10000/10672]\n",
      "loss: 1.099002  [10100/10672]\n",
      "loss: 0.991720  [10200/10672]\n",
      "loss: 1.139814  [10300/10672]\n",
      "loss: 1.192930  [10400/10672]\n",
      "loss: 1.183734  [10500/10672]\n",
      "loss: 1.668674  [10600/10672]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.037431  [    0/10672]\n",
      "loss: 1.104870  [  100/10672]\n",
      "loss: 0.944676  [  200/10672]\n",
      "loss: 0.977029  [  300/10672]\n",
      "loss: 1.258442  [  400/10672]\n",
      "loss: 1.114166  [  500/10672]\n",
      "loss: 1.666713  [  600/10672]\n",
      "loss: 1.017399  [  700/10672]\n",
      "loss: 1.666119  [  800/10672]\n",
      "loss: 1.444886  [  900/10672]\n",
      "loss: 1.666751  [ 1000/10672]\n",
      "loss: 1.276951  [ 1100/10672]\n",
      "loss: 1.148909  [ 1200/10672]\n",
      "loss: 1.132729  [ 1300/10672]\n",
      "loss: 1.040149  [ 1400/10672]\n",
      "loss: 1.046728  [ 1500/10672]\n",
      "loss: 1.072405  [ 1600/10672]\n",
      "loss: 1.028731  [ 1700/10672]\n",
      "loss: 1.237732  [ 1800/10672]\n",
      "loss: 1.093171  [ 1900/10672]\n",
      "loss: 1.161048  [ 2000/10672]\n",
      "loss: 1.157081  [ 2100/10672]\n",
      "loss: 1.119004  [ 2200/10672]\n",
      "loss: 1.236389  [ 2300/10672]\n",
      "loss: 1.135285  [ 2400/10672]\n",
      "loss: 1.113810  [ 2500/10672]\n",
      "loss: 1.127369  [ 2600/10672]\n",
      "loss: 1.665570  [ 2700/10672]\n",
      "loss: 1.104964  [ 2800/10672]\n",
      "loss: 1.667196  [ 2900/10672]\n",
      "loss: 1.667757  [ 3000/10672]\n",
      "loss: 1.217957  [ 3100/10672]\n",
      "loss: 1.036017  [ 3200/10672]\n",
      "loss: 1.049144  [ 3300/10672]\n",
      "loss: 1.673482  [ 3400/10672]\n",
      "loss: 1.669051  [ 3500/10672]\n",
      "loss: 1.669226  [ 3600/10672]\n",
      "loss: 1.117188  [ 3700/10672]\n",
      "loss: 1.084350  [ 3800/10672]\n",
      "loss: 1.214547  [ 3900/10672]\n",
      "loss: 1.039696  [ 4000/10672]\n",
      "loss: 1.666172  [ 4100/10672]\n",
      "loss: 1.668097  [ 4200/10672]\n",
      "loss: 1.141710  [ 4300/10672]\n",
      "loss: 1.665551  [ 4400/10672]\n",
      "loss: 1.106536  [ 4500/10672]\n",
      "loss: 1.282251  [ 4600/10672]\n",
      "loss: 1.097095  [ 4700/10672]\n",
      "loss: 1.669304  [ 4800/10672]\n",
      "loss: 1.163601  [ 4900/10672]\n",
      "loss: 1.665606  [ 5000/10672]\n",
      "loss: 1.213698  [ 5100/10672]\n",
      "loss: 1.089250  [ 5200/10672]\n",
      "loss: 1.408882  [ 5300/10672]\n",
      "loss: 1.120462  [ 5400/10672]\n",
      "loss: 0.981404  [ 5500/10672]\n",
      "loss: 1.139312  [ 5600/10672]\n",
      "loss: 1.669641  [ 5700/10672]\n",
      "loss: 1.074021  [ 5800/10672]\n",
      "loss: 1.204651  [ 5900/10672]\n",
      "loss: 1.666768  [ 6000/10672]\n",
      "loss: 1.187574  [ 6100/10672]\n",
      "loss: 1.073197  [ 6200/10672]\n",
      "loss: 1.054969  [ 6300/10672]\n",
      "loss: 1.173015  [ 6400/10672]\n",
      "loss: 1.673687  [ 6500/10672]\n",
      "loss: 1.073947  [ 6600/10672]\n",
      "loss: 1.203528  [ 6700/10672]\n",
      "loss: 1.097057  [ 6800/10672]\n",
      "loss: 1.121454  [ 6900/10672]\n",
      "loss: 1.146914  [ 7000/10672]\n",
      "loss: 0.983902  [ 7100/10672]\n",
      "loss: 1.199521  [ 7200/10672]\n",
      "loss: 1.261394  [ 7300/10672]\n",
      "loss: 1.156721  [ 7400/10672]\n",
      "loss: 0.999808  [ 7500/10672]\n",
      "loss: 1.203316  [ 7600/10672]\n",
      "loss: 1.223209  [ 7700/10672]\n",
      "loss: 1.242310  [ 7800/10672]\n",
      "loss: 1.110649  [ 7900/10672]\n",
      "loss: 1.078110  [ 8000/10672]\n",
      "loss: 1.122409  [ 8100/10672]\n",
      "loss: 1.097197  [ 8200/10672]\n",
      "loss: 1.105016  [ 8300/10672]\n",
      "loss: 1.195466  [ 8400/10672]\n",
      "loss: 1.040536  [ 8500/10672]\n",
      "loss: 1.199484  [ 8600/10672]\n",
      "loss: 1.204662  [ 8700/10672]\n",
      "loss: 1.085089  [ 8800/10672]\n",
      "loss: 1.028693  [ 8900/10672]\n",
      "loss: 1.666139  [ 9000/10672]\n",
      "loss: 1.048014  [ 9100/10672]\n",
      "loss: 1.047473  [ 9200/10672]\n",
      "loss: 1.202135  [ 9300/10672]\n",
      "loss: 1.196538  [ 9400/10672]\n",
      "loss: 1.138619  [ 9500/10672]\n",
      "loss: 1.674550  [ 9600/10672]\n",
      "loss: 1.208582  [ 9700/10672]\n",
      "loss: 1.160563  [ 9800/10672]\n",
      "loss: 1.102442  [ 9900/10672]\n",
      "loss: 1.219077  [10000/10672]\n",
      "loss: 1.098589  [10100/10672]\n",
      "loss: 0.990209  [10200/10672]\n",
      "loss: 1.138808  [10300/10672]\n",
      "loss: 1.192482  [10400/10672]\n",
      "loss: 1.183151  [10500/10672]\n",
      "loss: 1.668764  [10600/10672]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.036618  [    0/10672]\n",
      "loss: 1.104499  [  100/10672]\n",
      "loss: 0.942800  [  200/10672]\n",
      "loss: 0.975412  [  300/10672]\n",
      "loss: 1.258708  [  400/10672]\n",
      "loss: 1.112776  [  500/10672]\n",
      "loss: 1.666735  [  600/10672]\n",
      "loss: 1.016240  [  700/10672]\n",
      "loss: 1.666141  [  800/10672]\n",
      "loss: 1.447349  [  900/10672]\n",
      "loss: 1.666769  [ 1000/10672]\n",
      "loss: 1.277469  [ 1100/10672]\n",
      "loss: 1.147775  [ 1200/10672]\n",
      "loss: 1.132860  [ 1300/10672]\n",
      "loss: 1.038001  [ 1400/10672]\n",
      "loss: 1.045855  [ 1500/10672]\n",
      "loss: 1.070514  [ 1600/10672]\n",
      "loss: 1.027672  [ 1700/10672]\n",
      "loss: 1.238023  [ 1800/10672]\n",
      "loss: 1.091632  [ 1900/10672]\n",
      "loss: 1.160218  [ 2000/10672]\n",
      "loss: 1.156405  [ 2100/10672]\n",
      "loss: 1.117815  [ 2200/10672]\n",
      "loss: 1.236723  [ 2300/10672]\n",
      "loss: 1.135197  [ 2400/10672]\n",
      "loss: 1.113493  [ 2500/10672]\n",
      "loss: 1.127333  [ 2600/10672]\n",
      "loss: 1.665596  [ 2700/10672]\n",
      "loss: 1.104496  [ 2800/10672]\n",
      "loss: 1.667297  [ 2900/10672]\n",
      "loss: 1.667814  [ 3000/10672]\n",
      "loss: 1.218308  [ 3100/10672]\n",
      "loss: 1.034181  [ 3200/10672]\n",
      "loss: 1.048080  [ 3300/10672]\n",
      "loss: 1.673660  [ 3400/10672]\n",
      "loss: 1.669131  [ 3500/10672]\n",
      "loss: 1.669346  [ 3600/10672]\n",
      "loss: 1.116020  [ 3700/10672]\n",
      "loss: 1.083778  [ 3800/10672]\n",
      "loss: 1.214486  [ 3900/10672]\n",
      "loss: 1.037567  [ 4000/10672]\n",
      "loss: 1.666208  [ 4100/10672]\n",
      "loss: 1.668225  [ 4200/10672]\n",
      "loss: 1.141740  [ 4300/10672]\n",
      "loss: 1.665577  [ 4400/10672]\n",
      "loss: 1.105194  [ 4500/10672]\n",
      "loss: 1.283255  [ 4600/10672]\n",
      "loss: 1.095795  [ 4700/10672]\n",
      "loss: 1.669394  [ 4800/10672]\n",
      "loss: 1.164018  [ 4900/10672]\n",
      "loss: 1.665646  [ 5000/10672]\n",
      "loss: 1.214795  [ 5100/10672]\n",
      "loss: 1.088501  [ 5200/10672]\n",
      "loss: 1.411364  [ 5300/10672]\n",
      "loss: 1.120285  [ 5400/10672]\n",
      "loss: 0.979028  [ 5500/10672]\n",
      "loss: 1.139252  [ 5600/10672]\n",
      "loss: 1.669735  [ 5700/10672]\n",
      "loss: 1.072352  [ 5800/10672]\n",
      "loss: 1.205509  [ 5900/10672]\n",
      "loss: 1.666828  [ 6000/10672]\n",
      "loss: 1.187436  [ 6100/10672]\n",
      "loss: 1.072349  [ 6200/10672]\n",
      "loss: 1.053421  [ 6300/10672]\n",
      "loss: 1.173663  [ 6400/10672]\n",
      "loss: 1.673851  [ 6500/10672]\n",
      "loss: 1.073343  [ 6600/10672]\n",
      "loss: 1.203468  [ 6700/10672]\n",
      "loss: 1.096545  [ 6800/10672]\n",
      "loss: 1.120205  [ 6900/10672]\n",
      "loss: 1.147151  [ 7000/10672]\n",
      "loss: 0.982316  [ 7100/10672]\n",
      "loss: 1.199377  [ 7200/10672]\n",
      "loss: 1.262938  [ 7300/10672]\n",
      "loss: 1.156211  [ 7400/10672]\n",
      "loss: 0.998221  [ 7500/10672]\n",
      "loss: 1.203160  [ 7600/10672]\n",
      "loss: 1.223404  [ 7700/10672]\n",
      "loss: 1.242643  [ 7800/10672]\n",
      "loss: 1.110334  [ 7900/10672]\n",
      "loss: 1.077507  [ 8000/10672]\n",
      "loss: 1.122101  [ 8100/10672]\n",
      "loss: 1.095810  [ 8200/10672]\n",
      "loss: 1.103736  [ 8300/10672]\n",
      "loss: 1.196261  [ 8400/10672]\n",
      "loss: 1.039577  [ 8500/10672]\n",
      "loss: 1.200322  [ 8600/10672]\n",
      "loss: 1.204778  [ 8700/10672]\n",
      "loss: 1.083804  [ 8800/10672]\n",
      "loss: 1.027526  [ 8900/10672]\n",
      "loss: 1.666180  [ 9000/10672]\n",
      "loss: 1.045918  [ 9100/10672]\n",
      "loss: 1.046406  [ 9200/10672]\n",
      "loss: 1.203096  [ 9300/10672]\n",
      "loss: 1.197470  [ 9400/10672]\n",
      "loss: 1.137577  [ 9500/10672]\n",
      "loss: 1.674695  [ 9600/10672]\n",
      "loss: 1.208324  [ 9700/10672]\n",
      "loss: 1.161097  [ 9800/10672]\n",
      "loss: 1.102235  [ 9900/10672]\n",
      "loss: 1.220344  [10000/10672]\n",
      "loss: 1.098175  [10100/10672]\n",
      "loss: 0.988712  [10200/10672]\n",
      "loss: 1.137810  [10300/10672]\n",
      "loss: 1.192037  [10400/10672]\n",
      "loss: 1.182572  [10500/10672]\n",
      "loss: 1.668852  [10600/10672]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.035815  [    0/10672]\n",
      "loss: 1.104124  [  100/10672]\n",
      "loss: 0.940947  [  200/10672]\n",
      "loss: 0.973811  [  300/10672]\n",
      "loss: 1.258971  [  400/10672]\n",
      "loss: 1.111399  [  500/10672]\n",
      "loss: 1.666757  [  600/10672]\n",
      "loss: 1.015088  [  700/10672]\n",
      "loss: 1.666163  [  800/10672]\n",
      "loss: 1.449785  [  900/10672]\n",
      "loss: 1.666785  [ 1000/10672]\n",
      "loss: 1.277982  [ 1100/10672]\n",
      "loss: 1.146646  [ 1200/10672]\n",
      "loss: 1.132986  [ 1300/10672]\n",
      "loss: 1.035877  [ 1400/10672]\n",
      "loss: 1.044988  [ 1500/10672]\n",
      "loss: 1.068641  [ 1600/10672]\n",
      "loss: 1.026617  [ 1700/10672]\n",
      "loss: 1.238316  [ 1800/10672]\n",
      "loss: 1.090107  [ 1900/10672]\n",
      "loss: 1.159392  [ 2000/10672]\n",
      "loss: 1.155738  [ 2100/10672]\n",
      "loss: 1.116637  [ 2200/10672]\n",
      "loss: 1.237055  [ 2300/10672]\n",
      "loss: 1.135106  [ 2400/10672]\n",
      "loss: 1.113175  [ 2500/10672]\n",
      "loss: 1.127298  [ 2600/10672]\n",
      "loss: 1.665622  [ 2700/10672]\n",
      "loss: 1.104028  [ 2800/10672]\n",
      "loss: 1.667397  [ 2900/10672]\n",
      "loss: 1.667870  [ 3000/10672]\n",
      "loss: 1.218662  [ 3100/10672]\n",
      "loss: 1.032366  [ 3200/10672]\n",
      "loss: 1.047023  [ 3300/10672]\n",
      "loss: 1.673838  [ 3400/10672]\n",
      "loss: 1.669210  [ 3500/10672]\n",
      "loss: 1.669466  [ 3600/10672]\n",
      "loss: 1.114860  [ 3700/10672]\n",
      "loss: 1.083208  [ 3800/10672]\n",
      "loss: 1.214425  [ 3900/10672]\n",
      "loss: 1.035457  [ 4000/10672]\n",
      "loss: 1.666242  [ 4100/10672]\n",
      "loss: 1.668354  [ 4200/10672]\n",
      "loss: 1.141766  [ 4300/10672]\n",
      "loss: 1.665603  [ 4400/10672]\n",
      "loss: 1.103858  [ 4500/10672]\n",
      "loss: 1.284253  [ 4600/10672]\n",
      "loss: 1.094507  [ 4700/10672]\n",
      "loss: 1.669483  [ 4800/10672]\n",
      "loss: 1.164434  [ 4900/10672]\n",
      "loss: 1.665685  [ 5000/10672]\n",
      "loss: 1.215891  [ 5100/10672]\n",
      "loss: 1.087756  [ 5200/10672]\n",
      "loss: 1.413822  [ 5300/10672]\n",
      "loss: 1.120109  [ 5400/10672]\n",
      "loss: 0.976681  [ 5500/10672]\n",
      "loss: 1.139192  [ 5600/10672]\n",
      "loss: 1.669828  [ 5700/10672]\n",
      "loss: 1.070692  [ 5800/10672]\n",
      "loss: 1.206365  [ 5900/10672]\n",
      "loss: 1.666887  [ 6000/10672]\n",
      "loss: 1.187299  [ 6100/10672]\n",
      "loss: 1.071509  [ 6200/10672]\n",
      "loss: 1.051890  [ 6300/10672]\n",
      "loss: 1.174312  [ 6400/10672]\n",
      "loss: 1.674015  [ 6500/10672]\n",
      "loss: 1.072748  [ 6600/10672]\n",
      "loss: 1.203408  [ 6700/10672]\n",
      "loss: 1.096037  [ 6800/10672]\n",
      "loss: 1.118958  [ 6900/10672]\n",
      "loss: 1.147390  [ 7000/10672]\n",
      "loss: 0.980748  [ 7100/10672]\n",
      "loss: 1.199231  [ 7200/10672]\n",
      "loss: 1.264472  [ 7300/10672]\n",
      "loss: 1.155703  [ 7400/10672]\n",
      "loss: 0.996652  [ 7500/10672]\n",
      "loss: 1.203003  [ 7600/10672]\n",
      "loss: 1.223596  [ 7700/10672]\n",
      "loss: 1.242969  [ 7800/10672]\n",
      "loss: 1.110018  [ 7900/10672]\n",
      "loss: 1.076908  [ 8000/10672]\n",
      "loss: 1.121791  [ 8100/10672]\n",
      "loss: 1.094437  [ 8200/10672]\n",
      "loss: 1.102468  [ 8300/10672]\n",
      "loss: 1.197052  [ 8400/10672]\n",
      "loss: 1.038627  [ 8500/10672]\n",
      "loss: 1.201157  [ 8600/10672]\n",
      "loss: 1.204901  [ 8700/10672]\n",
      "loss: 1.082532  [ 8800/10672]\n",
      "loss: 1.026367  [ 8900/10672]\n",
      "loss: 1.666222  [ 9000/10672]\n",
      "loss: 1.043840  [ 9100/10672]\n",
      "loss: 1.045345  [ 9200/10672]\n",
      "loss: 1.204051  [ 9300/10672]\n",
      "loss: 1.198395  [ 9400/10672]\n",
      "loss: 1.136544  [ 9500/10672]\n",
      "loss: 1.674840  [ 9600/10672]\n",
      "loss: 1.208065  [ 9700/10672]\n",
      "loss: 1.161629  [ 9800/10672]\n",
      "loss: 1.102030  [ 9900/10672]\n",
      "loss: 1.221604  [10000/10672]\n",
      "loss: 1.097760  [10100/10672]\n",
      "loss: 0.987229  [10200/10672]\n",
      "loss: 1.136821  [10300/10672]\n",
      "loss: 1.191596  [10400/10672]\n",
      "loss: 1.181997  [10500/10672]\n",
      "loss: 1.668940  [10600/10672]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.035021  [    0/10672]\n",
      "loss: 1.103745  [  100/10672]\n",
      "loss: 0.939117  [  200/10672]\n",
      "loss: 0.972225  [  300/10672]\n",
      "loss: 1.259230  [  400/10672]\n",
      "loss: 1.110034  [  500/10672]\n",
      "loss: 1.666778  [  600/10672]\n",
      "loss: 1.013942  [  700/10672]\n",
      "loss: 1.666183  [  800/10672]\n",
      "loss: 1.452197  [  900/10672]\n",
      "loss: 1.666802  [ 1000/10672]\n",
      "loss: 1.278489  [ 1100/10672]\n",
      "loss: 1.145521  [ 1200/10672]\n",
      "loss: 1.133107  [ 1300/10672]\n",
      "loss: 1.033776  [ 1400/10672]\n",
      "loss: 1.044126  [ 1500/10672]\n",
      "loss: 1.066784  [ 1600/10672]\n",
      "loss: 1.025568  [ 1700/10672]\n",
      "loss: 1.238609  [ 1800/10672]\n",
      "loss: 1.088596  [ 1900/10672]\n",
      "loss: 1.158571  [ 2000/10672]\n",
      "loss: 1.155081  [ 2100/10672]\n",
      "loss: 1.115470  [ 2200/10672]\n",
      "loss: 1.237387  [ 2300/10672]\n",
      "loss: 1.135011  [ 2400/10672]\n",
      "loss: 1.112856  [ 2500/10672]\n",
      "loss: 1.127264  [ 2600/10672]\n",
      "loss: 1.665648  [ 2700/10672]\n",
      "loss: 1.103559  [ 2800/10672]\n",
      "loss: 1.667497  [ 2900/10672]\n",
      "loss: 1.667925  [ 3000/10672]\n",
      "loss: 1.219019  [ 3100/10672]\n",
      "loss: 1.030575  [ 3200/10672]\n",
      "loss: 1.045972  [ 3300/10672]\n",
      "loss: 1.674016  [ 3400/10672]\n",
      "loss: 1.669289  [ 3500/10672]\n",
      "loss: 1.669586  [ 3600/10672]\n",
      "loss: 1.113710  [ 3700/10672]\n",
      "loss: 1.082639  [ 3800/10672]\n",
      "loss: 1.214364  [ 3900/10672]\n",
      "loss: 1.033370  [ 4000/10672]\n",
      "loss: 1.666276  [ 4100/10672]\n",
      "loss: 1.668482  [ 4200/10672]\n",
      "loss: 1.141786  [ 4300/10672]\n",
      "loss: 1.665628  [ 4400/10672]\n",
      "loss: 1.102534  [ 4500/10672]\n",
      "loss: 1.285250  [ 4600/10672]\n",
      "loss: 1.093234  [ 4700/10672]\n",
      "loss: 1.669572  [ 4800/10672]\n",
      "loss: 1.164846  [ 4900/10672]\n",
      "loss: 1.665722  [ 5000/10672]\n",
      "loss: 1.216979  [ 5100/10672]\n",
      "loss: 1.087007  [ 5200/10672]\n",
      "loss: 1.416263  [ 5300/10672]\n",
      "loss: 1.119928  [ 5400/10672]\n",
      "loss: 0.974368  [ 5500/10672]\n",
      "loss: 1.139123  [ 5600/10672]\n",
      "loss: 1.669921  [ 5700/10672]\n",
      "loss: 1.069047  [ 5800/10672]\n",
      "loss: 1.207212  [ 5900/10672]\n",
      "loss: 1.666946  [ 6000/10672]\n",
      "loss: 1.187168  [ 6100/10672]\n",
      "loss: 1.070671  [ 6200/10672]\n",
      "loss: 1.050381  [ 6300/10672]\n",
      "loss: 1.174955  [ 6400/10672]\n",
      "loss: 1.674179  [ 6500/10672]\n",
      "loss: 1.072153  [ 6600/10672]\n",
      "loss: 1.203355  [ 6700/10672]\n",
      "loss: 1.095528  [ 6800/10672]\n",
      "loss: 1.117721  [ 6900/10672]\n",
      "loss: 1.147626  [ 7000/10672]\n",
      "loss: 0.979194  [ 7100/10672]\n",
      "loss: 1.199089  [ 7200/10672]\n",
      "loss: 1.265992  [ 7300/10672]\n",
      "loss: 1.155204  [ 7400/10672]\n",
      "loss: 0.995096  [ 7500/10672]\n",
      "loss: 1.202848  [ 7600/10672]\n",
      "loss: 1.223787  [ 7700/10672]\n",
      "loss: 1.243292  [ 7800/10672]\n",
      "loss: 1.109700  [ 7900/10672]\n",
      "loss: 1.076309  [ 8000/10672]\n",
      "loss: 1.121476  [ 8100/10672]\n",
      "loss: 1.093080  [ 8200/10672]\n",
      "loss: 1.101213  [ 8300/10672]\n",
      "loss: 1.197836  [ 8400/10672]\n",
      "loss: 1.037684  [ 8500/10672]\n",
      "loss: 1.201986  [ 8600/10672]\n",
      "loss: 1.205032  [ 8700/10672]\n",
      "loss: 1.081277  [ 8800/10672]\n",
      "loss: 1.025218  [ 8900/10672]\n",
      "loss: 1.666263  [ 9000/10672]\n",
      "loss: 1.041781  [ 9100/10672]\n",
      "loss: 1.044290  [ 9200/10672]\n",
      "loss: 1.204998  [ 9300/10672]\n",
      "loss: 1.199313  [ 9400/10672]\n",
      "loss: 1.135521  [ 9500/10672]\n",
      "loss: 1.674985  [ 9600/10672]\n",
      "loss: 1.207805  [ 9700/10672]\n",
      "loss: 1.162158  [ 9800/10672]\n",
      "loss: 1.101824  [ 9900/10672]\n",
      "loss: 1.222854  [10000/10672]\n",
      "loss: 1.097341  [10100/10672]\n",
      "loss: 0.985758  [10200/10672]\n",
      "loss: 1.135843  [10300/10672]\n",
      "loss: 1.191162  [10400/10672]\n",
      "loss: 1.181427  [10500/10672]\n",
      "loss: 1.669027  [10600/10672]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.034234  [    0/10672]\n",
      "loss: 1.103362  [  100/10672]\n",
      "loss: 0.937309  [  200/10672]\n",
      "loss: 0.970653  [  300/10672]\n",
      "loss: 1.259488  [  400/10672]\n",
      "loss: 1.108682  [  500/10672]\n",
      "loss: 1.666799  [  600/10672]\n",
      "loss: 1.012803  [  700/10672]\n",
      "loss: 1.666203  [  800/10672]\n",
      "loss: 1.454583  [  900/10672]\n",
      "loss: 1.666817  [ 1000/10672]\n",
      "loss: 1.278990  [ 1100/10672]\n",
      "loss: 1.144398  [ 1200/10672]\n",
      "loss: 1.133226  [ 1300/10672]\n",
      "loss: 1.031697  [ 1400/10672]\n",
      "loss: 1.043272  [ 1500/10672]\n",
      "loss: 1.064941  [ 1600/10672]\n",
      "loss: 1.024528  [ 1700/10672]\n",
      "loss: 1.238899  [ 1800/10672]\n",
      "loss: 1.087097  [ 1900/10672]\n",
      "loss: 1.157751  [ 2000/10672]\n",
      "loss: 1.154431  [ 2100/10672]\n",
      "loss: 1.114312  [ 2200/10672]\n",
      "loss: 1.237716  [ 2300/10672]\n",
      "loss: 1.134914  [ 2400/10672]\n",
      "loss: 1.112538  [ 2500/10672]\n",
      "loss: 1.127233  [ 2600/10672]\n",
      "loss: 1.665673  [ 2700/10672]\n",
      "loss: 1.103092  [ 2800/10672]\n",
      "loss: 1.667595  [ 2900/10672]\n",
      "loss: 1.667979  [ 3000/10672]\n",
      "loss: 1.219376  [ 3100/10672]\n",
      "loss: 1.028802  [ 3200/10672]\n",
      "loss: 1.044928  [ 3300/10672]\n",
      "loss: 1.674194  [ 3400/10672]\n",
      "loss: 1.669367  [ 3500/10672]\n",
      "loss: 1.669705  [ 3600/10672]\n",
      "loss: 1.112565  [ 3700/10672]\n",
      "loss: 1.082075  [ 3800/10672]\n",
      "loss: 1.214300  [ 3900/10672]\n",
      "loss: 1.031300  [ 4000/10672]\n",
      "loss: 1.666309  [ 4100/10672]\n",
      "loss: 1.668611  [ 4200/10672]\n",
      "loss: 1.141804  [ 4300/10672]\n",
      "loss: 1.665652  [ 4400/10672]\n",
      "loss: 1.101217  [ 4500/10672]\n",
      "loss: 1.286240  [ 4600/10672]\n",
      "loss: 1.091972  [ 4700/10672]\n",
      "loss: 1.669660  [ 4800/10672]\n",
      "loss: 1.165257  [ 4900/10672]\n",
      "loss: 1.665759  [ 5000/10672]\n",
      "loss: 1.218065  [ 5100/10672]\n",
      "loss: 1.086260  [ 5200/10672]\n",
      "loss: 1.418682  [ 5300/10672]\n",
      "loss: 1.119747  [ 5400/10672]\n",
      "loss: 0.972086  [ 5500/10672]\n",
      "loss: 1.139051  [ 5600/10672]\n",
      "loss: 1.670013  [ 5700/10672]\n",
      "loss: 1.067412  [ 5800/10672]\n",
      "loss: 1.208053  [ 5900/10672]\n",
      "loss: 1.667004  [ 6000/10672]\n",
      "loss: 1.187038  [ 6100/10672]\n",
      "loss: 1.069840  [ 6200/10672]\n",
      "loss: 1.048890  [ 6300/10672]\n",
      "loss: 1.175598  [ 6400/10672]\n",
      "loss: 1.674342  [ 6500/10672]\n",
      "loss: 1.071565  [ 6600/10672]\n",
      "loss: 1.203302  [ 6700/10672]\n",
      "loss: 1.095022  [ 6800/10672]\n",
      "loss: 1.116486  [ 6900/10672]\n",
      "loss: 1.147862  [ 7000/10672]\n",
      "loss: 0.977658  [ 7100/10672]\n",
      "loss: 1.198946  [ 7200/10672]\n",
      "loss: 1.267500  [ 7300/10672]\n",
      "loss: 1.154708  [ 7400/10672]\n",
      "loss: 0.993556  [ 7500/10672]\n",
      "loss: 1.202693  [ 7600/10672]\n",
      "loss: 1.223977  [ 7700/10672]\n",
      "loss: 1.243609  [ 7800/10672]\n",
      "loss: 1.109380  [ 7900/10672]\n",
      "loss: 1.075714  [ 8000/10672]\n",
      "loss: 1.121159  [ 8100/10672]\n",
      "loss: 1.091735  [ 8200/10672]\n",
      "loss: 1.099970  [ 8300/10672]\n",
      "loss: 1.198615  [ 8400/10672]\n",
      "loss: 1.036748  [ 8500/10672]\n",
      "loss: 1.202810  [ 8600/10672]\n",
      "loss: 1.205170  [ 8700/10672]\n",
      "loss: 1.080037  [ 8800/10672]\n",
      "loss: 1.024075  [ 8900/10672]\n",
      "loss: 1.666303  [ 9000/10672]\n",
      "loss: 1.039740  [ 9100/10672]\n",
      "loss: 1.043239  [ 9200/10672]\n",
      "loss: 1.205938  [ 9300/10672]\n",
      "loss: 1.200222  [ 9400/10672]\n",
      "loss: 1.134509  [ 9500/10672]\n",
      "loss: 1.675130  [ 9600/10672]\n",
      "loss: 1.207547  [ 9700/10672]\n",
      "loss: 1.162683  [ 9800/10672]\n",
      "loss: 1.101619  [ 9900/10672]\n",
      "loss: 1.224094  [10000/10672]\n",
      "loss: 1.096920  [10100/10672]\n",
      "loss: 0.984299  [10200/10672]\n",
      "loss: 1.134875  [10300/10672]\n",
      "loss: 1.190734  [10400/10672]\n",
      "loss: 1.180864  [10500/10672]\n",
      "loss: 1.669113  [10600/10672]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.033456  [    0/10672]\n",
      "loss: 1.102973  [  100/10672]\n",
      "loss: 0.935521  [  200/10672]\n",
      "loss: 0.969095  [  300/10672]\n",
      "loss: 1.259743  [  400/10672]\n",
      "loss: 1.107344  [  500/10672]\n",
      "loss: 1.666819  [  600/10672]\n",
      "loss: 1.011669  [  700/10672]\n",
      "loss: 1.666223  [  800/10672]\n",
      "loss: 1.456947  [  900/10672]\n",
      "loss: 1.666832  [ 1000/10672]\n",
      "loss: 1.279488  [ 1100/10672]\n",
      "loss: 1.143284  [ 1200/10672]\n",
      "loss: 1.133336  [ 1300/10672]\n",
      "loss: 1.029644  [ 1400/10672]\n",
      "loss: 1.042420  [ 1500/10672]\n",
      "loss: 1.063119  [ 1600/10672]\n",
      "loss: 1.023489  [ 1700/10672]\n",
      "loss: 1.239195  [ 1800/10672]\n",
      "loss: 1.085615  [ 1900/10672]\n",
      "loss: 1.156939  [ 2000/10672]\n",
      "loss: 1.153794  [ 2100/10672]\n",
      "loss: 1.113170  [ 2200/10672]\n",
      "loss: 1.238048  [ 2300/10672]\n",
      "loss: 1.134808  [ 2400/10672]\n",
      "loss: 1.112215  [ 2500/10672]\n",
      "loss: 1.127198  [ 2600/10672]\n",
      "loss: 1.665697  [ 2700/10672]\n",
      "loss: 1.102620  [ 2800/10672]\n",
      "loss: 1.667693  [ 2900/10672]\n",
      "loss: 1.668033  [ 3000/10672]\n",
      "loss: 1.219741  [ 3100/10672]\n",
      "loss: 1.027056  [ 3200/10672]\n",
      "loss: 1.043886  [ 3300/10672]\n",
      "loss: 1.674371  [ 3400/10672]\n",
      "loss: 1.669446  [ 3500/10672]\n",
      "loss: 1.669824  [ 3600/10672]\n",
      "loss: 1.111435  [ 3700/10672]\n",
      "loss: 1.081506  [ 3800/10672]\n",
      "loss: 1.214242  [ 3900/10672]\n",
      "loss: 1.029256  [ 4000/10672]\n",
      "loss: 1.666342  [ 4100/10672]\n",
      "loss: 1.668740  [ 4200/10672]\n",
      "loss: 1.141812  [ 4300/10672]\n",
      "loss: 1.665676  [ 4400/10672]\n",
      "loss: 1.099914  [ 4500/10672]\n",
      "loss: 1.287230  [ 4600/10672]\n",
      "loss: 1.090726  [ 4700/10672]\n",
      "loss: 1.669748  [ 4800/10672]\n",
      "loss: 1.165661  [ 4900/10672]\n",
      "loss: 1.665795  [ 5000/10672]\n",
      "loss: 1.219142  [ 5100/10672]\n",
      "loss: 1.085509  [ 5200/10672]\n",
      "loss: 1.421084  [ 5300/10672]\n",
      "loss: 1.119561  [ 5400/10672]\n",
      "loss: 0.969836  [ 5500/10672]\n",
      "loss: 1.138970  [ 5600/10672]\n",
      "loss: 1.670104  [ 5700/10672]\n",
      "loss: 1.065792  [ 5800/10672]\n",
      "loss: 1.208886  [ 5900/10672]\n",
      "loss: 1.667061  [ 6000/10672]\n",
      "loss: 1.186916  [ 6100/10672]\n",
      "loss: 1.069012  [ 6200/10672]\n",
      "loss: 1.047420  [ 6300/10672]\n",
      "loss: 1.176236  [ 6400/10672]\n",
      "loss: 1.674505  [ 6500/10672]\n",
      "loss: 1.070980  [ 6600/10672]\n",
      "loss: 1.203254  [ 6700/10672]\n",
      "loss: 1.094515  [ 6800/10672]\n",
      "loss: 1.115259  [ 6900/10672]\n",
      "loss: 1.148097  [ 7000/10672]\n",
      "loss: 0.976136  [ 7100/10672]\n",
      "loss: 1.198807  [ 7200/10672]\n",
      "loss: 1.268994  [ 7300/10672]\n",
      "loss: 1.154220  [ 7400/10672]\n",
      "loss: 0.992029  [ 7500/10672]\n",
      "loss: 1.202540  [ 7600/10672]\n",
      "loss: 1.224166  [ 7700/10672]\n",
      "loss: 1.243923  [ 7800/10672]\n",
      "loss: 1.109058  [ 7900/10672]\n",
      "loss: 1.075120  [ 8000/10672]\n",
      "loss: 1.120836  [ 8100/10672]\n",
      "loss: 1.090406  [ 8200/10672]\n",
      "loss: 1.098740  [ 8300/10672]\n",
      "loss: 1.199388  [ 8400/10672]\n",
      "loss: 1.035819  [ 8500/10672]\n",
      "loss: 1.203628  [ 8600/10672]\n",
      "loss: 1.205314  [ 8700/10672]\n",
      "loss: 1.078811  [ 8800/10672]\n",
      "loss: 1.022941  [ 8900/10672]\n",
      "loss: 1.666343  [ 9000/10672]\n",
      "loss: 1.037716  [ 9100/10672]\n",
      "loss: 1.042195  [ 9200/10672]\n",
      "loss: 1.206872  [ 9300/10672]\n",
      "loss: 1.201127  [ 9400/10672]\n",
      "loss: 1.133506  [ 9500/10672]\n",
      "loss: 1.675274  [ 9600/10672]\n",
      "loss: 1.207287  [ 9700/10672]\n",
      "loss: 1.163206  [ 9800/10672]\n",
      "loss: 1.101416  [ 9900/10672]\n",
      "loss: 1.225327  [10000/10672]\n",
      "loss: 1.096498  [10100/10672]\n",
      "loss: 0.982855  [10200/10672]\n",
      "loss: 1.133914  [10300/10672]\n",
      "loss: 1.190309  [10400/10672]\n",
      "loss: 1.180303  [10500/10672]\n",
      "loss: 1.669198  [10600/10672]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.032686  [    0/10672]\n",
      "loss: 1.102582  [  100/10672]\n",
      "loss: 0.933757  [  200/10672]\n",
      "loss: 0.967552  [  300/10672]\n",
      "loss: 1.259994  [  400/10672]\n",
      "loss: 1.106018  [  500/10672]\n",
      "loss: 1.666839  [  600/10672]\n",
      "loss: 1.010542  [  700/10672]\n",
      "loss: 1.666241  [  800/10672]\n",
      "loss: 1.459283  [  900/10672]\n",
      "loss: 1.666847  [ 1000/10672]\n",
      "loss: 1.279978  [ 1100/10672]\n",
      "loss: 1.142172  [ 1200/10672]\n",
      "loss: 1.133445  [ 1300/10672]\n",
      "loss: 1.027610  [ 1400/10672]\n",
      "loss: 1.041576  [ 1500/10672]\n",
      "loss: 1.061309  [ 1600/10672]\n",
      "loss: 1.022460  [ 1700/10672]\n",
      "loss: 1.239487  [ 1800/10672]\n",
      "loss: 1.084144  [ 1900/10672]\n",
      "loss: 1.156129  [ 2000/10672]\n",
      "loss: 1.153162  [ 2100/10672]\n",
      "loss: 1.112035  [ 2200/10672]\n",
      "loss: 1.238376  [ 2300/10672]\n",
      "loss: 1.134703  [ 2400/10672]\n",
      "loss: 1.111893  [ 2500/10672]\n",
      "loss: 1.127168  [ 2600/10672]\n",
      "loss: 1.665721  [ 2700/10672]\n",
      "loss: 1.102152  [ 2800/10672]\n",
      "loss: 1.667789  [ 2900/10672]\n",
      "loss: 1.668086  [ 3000/10672]\n",
      "loss: 1.220105  [ 3100/10672]\n",
      "loss: 1.025329  [ 3200/10672]\n",
      "loss: 1.042853  [ 3300/10672]\n",
      "loss: 1.674549  [ 3400/10672]\n",
      "loss: 1.669523  [ 3500/10672]\n",
      "loss: 1.669942  [ 3600/10672]\n",
      "loss: 1.110310  [ 3700/10672]\n",
      "loss: 1.080942  [ 3800/10672]\n",
      "loss: 1.214181  [ 3900/10672]\n",
      "loss: 1.027231  [ 4000/10672]\n",
      "loss: 1.666373  [ 4100/10672]\n",
      "loss: 1.668869  [ 4200/10672]\n",
      "loss: 1.141816  [ 4300/10672]\n",
      "loss: 1.665699  [ 4400/10672]\n",
      "loss: 1.098618  [ 4500/10672]\n",
      "loss: 1.288215  [ 4600/10672]\n",
      "loss: 1.089493  [ 4700/10672]\n",
      "loss: 1.669836  [ 4800/10672]\n",
      "loss: 1.166063  [ 4900/10672]\n",
      "loss: 1.665830  [ 5000/10672]\n",
      "loss: 1.220215  [ 5100/10672]\n",
      "loss: 1.084759  [ 5200/10672]\n",
      "loss: 1.423465  [ 5300/10672]\n",
      "loss: 1.119374  [ 5400/10672]\n",
      "loss: 0.967616  [ 5500/10672]\n",
      "loss: 1.138887  [ 5600/10672]\n",
      "loss: 1.670195  [ 5700/10672]\n",
      "loss: 1.064184  [ 5800/10672]\n",
      "loss: 1.209714  [ 5900/10672]\n",
      "loss: 1.667118  [ 6000/10672]\n",
      "loss: 1.186797  [ 6100/10672]\n",
      "loss: 1.068188  [ 6200/10672]\n",
      "loss: 1.045969  [ 6300/10672]\n",
      "loss: 1.176872  [ 6400/10672]\n",
      "loss: 1.674668  [ 6500/10672]\n",
      "loss: 1.070399  [ 6600/10672]\n",
      "loss: 1.203210  [ 6700/10672]\n",
      "loss: 1.094011  [ 6800/10672]\n",
      "loss: 1.114038  [ 6900/10672]\n",
      "loss: 1.148330  [ 7000/10672]\n",
      "loss: 0.974631  [ 7100/10672]\n",
      "loss: 1.198668  [ 7200/10672]\n",
      "loss: 1.270477  [ 7300/10672]\n",
      "loss: 1.153736  [ 7400/10672]\n",
      "loss: 0.990518  [ 7500/10672]\n",
      "loss: 1.202387  [ 7600/10672]\n",
      "loss: 1.224352  [ 7700/10672]\n",
      "loss: 1.244231  [ 7800/10672]\n",
      "loss: 1.108734  [ 7900/10672]\n",
      "loss: 1.074528  [ 8000/10672]\n",
      "loss: 1.120511  [ 8100/10672]\n",
      "loss: 1.089091  [ 8200/10672]\n",
      "loss: 1.097523  [ 8300/10672]\n",
      "loss: 1.200155  [ 8400/10672]\n",
      "loss: 1.034897  [ 8500/10672]\n",
      "loss: 1.204440  [ 8600/10672]\n",
      "loss: 1.205466  [ 8700/10672]\n",
      "loss: 1.077600  [ 8800/10672]\n",
      "loss: 1.021816  [ 8900/10672]\n",
      "loss: 1.666383  [ 9000/10672]\n",
      "loss: 1.035710  [ 9100/10672]\n",
      "loss: 1.041156  [ 9200/10672]\n",
      "loss: 1.207799  [ 9300/10672]\n",
      "loss: 1.202024  [ 9400/10672]\n",
      "loss: 1.132513  [ 9500/10672]\n",
      "loss: 1.675418  [ 9600/10672]\n",
      "loss: 1.207027  [ 9700/10672]\n",
      "loss: 1.163726  [ 9800/10672]\n",
      "loss: 1.101214  [ 9900/10672]\n",
      "loss: 1.226551  [10000/10672]\n",
      "loss: 1.096075  [10100/10672]\n",
      "loss: 0.981422  [10200/10672]\n",
      "loss: 1.132963  [10300/10672]\n",
      "loss: 1.189890  [10400/10672]\n",
      "loss: 1.179747  [10500/10672]\n",
      "loss: 1.669282  [10600/10672]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.031925  [    0/10672]\n",
      "loss: 1.102188  [  100/10672]\n",
      "loss: 0.932014  [  200/10672]\n",
      "loss: 0.966025  [  300/10672]\n",
      "loss: 1.260241  [  400/10672]\n",
      "loss: 1.104703  [  500/10672]\n",
      "loss: 1.666858  [  600/10672]\n",
      "loss: 1.009422  [  700/10672]\n",
      "loss: 1.666259  [  800/10672]\n",
      "loss: 1.461595  [  900/10672]\n",
      "loss: 1.666861  [ 1000/10672]\n",
      "loss: 1.280464  [ 1100/10672]\n",
      "loss: 1.141063  [ 1200/10672]\n",
      "loss: 1.133551  [ 1300/10672]\n",
      "loss: 1.025599  [ 1400/10672]\n",
      "loss: 1.040737  [ 1500/10672]\n",
      "loss: 1.059516  [ 1600/10672]\n",
      "loss: 1.021436  [ 1700/10672]\n",
      "loss: 1.239779  [ 1800/10672]\n",
      "loss: 1.082686  [ 1900/10672]\n",
      "loss: 1.155323  [ 2000/10672]\n",
      "loss: 1.152539  [ 2100/10672]\n",
      "loss: 1.110911  [ 2200/10672]\n",
      "loss: 1.238703  [ 2300/10672]\n",
      "loss: 1.134594  [ 2400/10672]\n",
      "loss: 1.111571  [ 2500/10672]\n",
      "loss: 1.127139  [ 2600/10672]\n",
      "loss: 1.665744  [ 2700/10672]\n",
      "loss: 1.101683  [ 2800/10672]\n",
      "loss: 1.667885  [ 2900/10672]\n",
      "loss: 1.668138  [ 3000/10672]\n",
      "loss: 1.220471  [ 3100/10672]\n",
      "loss: 1.023621  [ 3200/10672]\n",
      "loss: 1.041825  [ 3300/10672]\n",
      "loss: 1.674725  [ 3400/10672]\n",
      "loss: 1.669601  [ 3500/10672]\n",
      "loss: 1.670060  [ 3600/10672]\n",
      "loss: 1.109194  [ 3700/10672]\n",
      "loss: 1.080380  [ 3800/10672]\n",
      "loss: 1.214119  [ 3900/10672]\n",
      "loss: 1.025225  [ 4000/10672]\n",
      "loss: 1.666405  [ 4100/10672]\n",
      "loss: 1.668998  [ 4200/10672]\n",
      "loss: 1.141817  [ 4300/10672]\n",
      "loss: 1.665721  [ 4400/10672]\n",
      "loss: 1.097330  [ 4500/10672]\n",
      "loss: 1.289196  [ 4600/10672]\n",
      "loss: 1.088272  [ 4700/10672]\n",
      "loss: 1.669924  [ 4800/10672]\n",
      "loss: 1.166463  [ 4900/10672]\n",
      "loss: 1.665864  [ 5000/10672]\n",
      "loss: 1.221284  [ 5100/10672]\n",
      "loss: 1.084009  [ 5200/10672]\n",
      "loss: 1.425825  [ 5300/10672]\n",
      "loss: 1.119184  [ 5400/10672]\n",
      "loss: 0.965426  [ 5500/10672]\n",
      "loss: 1.138799  [ 5600/10672]\n",
      "loss: 1.670286  [ 5700/10672]\n",
      "loss: 1.062587  [ 5800/10672]\n",
      "loss: 1.210535  [ 5900/10672]\n",
      "loss: 1.667175  [ 6000/10672]\n",
      "loss: 1.186679  [ 6100/10672]\n",
      "loss: 1.067370  [ 6200/10672]\n",
      "loss: 1.044535  [ 6300/10672]\n",
      "loss: 1.177508  [ 6400/10672]\n",
      "loss: 1.674831  [ 6500/10672]\n",
      "loss: 1.069823  [ 6600/10672]\n",
      "loss: 1.203166  [ 6700/10672]\n",
      "loss: 1.093508  [ 6800/10672]\n",
      "loss: 1.112821  [ 6900/10672]\n",
      "loss: 1.148565  [ 7000/10672]\n",
      "loss: 0.973142  [ 7100/10672]\n",
      "loss: 1.198528  [ 7200/10672]\n",
      "loss: 1.271950  [ 7300/10672]\n",
      "loss: 1.153254  [ 7400/10672]\n",
      "loss: 0.989023  [ 7500/10672]\n",
      "loss: 1.202231  [ 7600/10672]\n",
      "loss: 1.224534  [ 7700/10672]\n",
      "loss: 1.244531  [ 7800/10672]\n",
      "loss: 1.108412  [ 7900/10672]\n",
      "loss: 1.073941  [ 8000/10672]\n",
      "loss: 1.120186  [ 8100/10672]\n",
      "loss: 1.087786  [ 8200/10672]\n",
      "loss: 1.096314  [ 8300/10672]\n",
      "loss: 1.200921  [ 8400/10672]\n",
      "loss: 1.033984  [ 8500/10672]\n",
      "loss: 1.205250  [ 8600/10672]\n",
      "loss: 1.205621  [ 8700/10672]\n",
      "loss: 1.076401  [ 8800/10672]\n",
      "loss: 1.020700  [ 8900/10672]\n",
      "loss: 1.666422  [ 9000/10672]\n",
      "loss: 1.033719  [ 9100/10672]\n",
      "loss: 1.040125  [ 9200/10672]\n",
      "loss: 1.208722  [ 9300/10672]\n",
      "loss: 1.202917  [ 9400/10672]\n",
      "loss: 1.131527  [ 9500/10672]\n",
      "loss: 1.675561  [ 9600/10672]\n",
      "loss: 1.206765  [ 9700/10672]\n",
      "loss: 1.164246  [ 9800/10672]\n",
      "loss: 1.101015  [ 9900/10672]\n",
      "loss: 1.227768  [10000/10672]\n",
      "loss: 1.095651  [10100/10672]\n",
      "loss: 0.980004  [10200/10672]\n",
      "loss: 1.132019  [10300/10672]\n",
      "loss: 1.189473  [10400/10672]\n",
      "loss: 1.179194  [10500/10672]\n",
      "loss: 1.669365  [10600/10672]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.031173  [    0/10672]\n",
      "loss: 1.101791  [  100/10672]\n",
      "loss: 0.930293  [  200/10672]\n",
      "loss: 0.964511  [  300/10672]\n",
      "loss: 1.260485  [  400/10672]\n",
      "loss: 1.103398  [  500/10672]\n",
      "loss: 1.666876  [  600/10672]\n",
      "loss: 1.008309  [  700/10672]\n",
      "loss: 1.666276  [  800/10672]\n",
      "loss: 1.463881  [  900/10672]\n",
      "loss: 1.666875  [ 1000/10672]\n",
      "loss: 1.280941  [ 1100/10672]\n",
      "loss: 1.139958  [ 1200/10672]\n",
      "loss: 1.133653  [ 1300/10672]\n",
      "loss: 1.023609  [ 1400/10672]\n",
      "loss: 1.039905  [ 1500/10672]\n",
      "loss: 1.057737  [ 1600/10672]\n",
      "loss: 1.020419  [ 1700/10672]\n",
      "loss: 1.240070  [ 1800/10672]\n",
      "loss: 1.081240  [ 1900/10672]\n",
      "loss: 1.154519  [ 2000/10672]\n",
      "loss: 1.151924  [ 2100/10672]\n",
      "loss: 1.109797  [ 2200/10672]\n",
      "loss: 1.239028  [ 2300/10672]\n",
      "loss: 1.134483  [ 2400/10672]\n",
      "loss: 1.111249  [ 2500/10672]\n",
      "loss: 1.127112  [ 2600/10672]\n",
      "loss: 1.665766  [ 2700/10672]\n",
      "loss: 1.101214  [ 2800/10672]\n",
      "loss: 1.667981  [ 2900/10672]\n",
      "loss: 1.668189  [ 3000/10672]\n",
      "loss: 1.220839  [ 3100/10672]\n",
      "loss: 1.021935  [ 3200/10672]\n",
      "loss: 1.040804  [ 3300/10672]\n",
      "loss: 1.674902  [ 3400/10672]\n",
      "loss: 1.669678  [ 3500/10672]\n",
      "loss: 1.670178  [ 3600/10672]\n",
      "loss: 1.108086  [ 3700/10672]\n",
      "loss: 1.079818  [ 3800/10672]\n",
      "loss: 1.214058  [ 3900/10672]\n",
      "loss: 1.023239  [ 4000/10672]\n",
      "loss: 1.666435  [ 4100/10672]\n",
      "loss: 1.669127  [ 4200/10672]\n",
      "loss: 1.141813  [ 4300/10672]\n",
      "loss: 1.665742  [ 4400/10672]\n",
      "loss: 1.096053  [ 4500/10672]\n",
      "loss: 1.290173  [ 4600/10672]\n",
      "loss: 1.087064  [ 4700/10672]\n",
      "loss: 1.670011  [ 4800/10672]\n",
      "loss: 1.166860  [ 4900/10672]\n",
      "loss: 1.665897  [ 5000/10672]\n",
      "loss: 1.222347  [ 5100/10672]\n",
      "loss: 1.083259  [ 5200/10672]\n",
      "loss: 1.428166  [ 5300/10672]\n",
      "loss: 1.118993  [ 5400/10672]\n",
      "loss: 0.963265  [ 5500/10672]\n",
      "loss: 1.138705  [ 5600/10672]\n",
      "loss: 1.670377  [ 5700/10672]\n",
      "loss: 1.061002  [ 5800/10672]\n",
      "loss: 1.211350  [ 5900/10672]\n",
      "loss: 1.667231  [ 6000/10672]\n",
      "loss: 1.186566  [ 6100/10672]\n",
      "loss: 1.066557  [ 6200/10672]\n",
      "loss: 1.043121  [ 6300/10672]\n",
      "loss: 1.178140  [ 6400/10672]\n",
      "loss: 1.674994  [ 6500/10672]\n",
      "loss: 1.069252  [ 6600/10672]\n",
      "loss: 1.203126  [ 6700/10672]\n",
      "loss: 1.093008  [ 6800/10672]\n",
      "loss: 1.111609  [ 6900/10672]\n",
      "loss: 1.148799  [ 7000/10672]\n",
      "loss: 0.971669  [ 7100/10672]\n",
      "loss: 1.198389  [ 7200/10672]\n",
      "loss: 1.273410  [ 7300/10672]\n",
      "loss: 1.152779  [ 7400/10672]\n",
      "loss: 0.987542  [ 7500/10672]\n",
      "loss: 1.202077  [ 7600/10672]\n",
      "loss: 1.224716  [ 7700/10672]\n",
      "loss: 1.244829  [ 7800/10672]\n",
      "loss: 1.108087  [ 7900/10672]\n",
      "loss: 1.073355  [ 8000/10672]\n",
      "loss: 1.119855  [ 8100/10672]\n",
      "loss: 1.086496  [ 8200/10672]\n",
      "loss: 1.095118  [ 8300/10672]\n",
      "loss: 1.201679  [ 8400/10672]\n",
      "loss: 1.033078  [ 8500/10672]\n",
      "loss: 1.206054  [ 8600/10672]\n",
      "loss: 1.205784  [ 8700/10672]\n",
      "loss: 1.075218  [ 8800/10672]\n",
      "loss: 1.019591  [ 8900/10672]\n",
      "loss: 1.666462  [ 9000/10672]\n",
      "loss: 1.031747  [ 9100/10672]\n",
      "loss: 1.039098  [ 9200/10672]\n",
      "loss: 1.209636  [ 9300/10672]\n",
      "loss: 1.203802  [ 9400/10672]\n",
      "loss: 1.130552  [ 9500/10672]\n",
      "loss: 1.675704  [ 9600/10672]\n",
      "loss: 1.206504  [ 9700/10672]\n",
      "loss: 1.164761  [ 9800/10672]\n",
      "loss: 1.100814  [ 9900/10672]\n",
      "loss: 1.228974  [10000/10672]\n",
      "loss: 1.095223  [10100/10672]\n",
      "loss: 0.978596  [10200/10672]\n",
      "loss: 1.131086  [10300/10672]\n",
      "loss: 1.189064  [10400/10672]\n",
      "loss: 1.178647  [10500/10672]\n",
      "loss: 1.669448  [10600/10672]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.030427  [    0/10672]\n",
      "loss: 1.101387  [  100/10672]\n",
      "loss: 0.928591  [  200/10672]\n",
      "loss: 0.963011  [  300/10672]\n",
      "loss: 1.260728  [  400/10672]\n",
      "loss: 1.102108  [  500/10672]\n",
      "loss: 1.666894  [  600/10672]\n",
      "loss: 1.007201  [  700/10672]\n",
      "loss: 1.666293  [  800/10672]\n",
      "loss: 1.466146  [  900/10672]\n",
      "loss: 1.666888  [ 1000/10672]\n",
      "loss: 1.281418  [ 1100/10672]\n",
      "loss: 1.138860  [ 1200/10672]\n",
      "loss: 1.133748  [ 1300/10672]\n",
      "loss: 1.021644  [ 1400/10672]\n",
      "loss: 1.039075  [ 1500/10672]\n",
      "loss: 1.055977  [ 1600/10672]\n",
      "loss: 1.019405  [ 1700/10672]\n",
      "loss: 1.240365  [ 1800/10672]\n",
      "loss: 1.079810  [ 1900/10672]\n",
      "loss: 1.153723  [ 2000/10672]\n",
      "loss: 1.151320  [ 2100/10672]\n",
      "loss: 1.108696  [ 2200/10672]\n",
      "loss: 1.239356  [ 2300/10672]\n",
      "loss: 1.134363  [ 2400/10672]\n",
      "loss: 1.110922  [ 2500/10672]\n",
      "loss: 1.127082  [ 2600/10672]\n",
      "loss: 1.665788  [ 2700/10672]\n",
      "loss: 1.100741  [ 2800/10672]\n",
      "loss: 1.668076  [ 2900/10672]\n",
      "loss: 1.668241  [ 3000/10672]\n",
      "loss: 1.221213  [ 3100/10672]\n",
      "loss: 1.020271  [ 3200/10672]\n",
      "loss: 1.039785  [ 3300/10672]\n",
      "loss: 1.675079  [ 3400/10672]\n",
      "loss: 1.669756  [ 3500/10672]\n",
      "loss: 1.670295  [ 3600/10672]\n",
      "loss: 1.106989  [ 3700/10672]\n",
      "loss: 1.079257  [ 3800/10672]\n",
      "loss: 1.213999  [ 3900/10672]\n",
      "loss: 1.021274  [ 4000/10672]\n",
      "loss: 1.666466  [ 4100/10672]\n",
      "loss: 1.669256  [ 4200/10672]\n",
      "loss: 1.141802  [ 4300/10672]\n",
      "loss: 1.665763  [ 4400/10672]\n",
      "loss: 1.094786  [ 4500/10672]\n",
      "loss: 1.291149  [ 4600/10672]\n",
      "loss: 1.085871  [ 4700/10672]\n",
      "loss: 1.670099  [ 4800/10672]\n",
      "loss: 1.167251  [ 4900/10672]\n",
      "loss: 1.665929  [ 5000/10672]\n",
      "loss: 1.223403  [ 5100/10672]\n",
      "loss: 1.082505  [ 5200/10672]\n",
      "loss: 1.430489  [ 5300/10672]\n",
      "loss: 1.118797  [ 5400/10672]\n",
      "loss: 0.961135  [ 5500/10672]\n",
      "loss: 1.138605  [ 5600/10672]\n",
      "loss: 1.670467  [ 5700/10672]\n",
      "loss: 1.059431  [ 5800/10672]\n",
      "loss: 1.212157  [ 5900/10672]\n",
      "loss: 1.667287  [ 6000/10672]\n",
      "loss: 1.186458  [ 6100/10672]\n",
      "loss: 1.065746  [ 6200/10672]\n",
      "loss: 1.041726  [ 6300/10672]\n",
      "loss: 1.178769  [ 6400/10672]\n",
      "loss: 1.675156  [ 6500/10672]\n",
      "loss: 1.068684  [ 6600/10672]\n",
      "loss: 1.203091  [ 6700/10672]\n",
      "loss: 1.092506  [ 6800/10672]\n",
      "loss: 1.110404  [ 6900/10672]\n",
      "loss: 1.149030  [ 7000/10672]\n",
      "loss: 0.970210  [ 7100/10672]\n",
      "loss: 1.198253  [ 7200/10672]\n",
      "loss: 1.274857  [ 7300/10672]\n",
      "loss: 1.152309  [ 7400/10672]\n",
      "loss: 0.986073  [ 7500/10672]\n",
      "loss: 1.201925  [ 7600/10672]\n",
      "loss: 1.224898  [ 7700/10672]\n",
      "loss: 1.245122  [ 7800/10672]\n",
      "loss: 1.107760  [ 7900/10672]\n",
      "loss: 1.072771  [ 8000/10672]\n",
      "loss: 1.119521  [ 8100/10672]\n",
      "loss: 1.085220  [ 8200/10672]\n",
      "loss: 1.093935  [ 8300/10672]\n",
      "loss: 1.202431  [ 8400/10672]\n",
      "loss: 1.032178  [ 8500/10672]\n",
      "loss: 1.206852  [ 8600/10672]\n",
      "loss: 1.205954  [ 8700/10672]\n",
      "loss: 1.074048  [ 8800/10672]\n",
      "loss: 1.018492  [ 8900/10672]\n",
      "loss: 1.666500  [ 9000/10672]\n",
      "loss: 1.029792  [ 9100/10672]\n",
      "loss: 1.038078  [ 9200/10672]\n",
      "loss: 1.210545  [ 9300/10672]\n",
      "loss: 1.204682  [ 9400/10672]\n",
      "loss: 1.129584  [ 9500/10672]\n",
      "loss: 1.675847  [ 9600/10672]\n",
      "loss: 1.206240  [ 9700/10672]\n",
      "loss: 1.165276  [ 9800/10672]\n",
      "loss: 1.100616  [ 9900/10672]\n",
      "loss: 1.230175  [10000/10672]\n",
      "loss: 1.094796  [10100/10672]\n",
      "loss: 0.977202  [10200/10672]\n",
      "loss: 1.130160  [10300/10672]\n",
      "loss: 1.188657  [10400/10672]\n",
      "loss: 1.178104  [10500/10672]\n",
      "loss: 1.669530  [10600/10672]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.029691  [    0/10672]\n",
      "loss: 1.100982  [  100/10672]\n",
      "loss: 0.926911  [  200/10672]\n",
      "loss: 0.961526  [  300/10672]\n",
      "loss: 1.260966  [  400/10672]\n",
      "loss: 1.100829  [  500/10672]\n",
      "loss: 1.666912  [  600/10672]\n",
      "loss: 1.006101  [  700/10672]\n",
      "loss: 1.666309  [  800/10672]\n",
      "loss: 1.468385  [  900/10672]\n",
      "loss: 1.666901  [ 1000/10672]\n",
      "loss: 1.281886  [ 1100/10672]\n",
      "loss: 1.137763  [ 1200/10672]\n",
      "loss: 1.133842  [ 1300/10672]\n",
      "loss: 1.019696  [ 1400/10672]\n",
      "loss: 1.038254  [ 1500/10672]\n",
      "loss: 1.054228  [ 1600/10672]\n",
      "loss: 1.018400  [ 1700/10672]\n",
      "loss: 1.240654  [ 1800/10672]\n",
      "loss: 1.078388  [ 1900/10672]\n",
      "loss: 1.152924  [ 2000/10672]\n",
      "loss: 1.150720  [ 2100/10672]\n",
      "loss: 1.107601  [ 2200/10672]\n",
      "loss: 1.239677  [ 2300/10672]\n",
      "loss: 1.134246  [ 2400/10672]\n",
      "loss: 1.110599  [ 2500/10672]\n",
      "loss: 1.127058  [ 2600/10672]\n",
      "loss: 1.665810  [ 2700/10672]\n",
      "loss: 1.100274  [ 2800/10672]\n",
      "loss: 1.668170  [ 2900/10672]\n",
      "loss: 1.668291  [ 3000/10672]\n",
      "loss: 1.221583  [ 3100/10672]\n",
      "loss: 1.018624  [ 3200/10672]\n",
      "loss: 1.038776  [ 3300/10672]\n",
      "loss: 1.675255  [ 3400/10672]\n",
      "loss: 1.669833  [ 3500/10672]\n",
      "loss: 1.670412  [ 3600/10672]\n",
      "loss: 1.105896  [ 3700/10672]\n",
      "loss: 1.078699  [ 3800/10672]\n",
      "loss: 1.213937  [ 3900/10672]\n",
      "loss: 1.019328  [ 4000/10672]\n",
      "loss: 1.666495  [ 4100/10672]\n",
      "loss: 1.669386  [ 4200/10672]\n",
      "loss: 1.141789  [ 4300/10672]\n",
      "loss: 1.665783  [ 4400/10672]\n",
      "loss: 1.093526  [ 4500/10672]\n",
      "loss: 1.292117  [ 4600/10672]\n",
      "loss: 1.084688  [ 4700/10672]\n",
      "loss: 1.670186  [ 4800/10672]\n",
      "loss: 1.167643  [ 4900/10672]\n",
      "loss: 1.665960  [ 5000/10672]\n",
      "loss: 1.224457  [ 5100/10672]\n",
      "loss: 1.081755  [ 5200/10672]\n",
      "loss: 1.432791  [ 5300/10672]\n",
      "loss: 1.118601  [ 5400/10672]\n",
      "loss: 0.959032  [ 5500/10672]\n",
      "loss: 1.138502  [ 5600/10672]\n",
      "loss: 1.670556  [ 5700/10672]\n",
      "loss: 1.057870  [ 5800/10672]\n",
      "loss: 1.212960  [ 5900/10672]\n",
      "loss: 1.667342  [ 6000/10672]\n",
      "loss: 1.186352  [ 6100/10672]\n",
      "loss: 1.064941  [ 6200/10672]\n",
      "loss: 1.040348  [ 6300/10672]\n",
      "loss: 1.179396  [ 6400/10672]\n",
      "loss: 1.675319  [ 6500/10672]\n",
      "loss: 1.068120  [ 6600/10672]\n",
      "loss: 1.203058  [ 6700/10672]\n",
      "loss: 1.092006  [ 6800/10672]\n",
      "loss: 1.109205  [ 6900/10672]\n",
      "loss: 1.149261  [ 7000/10672]\n",
      "loss: 0.968767  [ 7100/10672]\n",
      "loss: 1.198117  [ 7200/10672]\n",
      "loss: 1.276293  [ 7300/10672]\n",
      "loss: 1.151844  [ 7400/10672]\n",
      "loss: 0.984619  [ 7500/10672]\n",
      "loss: 1.201772  [ 7600/10672]\n",
      "loss: 1.225077  [ 7700/10672]\n",
      "loss: 1.245412  [ 7800/10672]\n",
      "loss: 1.107431  [ 7900/10672]\n",
      "loss: 1.072188  [ 8000/10672]\n",
      "loss: 1.119184  [ 8100/10672]\n",
      "loss: 1.083958  [ 8200/10672]\n",
      "loss: 1.092764  [ 8300/10672]\n",
      "loss: 1.203178  [ 8400/10672]\n",
      "loss: 1.031285  [ 8500/10672]\n",
      "loss: 1.207645  [ 8600/10672]\n",
      "loss: 1.206129  [ 8700/10672]\n",
      "loss: 1.072893  [ 8800/10672]\n",
      "loss: 1.017399  [ 8900/10672]\n",
      "loss: 1.666539  [ 9000/10672]\n",
      "loss: 1.027854  [ 9100/10672]\n",
      "loss: 1.037063  [ 9200/10672]\n",
      "loss: 1.211447  [ 9300/10672]\n",
      "loss: 1.205554  [ 9400/10672]\n",
      "loss: 1.128628  [ 9500/10672]\n",
      "loss: 1.675990  [ 9600/10672]\n",
      "loss: 1.205979  [ 9700/10672]\n",
      "loss: 1.165786  [ 9800/10672]\n",
      "loss: 1.100418  [ 9900/10672]\n",
      "loss: 1.231365  [10000/10672]\n",
      "loss: 1.094365  [10100/10672]\n",
      "loss: 0.975820  [10200/10672]\n",
      "loss: 1.129243  [10300/10672]\n",
      "loss: 1.188256  [10400/10672]\n",
      "loss: 1.177565  [10500/10672]\n",
      "loss: 1.669612  [10600/10672]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.028962  [    0/10672]\n",
      "loss: 1.100573  [  100/10672]\n",
      "loss: 0.925251  [  200/10672]\n",
      "loss: 0.960055  [  300/10672]\n",
      "loss: 1.261202  [  400/10672]\n",
      "loss: 1.099561  [  500/10672]\n",
      "loss: 1.666929  [  600/10672]\n",
      "loss: 1.005006  [  700/10672]\n",
      "loss: 1.666324  [  800/10672]\n",
      "loss: 1.470600  [  900/10672]\n",
      "loss: 1.666914  [ 1000/10672]\n",
      "loss: 1.282349  [ 1100/10672]\n",
      "loss: 1.136671  [ 1200/10672]\n",
      "loss: 1.133932  [ 1300/10672]\n",
      "loss: 1.017771  [ 1400/10672]\n",
      "loss: 1.037438  [ 1500/10672]\n",
      "loss: 1.052497  [ 1600/10672]\n",
      "loss: 1.017400  [ 1700/10672]\n",
      "loss: 1.240947  [ 1800/10672]\n",
      "loss: 1.076982  [ 1900/10672]\n",
      "loss: 1.152134  [ 2000/10672]\n",
      "loss: 1.150131  [ 2100/10672]\n",
      "loss: 1.106520  [ 2200/10672]\n",
      "loss: 1.240001  [ 2300/10672]\n",
      "loss: 1.134122  [ 2400/10672]\n",
      "loss: 1.110272  [ 2500/10672]\n",
      "loss: 1.127031  [ 2600/10672]\n",
      "loss: 1.665831  [ 2700/10672]\n",
      "loss: 1.099803  [ 2800/10672]\n",
      "loss: 1.668263  [ 2900/10672]\n",
      "loss: 1.668340  [ 3000/10672]\n",
      "loss: 1.221960  [ 3100/10672]\n",
      "loss: 1.016998  [ 3200/10672]\n",
      "loss: 1.037771  [ 3300/10672]\n",
      "loss: 1.675431  [ 3400/10672]\n",
      "loss: 1.669909  [ 3500/10672]\n",
      "loss: 1.670528  [ 3600/10672]\n",
      "loss: 1.104813  [ 3700/10672]\n",
      "loss: 1.078142  [ 3800/10672]\n",
      "loss: 1.213876  [ 3900/10672]\n",
      "loss: 1.017401  [ 4000/10672]\n",
      "loss: 1.666524  [ 4100/10672]\n",
      "loss: 1.669515  [ 4200/10672]\n",
      "loss: 1.141769  [ 4300/10672]\n",
      "loss: 1.665802  [ 4400/10672]\n",
      "loss: 1.092276  [ 4500/10672]\n",
      "loss: 1.293085  [ 4600/10672]\n",
      "loss: 1.083518  [ 4700/10672]\n",
      "loss: 1.670273  [ 4800/10672]\n",
      "loss: 1.168029  [ 4900/10672]\n",
      "loss: 1.665991  [ 5000/10672]\n",
      "loss: 1.225503  [ 5100/10672]\n",
      "loss: 1.081002  [ 5200/10672]\n",
      "loss: 1.435075  [ 5300/10672]\n",
      "loss: 1.118401  [ 5400/10672]\n",
      "loss: 0.956960  [ 5500/10672]\n",
      "loss: 1.138391  [ 5600/10672]\n",
      "loss: 1.670645  [ 5700/10672]\n",
      "loss: 1.056323  [ 5800/10672]\n",
      "loss: 1.213753  [ 5900/10672]\n",
      "loss: 1.667397  [ 6000/10672]\n",
      "loss: 1.186252  [ 6100/10672]\n",
      "loss: 1.064138  [ 6200/10672]\n",
      "loss: 1.038989  [ 6300/10672]\n",
      "loss: 1.180019  [ 6400/10672]\n",
      "loss: 1.675481  [ 6500/10672]\n",
      "loss: 1.067559  [ 6600/10672]\n",
      "loss: 1.203029  [ 6700/10672]\n",
      "loss: 1.091507  [ 6800/10672]\n",
      "loss: 1.108012  [ 6900/10672]\n",
      "loss: 1.149491  [ 7000/10672]\n",
      "loss: 0.967337  [ 7100/10672]\n",
      "loss: 1.197984  [ 7200/10672]\n",
      "loss: 1.277715  [ 7300/10672]\n",
      "loss: 1.151385  [ 7400/10672]\n",
      "loss: 0.983178  [ 7500/10672]\n",
      "loss: 1.201621  [ 7600/10672]\n",
      "loss: 1.225256  [ 7700/10672]\n",
      "loss: 1.245696  [ 7800/10672]\n",
      "loss: 1.107101  [ 7900/10672]\n",
      "loss: 1.071608  [ 8000/10672]\n",
      "loss: 1.118843  [ 8100/10672]\n",
      "loss: 1.082708  [ 8200/10672]\n",
      "loss: 1.091604  [ 8300/10672]\n",
      "loss: 1.203918  [ 8400/10672]\n",
      "loss: 1.030397  [ 8500/10672]\n",
      "loss: 1.208432  [ 8600/10672]\n",
      "loss: 1.206312  [ 8700/10672]\n",
      "loss: 1.071751  [ 8800/10672]\n",
      "loss: 1.016315  [ 8900/10672]\n",
      "loss: 1.666577  [ 9000/10672]\n",
      "loss: 1.025933  [ 9100/10672]\n",
      "loss: 1.036053  [ 9200/10672]\n",
      "loss: 1.212343  [ 9300/10672]\n",
      "loss: 1.206420  [ 9400/10672]\n",
      "loss: 1.127680  [ 9500/10672]\n",
      "loss: 1.676132  [ 9600/10672]\n",
      "loss: 1.205716  [ 9700/10672]\n",
      "loss: 1.166294  [ 9800/10672]\n",
      "loss: 1.100221  [ 9900/10672]\n",
      "loss: 1.232546  [10000/10672]\n",
      "loss: 1.093932  [10100/10672]\n",
      "loss: 0.974449  [10200/10672]\n",
      "loss: 1.128336  [10300/10672]\n",
      "loss: 1.187860  [10400/10672]\n",
      "loss: 1.177031  [10500/10672]\n",
      "loss: 1.669693  [10600/10672]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.028240  [    0/10672]\n",
      "loss: 1.100160  [  100/10672]\n",
      "loss: 0.923611  [  200/10672]\n",
      "loss: 0.958596  [  300/10672]\n",
      "loss: 1.261435  [  400/10672]\n",
      "loss: 1.098305  [  500/10672]\n",
      "loss: 1.666946  [  600/10672]\n",
      "loss: 1.003917  [  700/10672]\n",
      "loss: 1.666339  [  800/10672]\n",
      "loss: 1.472793  [  900/10672]\n",
      "loss: 1.666926  [ 1000/10672]\n",
      "loss: 1.282809  [ 1100/10672]\n",
      "loss: 1.135585  [ 1200/10672]\n",
      "loss: 1.134017  [ 1300/10672]\n",
      "loss: 1.015869  [ 1400/10672]\n",
      "loss: 1.036625  [ 1500/10672]\n",
      "loss: 1.050781  [ 1600/10672]\n",
      "loss: 1.016405  [ 1700/10672]\n",
      "loss: 1.241239  [ 1800/10672]\n",
      "loss: 1.075589  [ 1900/10672]\n",
      "loss: 1.151347  [ 2000/10672]\n",
      "loss: 1.149549  [ 2100/10672]\n",
      "loss: 1.105448  [ 2200/10672]\n",
      "loss: 1.240323  [ 2300/10672]\n",
      "loss: 1.133996  [ 2400/10672]\n",
      "loss: 1.109945  [ 2500/10672]\n",
      "loss: 1.127007  [ 2600/10672]\n",
      "loss: 1.665851  [ 2700/10672]\n",
      "loss: 1.099333  [ 2800/10672]\n",
      "loss: 1.668357  [ 2900/10672]\n",
      "loss: 1.668389  [ 3000/10672]\n",
      "loss: 1.222337  [ 3100/10672]\n",
      "loss: 1.015391  [ 3200/10672]\n",
      "loss: 1.036772  [ 3300/10672]\n",
      "loss: 1.675607  [ 3400/10672]\n",
      "loss: 1.669986  [ 3500/10672]\n",
      "loss: 1.670644  [ 3600/10672]\n",
      "loss: 1.103739  [ 3700/10672]\n",
      "loss: 1.077586  [ 3800/10672]\n",
      "loss: 1.213816  [ 3900/10672]\n",
      "loss: 1.015495  [ 4000/10672]\n",
      "loss: 1.666553  [ 4100/10672]\n",
      "loss: 1.669645  [ 4200/10672]\n",
      "loss: 1.141744  [ 4300/10672]\n",
      "loss: 1.665821  [ 4400/10672]\n",
      "loss: 1.091035  [ 4500/10672]\n",
      "loss: 1.294049  [ 4600/10672]\n",
      "loss: 1.082362  [ 4700/10672]\n",
      "loss: 1.670359  [ 4800/10672]\n",
      "loss: 1.168412  [ 4900/10672]\n",
      "loss: 1.666021  [ 5000/10672]\n",
      "loss: 1.226544  [ 5100/10672]\n",
      "loss: 1.080248  [ 5200/10672]\n",
      "loss: 1.437340  [ 5300/10672]\n",
      "loss: 1.118198  [ 5400/10672]\n",
      "loss: 0.954916  [ 5500/10672]\n",
      "loss: 1.138276  [ 5600/10672]\n",
      "loss: 1.670735  [ 5700/10672]\n",
      "loss: 1.054788  [ 5800/10672]\n",
      "loss: 1.214541  [ 5900/10672]\n",
      "loss: 1.667452  [ 6000/10672]\n",
      "loss: 1.186154  [ 6100/10672]\n",
      "loss: 1.063341  [ 6200/10672]\n",
      "loss: 1.037647  [ 6300/10672]\n",
      "loss: 1.180641  [ 6400/10672]\n",
      "loss: 1.675643  [ 6500/10672]\n",
      "loss: 1.067003  [ 6600/10672]\n",
      "loss: 1.203001  [ 6700/10672]\n",
      "loss: 1.091011  [ 6800/10672]\n",
      "loss: 1.106823  [ 6900/10672]\n",
      "loss: 1.149722  [ 7000/10672]\n",
      "loss: 0.965924  [ 7100/10672]\n",
      "loss: 1.197849  [ 7200/10672]\n",
      "loss: 1.279129  [ 7300/10672]\n",
      "loss: 1.150928  [ 7400/10672]\n",
      "loss: 0.981753  [ 7500/10672]\n",
      "loss: 1.201468  [ 7600/10672]\n",
      "loss: 1.225431  [ 7700/10672]\n",
      "loss: 1.245974  [ 7800/10672]\n",
      "loss: 1.106771  [ 7900/10672]\n",
      "loss: 1.071031  [ 8000/10672]\n",
      "loss: 1.118502  [ 8100/10672]\n",
      "loss: 1.081470  [ 8200/10672]\n",
      "loss: 1.090454  [ 8300/10672]\n",
      "loss: 1.204656  [ 8400/10672]\n",
      "loss: 1.029519  [ 8500/10672]\n",
      "loss: 1.209215  [ 8600/10672]\n",
      "loss: 1.206499  [ 8700/10672]\n",
      "loss: 1.070621  [ 8800/10672]\n",
      "loss: 1.015239  [ 8900/10672]\n",
      "loss: 1.666615  [ 9000/10672]\n",
      "loss: 1.024029  [ 9100/10672]\n",
      "loss: 1.035049  [ 9200/10672]\n",
      "loss: 1.213232  [ 9300/10672]\n",
      "loss: 1.207281  [ 9400/10672]\n",
      "loss: 1.126740  [ 9500/10672]\n",
      "loss: 1.676273  [ 9600/10672]\n",
      "loss: 1.205452  [ 9700/10672]\n",
      "loss: 1.166800  [ 9800/10672]\n",
      "loss: 1.100026  [ 9900/10672]\n",
      "loss: 1.233721  [10000/10672]\n",
      "loss: 1.093501  [10100/10672]\n",
      "loss: 0.973092  [10200/10672]\n",
      "loss: 1.127435  [10300/10672]\n",
      "loss: 1.187466  [10400/10672]\n",
      "loss: 1.176500  [10500/10672]\n",
      "loss: 1.669772  [10600/10672]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.027527  [    0/10672]\n",
      "loss: 1.099745  [  100/10672]\n",
      "loss: 0.921992  [  200/10672]\n",
      "loss: 0.957152  [  300/10672]\n",
      "loss: 1.261664  [  400/10672]\n",
      "loss: 1.097060  [  500/10672]\n",
      "loss: 1.666962  [  600/10672]\n",
      "loss: 1.002836  [  700/10672]\n",
      "loss: 1.666353  [  800/10672]\n",
      "loss: 1.474960  [  900/10672]\n",
      "loss: 1.666938  [ 1000/10672]\n",
      "loss: 1.283262  [ 1100/10672]\n",
      "loss: 1.134500  [ 1200/10672]\n",
      "loss: 1.134099  [ 1300/10672]\n",
      "loss: 1.013985  [ 1400/10672]\n",
      "loss: 1.035819  [ 1500/10672]\n",
      "loss: 1.049079  [ 1600/10672]\n",
      "loss: 1.015416  [ 1700/10672]\n",
      "loss: 1.241530  [ 1800/10672]\n",
      "loss: 1.074207  [ 1900/10672]\n",
      "loss: 1.150562  [ 2000/10672]\n",
      "loss: 1.148975  [ 2100/10672]\n",
      "loss: 1.104385  [ 2200/10672]\n",
      "loss: 1.240643  [ 2300/10672]\n",
      "loss: 1.133866  [ 2400/10672]\n",
      "loss: 1.109618  [ 2500/10672]\n",
      "loss: 1.126985  [ 2600/10672]\n",
      "loss: 1.665872  [ 2700/10672]\n",
      "loss: 1.098864  [ 2800/10672]\n",
      "loss: 1.668449  [ 2900/10672]\n",
      "loss: 1.668438  [ 3000/10672]\n",
      "loss: 1.222716  [ 3100/10672]\n",
      "loss: 1.013803  [ 3200/10672]\n",
      "loss: 1.035779  [ 3300/10672]\n",
      "loss: 1.675783  [ 3400/10672]\n",
      "loss: 1.670062  [ 3500/10672]\n",
      "loss: 1.670759  [ 3600/10672]\n",
      "loss: 1.102672  [ 3700/10672]\n",
      "loss: 1.077033  [ 3800/10672]\n",
      "loss: 1.213755  [ 3900/10672]\n",
      "loss: 1.013607  [ 4000/10672]\n",
      "loss: 1.666581  [ 4100/10672]\n",
      "loss: 1.669775  [ 4200/10672]\n",
      "loss: 1.141716  [ 4300/10672]\n",
      "loss: 1.665839  [ 4400/10672]\n",
      "loss: 1.089801  [ 4500/10672]\n",
      "loss: 1.295006  [ 4600/10672]\n",
      "loss: 1.081215  [ 4700/10672]\n",
      "loss: 1.670446  [ 4800/10672]\n",
      "loss: 1.168795  [ 4900/10672]\n",
      "loss: 1.666051  [ 5000/10672]\n",
      "loss: 1.227584  [ 5100/10672]\n",
      "loss: 1.079498  [ 5200/10672]\n",
      "loss: 1.439583  [ 5300/10672]\n",
      "loss: 1.117996  [ 5400/10672]\n",
      "loss: 0.952897  [ 5500/10672]\n",
      "loss: 1.138160  [ 5600/10672]\n",
      "loss: 1.670823  [ 5700/10672]\n",
      "loss: 1.053261  [ 5800/10672]\n",
      "loss: 1.215326  [ 5900/10672]\n",
      "loss: 1.667506  [ 6000/10672]\n",
      "loss: 1.186056  [ 6100/10672]\n",
      "loss: 1.062550  [ 6200/10672]\n",
      "loss: 1.036320  [ 6300/10672]\n",
      "loss: 1.181262  [ 6400/10672]\n",
      "loss: 1.675805  [ 6500/10672]\n",
      "loss: 1.066453  [ 6600/10672]\n",
      "loss: 1.202975  [ 6700/10672]\n",
      "loss: 1.090516  [ 6800/10672]\n",
      "loss: 1.105639  [ 6900/10672]\n",
      "loss: 1.149952  [ 7000/10672]\n",
      "loss: 0.964526  [ 7100/10672]\n",
      "loss: 1.197715  [ 7200/10672]\n",
      "loss: 1.280530  [ 7300/10672]\n",
      "loss: 1.150476  [ 7400/10672]\n",
      "loss: 0.980340  [ 7500/10672]\n",
      "loss: 1.201316  [ 7600/10672]\n",
      "loss: 1.225605  [ 7700/10672]\n",
      "loss: 1.246249  [ 7800/10672]\n",
      "loss: 1.106439  [ 7900/10672]\n",
      "loss: 1.070455  [ 8000/10672]\n",
      "loss: 1.118156  [ 8100/10672]\n",
      "loss: 1.080245  [ 8200/10672]\n",
      "loss: 1.089315  [ 8300/10672]\n",
      "loss: 1.205387  [ 8400/10672]\n",
      "loss: 1.028646  [ 8500/10672]\n",
      "loss: 1.209993  [ 8600/10672]\n",
      "loss: 1.206692  [ 8700/10672]\n",
      "loss: 1.069505  [ 8800/10672]\n",
      "loss: 1.014171  [ 8900/10672]\n",
      "loss: 1.666654  [ 9000/10672]\n",
      "loss: 1.022142  [ 9100/10672]\n",
      "loss: 1.034051  [ 9200/10672]\n",
      "loss: 1.214115  [ 9300/10672]\n",
      "loss: 1.208134  [ 9400/10672]\n",
      "loss: 1.125810  [ 9500/10672]\n",
      "loss: 1.676414  [ 9600/10672]\n",
      "loss: 1.205188  [ 9700/10672]\n",
      "loss: 1.167304  [ 9800/10672]\n",
      "loss: 1.099832  [ 9900/10672]\n",
      "loss: 1.234888  [10000/10672]\n",
      "loss: 1.093067  [10100/10672]\n",
      "loss: 0.971746  [10200/10672]\n",
      "loss: 1.126543  [10300/10672]\n",
      "loss: 1.187077  [10400/10672]\n",
      "loss: 1.175973  [10500/10672]\n",
      "loss: 1.669852  [10600/10672]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.026822  [    0/10672]\n",
      "loss: 1.099327  [  100/10672]\n",
      "loss: 0.920392  [  200/10672]\n",
      "loss: 0.955722  [  300/10672]\n",
      "loss: 1.261890  [  400/10672]\n",
      "loss: 1.095827  [  500/10672]\n",
      "loss: 1.666978  [  600/10672]\n",
      "loss: 1.001760  [  700/10672]\n",
      "loss: 1.666367  [  800/10672]\n",
      "loss: 1.477104  [  900/10672]\n",
      "loss: 1.666949  [ 1000/10672]\n",
      "loss: 1.283708  [ 1100/10672]\n",
      "loss: 1.133420  [ 1200/10672]\n",
      "loss: 1.134178  [ 1300/10672]\n",
      "loss: 1.012122  [ 1400/10672]\n",
      "loss: 1.035019  [ 1500/10672]\n",
      "loss: 1.047392  [ 1600/10672]\n",
      "loss: 1.014433  [ 1700/10672]\n",
      "loss: 1.241820  [ 1800/10672]\n",
      "loss: 1.072836  [ 1900/10672]\n",
      "loss: 1.149781  [ 2000/10672]\n",
      "loss: 1.148408  [ 2100/10672]\n",
      "loss: 1.103332  [ 2200/10672]\n",
      "loss: 1.240962  [ 2300/10672]\n",
      "loss: 1.133734  [ 2400/10672]\n",
      "loss: 1.109290  [ 2500/10672]\n",
      "loss: 1.126962  [ 2600/10672]\n",
      "loss: 1.665891  [ 2700/10672]\n",
      "loss: 1.098395  [ 2800/10672]\n",
      "loss: 1.668541  [ 2900/10672]\n",
      "loss: 1.668486  [ 3000/10672]\n",
      "loss: 1.223096  [ 3100/10672]\n",
      "loss: 1.012233  [ 3200/10672]\n",
      "loss: 1.034792  [ 3300/10672]\n",
      "loss: 1.675958  [ 3400/10672]\n",
      "loss: 1.670138  [ 3500/10672]\n",
      "loss: 1.670875  [ 3600/10672]\n",
      "loss: 1.101612  [ 3700/10672]\n",
      "loss: 1.076481  [ 3800/10672]\n",
      "loss: 1.213693  [ 3900/10672]\n",
      "loss: 1.011737  [ 4000/10672]\n",
      "loss: 1.666609  [ 4100/10672]\n",
      "loss: 1.669905  [ 4200/10672]\n",
      "loss: 1.141685  [ 4300/10672]\n",
      "loss: 1.665857  [ 4400/10672]\n",
      "loss: 1.088577  [ 4500/10672]\n",
      "loss: 1.295961  [ 4600/10672]\n",
      "loss: 1.080081  [ 4700/10672]\n",
      "loss: 1.670532  [ 4800/10672]\n",
      "loss: 1.169173  [ 4900/10672]\n",
      "loss: 1.666080  [ 5000/10672]\n",
      "loss: 1.228616  [ 5100/10672]\n",
      "loss: 1.078745  [ 5200/10672]\n",
      "loss: 1.441808  [ 5300/10672]\n",
      "loss: 1.117791  [ 5400/10672]\n",
      "loss: 0.950907  [ 5500/10672]\n",
      "loss: 1.138037  [ 5600/10672]\n",
      "loss: 1.670911  [ 5700/10672]\n",
      "loss: 1.051748  [ 5800/10672]\n",
      "loss: 1.216102  [ 5900/10672]\n",
      "loss: 1.667560  [ 6000/10672]\n",
      "loss: 1.185966  [ 6100/10672]\n",
      "loss: 1.061761  [ 6200/10672]\n",
      "loss: 1.035013  [ 6300/10672]\n",
      "loss: 1.181878  [ 6400/10672]\n",
      "loss: 1.675966  [ 6500/10672]\n",
      "loss: 1.065905  [ 6600/10672]\n",
      "loss: 1.202952  [ 6700/10672]\n",
      "loss: 1.090022  [ 6800/10672]\n",
      "loss: 1.104461  [ 6900/10672]\n",
      "loss: 1.150180  [ 7000/10672]\n",
      "loss: 0.963141  [ 7100/10672]\n",
      "loss: 1.197582  [ 7200/10672]\n",
      "loss: 1.281920  [ 7300/10672]\n",
      "loss: 1.150029  [ 7400/10672]\n",
      "loss: 0.978941  [ 7500/10672]\n",
      "loss: 1.201164  [ 7600/10672]\n",
      "loss: 1.225777  [ 7700/10672]\n",
      "loss: 1.246518  [ 7800/10672]\n",
      "loss: 1.106107  [ 7900/10672]\n",
      "loss: 1.069883  [ 8000/10672]\n",
      "loss: 1.117808  [ 8100/10672]\n",
      "loss: 1.079033  [ 8200/10672]\n",
      "loss: 1.088188  [ 8300/10672]\n",
      "loss: 1.206114  [ 8400/10672]\n",
      "loss: 1.027780  [ 8500/10672]\n",
      "loss: 1.210765  [ 8600/10672]\n",
      "loss: 1.206890  [ 8700/10672]\n",
      "loss: 1.068402  [ 8800/10672]\n",
      "loss: 1.013111  [ 8900/10672]\n",
      "loss: 1.666691  [ 9000/10672]\n",
      "loss: 1.020270  [ 9100/10672]\n",
      "loss: 1.033058  [ 9200/10672]\n",
      "loss: 1.214993  [ 9300/10672]\n",
      "loss: 1.208983  [ 9400/10672]\n",
      "loss: 1.124887  [ 9500/10672]\n",
      "loss: 1.676555  [ 9600/10672]\n",
      "loss: 1.204923  [ 9700/10672]\n",
      "loss: 1.167805  [ 9800/10672]\n",
      "loss: 1.099640  [ 9900/10672]\n",
      "loss: 1.236046  [10000/10672]\n",
      "loss: 1.092632  [10100/10672]\n",
      "loss: 0.970412  [10200/10672]\n",
      "loss: 1.125659  [10300/10672]\n",
      "loss: 1.186693  [10400/10672]\n",
      "loss: 1.175450  [10500/10672]\n",
      "loss: 1.669931  [10600/10672]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.026124  [    0/10672]\n",
      "loss: 1.098904  [  100/10672]\n",
      "loss: 0.918811  [  200/10672]\n",
      "loss: 0.954304  [  300/10672]\n",
      "loss: 1.262114  [  400/10672]\n",
      "loss: 1.094604  [  500/10672]\n",
      "loss: 1.666994  [  600/10672]\n",
      "loss: 1.000691  [  700/10672]\n",
      "loss: 1.666380  [  800/10672]\n",
      "loss: 1.479226  [  900/10672]\n",
      "loss: 1.666960  [ 1000/10672]\n",
      "loss: 1.284152  [ 1100/10672]\n",
      "loss: 1.132344  [ 1200/10672]\n",
      "loss: 1.134253  [ 1300/10672]\n",
      "loss: 1.010280  [ 1400/10672]\n",
      "loss: 1.034223  [ 1500/10672]\n",
      "loss: 1.045720  [ 1600/10672]\n",
      "loss: 1.013456  [ 1700/10672]\n",
      "loss: 1.242111  [ 1800/10672]\n",
      "loss: 1.071478  [ 1900/10672]\n",
      "loss: 1.149003  [ 2000/10672]\n",
      "loss: 1.147849  [ 2100/10672]\n",
      "loss: 1.102289  [ 2200/10672]\n",
      "loss: 1.241279  [ 2300/10672]\n",
      "loss: 1.133599  [ 2400/10672]\n",
      "loss: 1.108962  [ 2500/10672]\n",
      "loss: 1.126941  [ 2600/10672]\n",
      "loss: 1.665911  [ 2700/10672]\n",
      "loss: 1.097925  [ 2800/10672]\n",
      "loss: 1.668633  [ 2900/10672]\n",
      "loss: 1.668533  [ 3000/10672]\n",
      "loss: 1.223479  [ 3100/10672]\n",
      "loss: 1.010683  [ 3200/10672]\n",
      "loss: 1.033810  [ 3300/10672]\n",
      "loss: 1.676134  [ 3400/10672]\n",
      "loss: 1.670215  [ 3500/10672]\n",
      "loss: 1.670989  [ 3600/10672]\n",
      "loss: 1.100562  [ 3700/10672]\n",
      "loss: 1.075929  [ 3800/10672]\n",
      "loss: 1.213633  [ 3900/10672]\n",
      "loss: 1.009888  [ 4000/10672]\n",
      "loss: 1.666636  [ 4100/10672]\n",
      "loss: 1.670035  [ 4200/10672]\n",
      "loss: 1.141647  [ 4300/10672]\n",
      "loss: 1.665875  [ 4400/10672]\n",
      "loss: 1.087362  [ 4500/10672]\n",
      "loss: 1.296913  [ 4600/10672]\n",
      "loss: 1.078960  [ 4700/10672]\n",
      "loss: 1.670618  [ 4800/10672]\n",
      "loss: 1.169549  [ 4900/10672]\n",
      "loss: 1.666108  [ 5000/10672]\n",
      "loss: 1.229645  [ 5100/10672]\n",
      "loss: 1.077994  [ 5200/10672]\n",
      "loss: 1.444014  [ 5300/10672]\n",
      "loss: 1.117584  [ 5400/10672]\n",
      "loss: 0.948944  [ 5500/10672]\n",
      "loss: 1.137912  [ 5600/10672]\n",
      "loss: 1.670999  [ 5700/10672]\n",
      "loss: 1.050245  [ 5800/10672]\n",
      "loss: 1.216875  [ 5900/10672]\n",
      "loss: 1.667614  [ 6000/10672]\n",
      "loss: 1.185875  [ 6100/10672]\n",
      "loss: 1.060979  [ 6200/10672]\n",
      "loss: 1.033719  [ 6300/10672]\n",
      "loss: 1.182496  [ 6400/10672]\n",
      "loss: 1.676127  [ 6500/10672]\n",
      "loss: 1.065364  [ 6600/10672]\n",
      "loss: 1.202930  [ 6700/10672]\n",
      "loss: 1.089532  [ 6800/10672]\n",
      "loss: 1.103286  [ 6900/10672]\n",
      "loss: 1.150410  [ 7000/10672]\n",
      "loss: 0.961772  [ 7100/10672]\n",
      "loss: 1.197449  [ 7200/10672]\n",
      "loss: 1.283300  [ 7300/10672]\n",
      "loss: 1.149585  [ 7400/10672]\n",
      "loss: 0.977556  [ 7500/10672]\n",
      "loss: 1.201012  [ 7600/10672]\n",
      "loss: 1.225946  [ 7700/10672]\n",
      "loss: 1.246782  [ 7800/10672]\n",
      "loss: 1.105774  [ 7900/10672]\n",
      "loss: 1.069313  [ 8000/10672]\n",
      "loss: 1.117458  [ 8100/10672]\n",
      "loss: 1.077832  [ 8200/10672]\n",
      "loss: 1.087070  [ 8300/10672]\n",
      "loss: 1.206836  [ 8400/10672]\n",
      "loss: 1.026922  [ 8500/10672]\n",
      "loss: 1.211534  [ 8600/10672]\n",
      "loss: 1.207093  [ 8700/10672]\n",
      "loss: 1.067310  [ 8800/10672]\n",
      "loss: 1.012060  [ 8900/10672]\n",
      "loss: 1.666729  [ 9000/10672]\n",
      "loss: 1.018414  [ 9100/10672]\n",
      "loss: 1.032073  [ 9200/10672]\n",
      "loss: 1.215866  [ 9300/10672]\n",
      "loss: 1.209828  [ 9400/10672]\n",
      "loss: 1.123971  [ 9500/10672]\n",
      "loss: 1.676696  [ 9600/10672]\n",
      "loss: 1.204655  [ 9700/10672]\n",
      "loss: 1.168307  [ 9800/10672]\n",
      "loss: 1.099450  [ 9900/10672]\n",
      "loss: 1.237198  [10000/10672]\n",
      "loss: 1.092197  [10100/10672]\n",
      "loss: 0.969092  [10200/10672]\n",
      "loss: 1.124781  [10300/10672]\n",
      "loss: 1.186311  [10400/10672]\n",
      "loss: 1.174930  [10500/10672]\n",
      "loss: 1.670009  [10600/10672]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.025434  [    0/10672]\n",
      "loss: 1.098482  [  100/10672]\n",
      "loss: 0.917251  [  200/10672]\n",
      "loss: 0.952902  [  300/10672]\n",
      "loss: 1.262333  [  400/10672]\n",
      "loss: 1.093391  [  500/10672]\n",
      "loss: 1.667009  [  600/10672]\n",
      "loss: 0.999629  [  700/10672]\n",
      "loss: 1.666393  [  800/10672]\n",
      "loss: 1.481324  [  900/10672]\n",
      "loss: 1.666971  [ 1000/10672]\n",
      "loss: 1.284587  [ 1100/10672]\n",
      "loss: 1.131270  [ 1200/10672]\n",
      "loss: 1.134326  [ 1300/10672]\n",
      "loss: 1.008456  [ 1400/10672]\n",
      "loss: 1.033434  [ 1500/10672]\n",
      "loss: 1.044061  [ 1600/10672]\n",
      "loss: 1.012487  [ 1700/10672]\n",
      "loss: 1.242398  [ 1800/10672]\n",
      "loss: 1.070130  [ 1900/10672]\n",
      "loss: 1.148226  [ 2000/10672]\n",
      "loss: 1.147295  [ 2100/10672]\n",
      "loss: 1.101253  [ 2200/10672]\n",
      "loss: 1.241593  [ 2300/10672]\n",
      "loss: 1.133461  [ 2400/10672]\n",
      "loss: 1.108634  [ 2500/10672]\n",
      "loss: 1.126924  [ 2600/10672]\n",
      "loss: 1.665930  [ 2700/10672]\n",
      "loss: 1.097457  [ 2800/10672]\n",
      "loss: 1.668724  [ 2900/10672]\n",
      "loss: 1.668580  [ 3000/10672]\n",
      "loss: 1.223861  [ 3100/10672]\n",
      "loss: 1.009148  [ 3200/10672]\n",
      "loss: 1.032836  [ 3300/10672]\n",
      "loss: 1.676308  [ 3400/10672]\n",
      "loss: 1.670290  [ 3500/10672]\n",
      "loss: 1.671103  [ 3600/10672]\n",
      "loss: 1.099517  [ 3700/10672]\n",
      "loss: 1.075381  [ 3800/10672]\n",
      "loss: 1.213571  [ 3900/10672]\n",
      "loss: 1.008055  [ 4000/10672]\n",
      "loss: 1.666663  [ 4100/10672]\n",
      "loss: 1.670165  [ 4200/10672]\n",
      "loss: 1.141606  [ 4300/10672]\n",
      "loss: 1.665892  [ 4400/10672]\n",
      "loss: 1.086153  [ 4500/10672]\n",
      "loss: 1.297860  [ 4600/10672]\n",
      "loss: 1.077848  [ 4700/10672]\n",
      "loss: 1.670704  [ 4800/10672]\n",
      "loss: 1.169922  [ 4900/10672]\n",
      "loss: 1.666136  [ 5000/10672]\n",
      "loss: 1.230668  [ 5100/10672]\n",
      "loss: 1.077243  [ 5200/10672]\n",
      "loss: 1.446200  [ 5300/10672]\n",
      "loss: 1.117376  [ 5400/10672]\n",
      "loss: 0.947007  [ 5500/10672]\n",
      "loss: 1.137780  [ 5600/10672]\n",
      "loss: 1.671087  [ 5700/10672]\n",
      "loss: 1.048754  [ 5800/10672]\n",
      "loss: 1.217639  [ 5900/10672]\n",
      "loss: 1.667668  [ 6000/10672]\n",
      "loss: 1.185791  [ 6100/10672]\n",
      "loss: 1.060197  [ 6200/10672]\n",
      "loss: 1.032446  [ 6300/10672]\n",
      "loss: 1.183107  [ 6400/10672]\n",
      "loss: 1.676289  [ 6500/10672]\n",
      "loss: 1.064823  [ 6600/10672]\n",
      "loss: 1.202914  [ 6700/10672]\n",
      "loss: 1.089039  [ 6800/10672]\n",
      "loss: 1.102120  [ 6900/10672]\n",
      "loss: 1.150636  [ 7000/10672]\n",
      "loss: 0.960415  [ 7100/10672]\n",
      "loss: 1.197319  [ 7200/10672]\n",
      "loss: 1.284666  [ 7300/10672]\n",
      "loss: 1.149147  [ 7400/10672]\n",
      "loss: 0.976183  [ 7500/10672]\n",
      "loss: 1.200861  [ 7600/10672]\n",
      "loss: 1.226115  [ 7700/10672]\n",
      "loss: 1.247043  [ 7800/10672]\n",
      "loss: 1.105438  [ 7900/10672]\n",
      "loss: 1.068744  [ 8000/10672]\n",
      "loss: 1.117105  [ 8100/10672]\n",
      "loss: 1.076645  [ 8200/10672]\n",
      "loss: 1.085965  [ 8300/10672]\n",
      "loss: 1.207552  [ 8400/10672]\n",
      "loss: 1.026069  [ 8500/10672]\n",
      "loss: 1.212297  [ 8600/10672]\n",
      "loss: 1.207302  [ 8700/10672]\n",
      "loss: 1.066232  [ 8800/10672]\n",
      "loss: 1.011015  [ 8900/10672]\n",
      "loss: 1.666766  [ 9000/10672]\n",
      "loss: 1.016576  [ 9100/10672]\n",
      "loss: 1.031091  [ 9200/10672]\n",
      "loss: 1.216730  [ 9300/10672]\n",
      "loss: 1.210665  [ 9400/10672]\n",
      "loss: 1.123065  [ 9500/10672]\n",
      "loss: 1.676836  [ 9600/10672]\n",
      "loss: 1.204389  [ 9700/10672]\n",
      "loss: 1.168804  [ 9800/10672]\n",
      "loss: 1.099259  [ 9900/10672]\n",
      "loss: 1.238340  [10000/10672]\n",
      "loss: 1.091761  [10100/10672]\n",
      "loss: 0.967782  [10200/10672]\n",
      "loss: 1.123913  [10300/10672]\n",
      "loss: 1.185934  [10400/10672]\n",
      "loss: 1.174415  [10500/10672]\n",
      "loss: 1.670087  [10600/10672]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.024752  [    0/10672]\n",
      "loss: 1.098054  [  100/10672]\n",
      "loss: 0.915709  [  200/10672]\n",
      "loss: 0.951510  [  300/10672]\n",
      "loss: 1.262550  [  400/10672]\n",
      "loss: 1.092190  [  500/10672]\n",
      "loss: 1.667024  [  600/10672]\n",
      "loss: 0.998573  [  700/10672]\n",
      "loss: 1.666406  [  800/10672]\n",
      "loss: 1.483399  [  900/10672]\n",
      "loss: 1.666981  [ 1000/10672]\n",
      "loss: 1.285019  [ 1100/10672]\n",
      "loss: 1.130202  [ 1200/10672]\n",
      "loss: 1.134394  [ 1300/10672]\n",
      "loss: 1.006653  [ 1400/10672]\n",
      "loss: 1.032650  [ 1500/10672]\n",
      "loss: 1.042418  [ 1600/10672]\n",
      "loss: 1.011521  [ 1700/10672]\n",
      "loss: 1.242687  [ 1800/10672]\n",
      "loss: 1.068796  [ 1900/10672]\n",
      "loss: 1.147456  [ 2000/10672]\n",
      "loss: 1.146751  [ 2100/10672]\n",
      "loss: 1.100228  [ 2200/10672]\n",
      "loss: 1.241908  [ 2300/10672]\n",
      "loss: 1.133319  [ 2400/10672]\n",
      "loss: 1.108305  [ 2500/10672]\n",
      "loss: 1.126905  [ 2600/10672]\n",
      "loss: 1.665948  [ 2700/10672]\n",
      "loss: 1.096989  [ 2800/10672]\n",
      "loss: 1.668815  [ 2900/10672]\n",
      "loss: 1.668627  [ 3000/10672]\n",
      "loss: 1.224247  [ 3100/10672]\n",
      "loss: 1.007634  [ 3200/10672]\n",
      "loss: 1.031865  [ 3300/10672]\n",
      "loss: 1.676483  [ 3400/10672]\n",
      "loss: 1.670366  [ 3500/10672]\n",
      "loss: 1.671217  [ 3600/10672]\n",
      "loss: 1.098480  [ 3700/10672]\n",
      "loss: 1.074833  [ 3800/10672]\n",
      "loss: 1.213510  [ 3900/10672]\n",
      "loss: 1.006242  [ 4000/10672]\n",
      "loss: 1.666690  [ 4100/10672]\n",
      "loss: 1.670295  [ 4200/10672]\n",
      "loss: 1.141560  [ 4300/10672]\n",
      "loss: 1.665908  [ 4400/10672]\n",
      "loss: 1.084955  [ 4500/10672]\n",
      "loss: 1.298804  [ 4600/10672]\n",
      "loss: 1.076750  [ 4700/10672]\n",
      "loss: 1.670790  [ 4800/10672]\n",
      "loss: 1.170291  [ 4900/10672]\n",
      "loss: 1.666163  [ 5000/10672]\n",
      "loss: 1.231685  [ 5100/10672]\n",
      "loss: 1.076490  [ 5200/10672]\n",
      "loss: 1.448370  [ 5300/10672]\n",
      "loss: 1.117163  [ 5400/10672]\n",
      "loss: 0.945096  [ 5500/10672]\n",
      "loss: 1.137644  [ 5600/10672]\n",
      "loss: 1.671175  [ 5700/10672]\n",
      "loss: 1.047275  [ 5800/10672]\n",
      "loss: 1.218398  [ 5900/10672]\n",
      "loss: 1.667721  [ 6000/10672]\n",
      "loss: 1.185708  [ 6100/10672]\n",
      "loss: 1.059422  [ 6200/10672]\n",
      "loss: 1.031186  [ 6300/10672]\n",
      "loss: 1.183717  [ 6400/10672]\n",
      "loss: 1.676450  [ 6500/10672]\n",
      "loss: 1.064287  [ 6600/10672]\n",
      "loss: 1.202899  [ 6700/10672]\n",
      "loss: 1.088550  [ 6800/10672]\n",
      "loss: 1.100958  [ 6900/10672]\n",
      "loss: 1.150863  [ 7000/10672]\n",
      "loss: 0.959074  [ 7100/10672]\n",
      "loss: 1.197188  [ 7200/10672]\n",
      "loss: 1.286023  [ 7300/10672]\n",
      "loss: 1.148711  [ 7400/10672]\n",
      "loss: 0.974824  [ 7500/10672]\n",
      "loss: 1.200709  [ 7600/10672]\n",
      "loss: 1.226282  [ 7700/10672]\n",
      "loss: 1.247298  [ 7800/10672]\n",
      "loss: 1.105103  [ 7900/10672]\n",
      "loss: 1.068178  [ 8000/10672]\n",
      "loss: 1.116749  [ 8100/10672]\n",
      "loss: 1.075468  [ 8200/10672]\n",
      "loss: 1.084868  [ 8300/10672]\n",
      "loss: 1.208264  [ 8400/10672]\n",
      "loss: 1.025224  [ 8500/10672]\n",
      "loss: 1.213056  [ 8600/10672]\n",
      "loss: 1.207516  [ 8700/10672]\n",
      "loss: 1.065166  [ 8800/10672]\n",
      "loss: 1.009980  [ 8900/10672]\n",
      "loss: 1.666803  [ 9000/10672]\n",
      "loss: 1.014752  [ 9100/10672]\n",
      "loss: 1.030116  [ 9200/10672]\n",
      "loss: 1.217590  [ 9300/10672]\n",
      "loss: 1.211496  [ 9400/10672]\n",
      "loss: 1.122167  [ 9500/10672]\n",
      "loss: 1.676975  [ 9600/10672]\n",
      "loss: 1.204121  [ 9700/10672]\n",
      "loss: 1.169299  [ 9800/10672]\n",
      "loss: 1.099072  [ 9900/10672]\n",
      "loss: 1.239475  [10000/10672]\n",
      "loss: 1.091323  [10100/10672]\n",
      "loss: 0.966484  [10200/10672]\n",
      "loss: 1.123052  [10300/10672]\n",
      "loss: 1.185561  [10400/10672]\n",
      "loss: 1.173904  [10500/10672]\n",
      "loss: 1.670164  [10600/10672]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.024076  [    0/10672]\n",
      "loss: 1.097624  [  100/10672]\n",
      "loss: 0.914185  [  200/10672]\n",
      "loss: 0.950133  [  300/10672]\n",
      "loss: 1.262765  [  400/10672]\n",
      "loss: 1.091000  [  500/10672]\n",
      "loss: 1.667038  [  600/10672]\n",
      "loss: 0.997522  [  700/10672]\n",
      "loss: 1.666418  [  800/10672]\n",
      "loss: 1.485453  [  900/10672]\n",
      "loss: 1.666991  [ 1000/10672]\n",
      "loss: 1.285445  [ 1100/10672]\n",
      "loss: 1.129137  [ 1200/10672]\n",
      "loss: 1.134459  [ 1300/10672]\n",
      "loss: 1.004870  [ 1400/10672]\n",
      "loss: 1.031870  [ 1500/10672]\n",
      "loss: 1.040789  [ 1600/10672]\n",
      "loss: 1.010561  [ 1700/10672]\n",
      "loss: 1.242976  [ 1800/10672]\n",
      "loss: 1.067473  [ 1900/10672]\n",
      "loss: 1.146688  [ 2000/10672]\n",
      "loss: 1.146213  [ 2100/10672]\n",
      "loss: 1.099213  [ 2200/10672]\n",
      "loss: 1.242221  [ 2300/10672]\n",
      "loss: 1.133175  [ 2400/10672]\n",
      "loss: 1.107976  [ 2500/10672]\n",
      "loss: 1.126887  [ 2600/10672]\n",
      "loss: 1.665966  [ 2700/10672]\n",
      "loss: 1.096521  [ 2800/10672]\n",
      "loss: 1.668905  [ 2900/10672]\n",
      "loss: 1.668673  [ 3000/10672]\n",
      "loss: 1.224634  [ 3100/10672]\n",
      "loss: 1.006137  [ 3200/10672]\n",
      "loss: 1.030901  [ 3300/10672]\n",
      "loss: 1.676658  [ 3400/10672]\n",
      "loss: 1.670442  [ 3500/10672]\n",
      "loss: 1.671330  [ 3600/10672]\n",
      "loss: 1.097451  [ 3700/10672]\n",
      "loss: 1.074288  [ 3800/10672]\n",
      "loss: 1.213447  [ 3900/10672]\n",
      "loss: 1.004446  [ 4000/10672]\n",
      "loss: 1.666716  [ 4100/10672]\n",
      "loss: 1.670426  [ 4200/10672]\n",
      "loss: 1.141511  [ 4300/10672]\n",
      "loss: 1.665924  [ 4400/10672]\n",
      "loss: 1.083763  [ 4500/10672]\n",
      "loss: 1.299743  [ 4600/10672]\n",
      "loss: 1.075662  [ 4700/10672]\n",
      "loss: 1.670875  [ 4800/10672]\n",
      "loss: 1.170658  [ 4900/10672]\n",
      "loss: 1.666189  [ 5000/10672]\n",
      "loss: 1.232699  [ 5100/10672]\n",
      "loss: 1.075739  [ 5200/10672]\n",
      "loss: 1.450519  [ 5300/10672]\n",
      "loss: 1.116951  [ 5400/10672]\n",
      "loss: 0.943211  [ 5500/10672]\n",
      "loss: 1.137504  [ 5600/10672]\n",
      "loss: 1.671262  [ 5700/10672]\n",
      "loss: 1.045807  [ 5800/10672]\n",
      "loss: 1.219151  [ 5900/10672]\n",
      "loss: 1.667774  [ 6000/10672]\n",
      "loss: 1.185630  [ 6100/10672]\n",
      "loss: 1.058650  [ 6200/10672]\n",
      "loss: 1.029944  [ 6300/10672]\n",
      "loss: 1.184325  [ 6400/10672]\n",
      "loss: 1.676611  [ 6500/10672]\n",
      "loss: 1.063755  [ 6600/10672]\n",
      "loss: 1.202887  [ 6700/10672]\n",
      "loss: 1.088061  [ 6800/10672]\n",
      "loss: 1.099802  [ 6900/10672]\n",
      "loss: 1.151088  [ 7000/10672]\n",
      "loss: 0.957745  [ 7100/10672]\n",
      "loss: 1.197060  [ 7200/10672]\n",
      "loss: 1.287367  [ 7300/10672]\n",
      "loss: 1.148282  [ 7400/10672]\n",
      "loss: 0.973476  [ 7500/10672]\n",
      "loss: 1.200559  [ 7600/10672]\n",
      "loss: 1.226448  [ 7700/10672]\n",
      "loss: 1.247551  [ 7800/10672]\n",
      "loss: 1.104765  [ 7900/10672]\n",
      "loss: 1.067612  [ 8000/10672]\n",
      "loss: 1.116389  [ 8100/10672]\n",
      "loss: 1.074306  [ 8200/10672]\n",
      "loss: 1.083785  [ 8300/10672]\n",
      "loss: 1.208969  [ 8400/10672]\n",
      "loss: 1.024383  [ 8500/10672]\n",
      "loss: 1.213807  [ 8600/10672]\n",
      "loss: 1.207737  [ 8700/10672]\n",
      "loss: 1.064113  [ 8800/10672]\n",
      "loss: 1.008950  [ 8900/10672]\n",
      "loss: 1.666840  [ 9000/10672]\n",
      "loss: 1.012946  [ 9100/10672]\n",
      "loss: 1.029145  [ 9200/10672]\n",
      "loss: 1.218443  [ 9300/10672]\n",
      "loss: 1.212321  [ 9400/10672]\n",
      "loss: 1.121278  [ 9500/10672]\n",
      "loss: 1.677115  [ 9600/10672]\n",
      "loss: 1.203855  [ 9700/10672]\n",
      "loss: 1.169792  [ 9800/10672]\n",
      "loss: 1.098883  [ 9900/10672]\n",
      "loss: 1.240602  [10000/10672]\n",
      "loss: 1.090883  [10100/10672]\n",
      "loss: 0.965197  [10200/10672]\n",
      "loss: 1.122199  [10300/10672]\n",
      "loss: 1.185192  [10400/10672]\n",
      "loss: 1.173396  [10500/10672]\n",
      "loss: 1.670240  [10600/10672]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.023408  [    0/10672]\n",
      "loss: 1.097191  [  100/10672]\n",
      "loss: 0.912681  [  200/10672]\n",
      "loss: 0.948768  [  300/10672]\n",
      "loss: 1.262975  [  400/10672]\n",
      "loss: 1.089820  [  500/10672]\n",
      "loss: 1.667053  [  600/10672]\n",
      "loss: 0.996479  [  700/10672]\n",
      "loss: 1.666429  [  800/10672]\n",
      "loss: 1.487484  [  900/10672]\n",
      "loss: 1.667001  [ 1000/10672]\n",
      "loss: 1.285866  [ 1100/10672]\n",
      "loss: 1.128075  [ 1200/10672]\n",
      "loss: 1.134521  [ 1300/10672]\n",
      "loss: 1.003106  [ 1400/10672]\n",
      "loss: 1.031096  [ 1500/10672]\n",
      "loss: 1.039173  [ 1600/10672]\n",
      "loss: 1.009608  [ 1700/10672]\n",
      "loss: 1.243263  [ 1800/10672]\n",
      "loss: 1.066161  [ 1900/10672]\n",
      "loss: 1.145923  [ 2000/10672]\n",
      "loss: 1.145682  [ 2100/10672]\n",
      "loss: 1.098205  [ 2200/10672]\n",
      "loss: 1.242531  [ 2300/10672]\n",
      "loss: 1.133028  [ 2400/10672]\n",
      "loss: 1.107646  [ 2500/10672]\n",
      "loss: 1.126871  [ 2600/10672]\n",
      "loss: 1.665984  [ 2700/10672]\n",
      "loss: 1.096053  [ 2800/10672]\n",
      "loss: 1.668995  [ 2900/10672]\n",
      "loss: 1.668718  [ 3000/10672]\n",
      "loss: 1.225022  [ 3100/10672]\n",
      "loss: 1.004656  [ 3200/10672]\n",
      "loss: 1.029943  [ 3300/10672]\n",
      "loss: 1.676832  [ 3400/10672]\n",
      "loss: 1.670517  [ 3500/10672]\n",
      "loss: 1.671443  [ 3600/10672]\n",
      "loss: 1.096429  [ 3700/10672]\n",
      "loss: 1.073744  [ 3800/10672]\n",
      "loss: 1.213386  [ 3900/10672]\n",
      "loss: 1.002668  [ 4000/10672]\n",
      "loss: 1.666742  [ 4100/10672]\n",
      "loss: 1.670556  [ 4200/10672]\n",
      "loss: 1.141458  [ 4300/10672]\n",
      "loss: 1.665939  [ 4400/10672]\n",
      "loss: 1.082580  [ 4500/10672]\n",
      "loss: 1.300678  [ 4600/10672]\n",
      "loss: 1.074584  [ 4700/10672]\n",
      "loss: 1.670961  [ 4800/10672]\n",
      "loss: 1.171023  [ 4900/10672]\n",
      "loss: 1.666215  [ 5000/10672]\n",
      "loss: 1.233709  [ 5100/10672]\n",
      "loss: 1.074988  [ 5200/10672]\n",
      "loss: 1.452648  [ 5300/10672]\n",
      "loss: 1.116737  [ 5400/10672]\n",
      "loss: 0.941351  [ 5500/10672]\n",
      "loss: 1.137362  [ 5600/10672]\n",
      "loss: 1.671348  [ 5700/10672]\n",
      "loss: 1.044347  [ 5800/10672]\n",
      "loss: 1.219901  [ 5900/10672]\n",
      "loss: 1.667827  [ 6000/10672]\n",
      "loss: 1.185550  [ 6100/10672]\n",
      "loss: 1.057884  [ 6200/10672]\n",
      "loss: 1.028715  [ 6300/10672]\n",
      "loss: 1.184933  [ 6400/10672]\n",
      "loss: 1.676771  [ 6500/10672]\n",
      "loss: 1.063229  [ 6600/10672]\n",
      "loss: 1.202874  [ 6700/10672]\n",
      "loss: 1.087577  [ 6800/10672]\n",
      "loss: 1.098647  [ 6900/10672]\n",
      "loss: 1.151317  [ 7000/10672]\n",
      "loss: 0.956433  [ 7100/10672]\n",
      "loss: 1.196928  [ 7200/10672]\n",
      "loss: 1.288705  [ 7300/10672]\n",
      "loss: 1.147852  [ 7400/10672]\n",
      "loss: 0.972144  [ 7500/10672]\n",
      "loss: 1.200405  [ 7600/10672]\n",
      "loss: 1.226607  [ 7700/10672]\n",
      "loss: 1.247794  [ 7800/10672]\n",
      "loss: 1.104431  [ 7900/10672]\n",
      "loss: 1.067054  [ 8000/10672]\n",
      "loss: 1.116032  [ 8100/10672]\n",
      "loss: 1.073150  [ 8200/10672]\n",
      "loss: 1.082706  [ 8300/10672]\n",
      "loss: 1.209673  [ 8400/10672]\n",
      "loss: 1.023552  [ 8500/10672]\n",
      "loss: 1.214558  [ 8600/10672]\n",
      "loss: 1.207958  [ 8700/10672]\n",
      "loss: 1.063069  [ 8800/10672]\n",
      "loss: 1.007931  [ 8900/10672]\n",
      "loss: 1.666877  [ 9000/10672]\n",
      "loss: 1.011153  [ 9100/10672]\n",
      "loss: 1.028183  [ 9200/10672]\n",
      "loss: 1.219293  [ 9300/10672]\n",
      "loss: 1.213143  [ 9400/10672]\n",
      "loss: 1.120395  [ 9500/10672]\n",
      "loss: 1.677254  [ 9600/10672]\n",
      "loss: 1.203585  [ 9700/10672]\n",
      "loss: 1.170285  [ 9800/10672]\n",
      "loss: 1.098699  [ 9900/10672]\n",
      "loss: 1.241723  [10000/10672]\n",
      "loss: 1.090445  [10100/10672]\n",
      "loss: 0.963924  [10200/10672]\n",
      "loss: 1.121350  [10300/10672]\n",
      "loss: 1.184824  [10400/10672]\n",
      "loss: 1.172889  [10500/10672]\n",
      "loss: 1.670316  [10600/10672]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.022749  [    0/10672]\n",
      "loss: 1.096758  [  100/10672]\n",
      "loss: 0.911196  [  200/10672]\n",
      "loss: 0.947418  [  300/10672]\n",
      "loss: 1.263180  [  400/10672]\n",
      "loss: 1.088648  [  500/10672]\n",
      "loss: 1.667067  [  600/10672]\n",
      "loss: 0.995443  [  700/10672]\n",
      "loss: 1.666440  [  800/10672]\n",
      "loss: 1.489491  [  900/10672]\n",
      "loss: 1.667011  [ 1000/10672]\n",
      "loss: 1.286280  [ 1100/10672]\n",
      "loss: 1.127016  [ 1200/10672]\n",
      "loss: 1.134583  [ 1300/10672]\n",
      "loss: 1.001359  [ 1400/10672]\n",
      "loss: 1.030328  [ 1500/10672]\n",
      "loss: 1.037570  [ 1600/10672]\n",
      "loss: 1.008661  [ 1700/10672]\n",
      "loss: 1.243548  [ 1800/10672]\n",
      "loss: 1.064857  [ 1900/10672]\n",
      "loss: 1.145159  [ 2000/10672]\n",
      "loss: 1.145155  [ 2100/10672]\n",
      "loss: 1.097206  [ 2200/10672]\n",
      "loss: 1.242839  [ 2300/10672]\n",
      "loss: 1.132880  [ 2400/10672]\n",
      "loss: 1.107318  [ 2500/10672]\n",
      "loss: 1.126858  [ 2600/10672]\n",
      "loss: 1.666002  [ 2700/10672]\n",
      "loss: 1.095587  [ 2800/10672]\n",
      "loss: 1.669085  [ 2900/10672]\n",
      "loss: 1.668763  [ 3000/10672]\n",
      "loss: 1.225409  [ 3100/10672]\n",
      "loss: 1.003192  [ 3200/10672]\n",
      "loss: 1.028992  [ 3300/10672]\n",
      "loss: 1.677006  [ 3400/10672]\n",
      "loss: 1.670593  [ 3500/10672]\n",
      "loss: 1.671556  [ 3600/10672]\n",
      "loss: 1.095413  [ 3700/10672]\n",
      "loss: 1.073202  [ 3800/10672]\n",
      "loss: 1.213322  [ 3900/10672]\n",
      "loss: 1.000907  [ 4000/10672]\n",
      "loss: 1.666767  [ 4100/10672]\n",
      "loss: 1.670687  [ 4200/10672]\n",
      "loss: 1.141402  [ 4300/10672]\n",
      "loss: 1.665955  [ 4400/10672]\n",
      "loss: 1.081404  [ 4500/10672]\n",
      "loss: 1.301609  [ 4600/10672]\n",
      "loss: 1.073516  [ 4700/10672]\n",
      "loss: 1.671046  [ 4800/10672]\n",
      "loss: 1.171386  [ 4900/10672]\n",
      "loss: 1.666240  [ 5000/10672]\n",
      "loss: 1.234714  [ 5100/10672]\n",
      "loss: 1.074238  [ 5200/10672]\n",
      "loss: 1.454760  [ 5300/10672]\n",
      "loss: 1.116521  [ 5400/10672]\n",
      "loss: 0.939515  [ 5500/10672]\n",
      "loss: 1.137214  [ 5600/10672]\n",
      "loss: 1.671435  [ 5700/10672]\n",
      "loss: 1.042900  [ 5800/10672]\n",
      "loss: 1.220643  [ 5900/10672]\n",
      "loss: 1.667880  [ 6000/10672]\n",
      "loss: 1.185476  [ 6100/10672]\n",
      "loss: 1.057122  [ 6200/10672]\n",
      "loss: 1.027502  [ 6300/10672]\n",
      "loss: 1.185537  [ 6400/10672]\n",
      "loss: 1.676932  [ 6500/10672]\n",
      "loss: 1.062706  [ 6600/10672]\n",
      "loss: 1.202865  [ 6700/10672]\n",
      "loss: 1.087093  [ 6800/10672]\n",
      "loss: 1.097499  [ 6900/10672]\n",
      "loss: 1.151543  [ 7000/10672]\n",
      "loss: 0.955132  [ 7100/10672]\n",
      "loss: 1.196798  [ 7200/10672]\n",
      "loss: 1.290031  [ 7300/10672]\n",
      "loss: 1.147428  [ 7400/10672]\n",
      "loss: 0.970824  [ 7500/10672]\n",
      "loss: 1.200253  [ 7600/10672]\n",
      "loss: 1.226768  [ 7700/10672]\n",
      "loss: 1.248035  [ 7800/10672]\n",
      "loss: 1.104093  [ 7900/10672]\n",
      "loss: 1.066495  [ 8000/10672]\n",
      "loss: 1.115670  [ 8100/10672]\n",
      "loss: 1.072008  [ 8200/10672]\n",
      "loss: 1.081639  [ 8300/10672]\n",
      "loss: 1.210373  [ 8400/10672]\n",
      "loss: 1.022727  [ 8500/10672]\n",
      "loss: 1.215304  [ 8600/10672]\n",
      "loss: 1.208186  [ 8700/10672]\n",
      "loss: 1.062037  [ 8800/10672]\n",
      "loss: 1.006919  [ 8900/10672]\n",
      "loss: 1.666913  [ 9000/10672]\n",
      "loss: 1.009376  [ 9100/10672]\n",
      "loss: 1.027224  [ 9200/10672]\n",
      "loss: 1.220135  [ 9300/10672]\n",
      "loss: 1.213959  [ 9400/10672]\n",
      "loss: 1.119520  [ 9500/10672]\n",
      "loss: 1.677392  [ 9600/10672]\n",
      "loss: 1.203315  [ 9700/10672]\n",
      "loss: 1.170774  [ 9800/10672]\n",
      "loss: 1.098514  [ 9900/10672]\n",
      "loss: 1.242835  [10000/10672]\n",
      "loss: 1.090005  [10100/10672]\n",
      "loss: 0.962660  [10200/10672]\n",
      "loss: 1.120512  [10300/10672]\n",
      "loss: 1.184462  [10400/10672]\n",
      "loss: 1.172390  [10500/10672]\n",
      "loss: 1.670392  [10600/10672]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.022094  [    0/10672]\n",
      "loss: 1.096319  [  100/10672]\n",
      "loss: 0.909727  [  200/10672]\n",
      "loss: 0.946078  [  300/10672]\n",
      "loss: 1.263387  [  400/10672]\n",
      "loss: 1.087489  [  500/10672]\n",
      "loss: 1.667080  [  600/10672]\n",
      "loss: 0.994410  [  700/10672]\n",
      "loss: 1.666451  [  800/10672]\n",
      "loss: 1.491479  [  900/10672]\n",
      "loss: 1.667020  [ 1000/10672]\n",
      "loss: 1.286691  [ 1100/10672]\n",
      "loss: 1.125962  [ 1200/10672]\n",
      "loss: 1.134637  [ 1300/10672]\n",
      "loss: 0.999632  [ 1400/10672]\n",
      "loss: 1.029564  [ 1500/10672]\n",
      "loss: 1.035982  [ 1600/10672]\n",
      "loss: 1.007718  [ 1700/10672]\n",
      "loss: 1.243834  [ 1800/10672]\n",
      "loss: 1.063568  [ 1900/10672]\n",
      "loss: 1.144400  [ 2000/10672]\n",
      "loss: 1.144638  [ 2100/10672]\n",
      "loss: 1.096217  [ 2200/10672]\n",
      "loss: 1.243147  [ 2300/10672]\n",
      "loss: 1.132727  [ 2400/10672]\n",
      "loss: 1.106987  [ 2500/10672]\n",
      "loss: 1.126844  [ 2600/10672]\n",
      "loss: 1.666019  [ 2700/10672]\n",
      "loss: 1.095120  [ 2800/10672]\n",
      "loss: 1.669174  [ 2900/10672]\n",
      "loss: 1.668808  [ 3000/10672]\n",
      "loss: 1.225801  [ 3100/10672]\n",
      "loss: 1.001747  [ 3200/10672]\n",
      "loss: 1.028043  [ 3300/10672]\n",
      "loss: 1.677180  [ 3400/10672]\n",
      "loss: 1.670668  [ 3500/10672]\n",
      "loss: 1.671667  [ 3600/10672]\n",
      "loss: 1.094407  [ 3700/10672]\n",
      "loss: 1.072660  [ 3800/10672]\n",
      "loss: 1.213261  [ 3900/10672]\n",
      "loss: 0.999166  [ 4000/10672]\n",
      "loss: 1.666793  [ 4100/10672]\n",
      "loss: 1.670817  [ 4200/10672]\n",
      "loss: 1.141339  [ 4300/10672]\n",
      "loss: 1.665969  [ 4400/10672]\n",
      "loss: 1.080238  [ 4500/10672]\n",
      "loss: 1.302538  [ 4600/10672]\n",
      "loss: 1.072462  [ 4700/10672]\n",
      "loss: 1.671132  [ 4800/10672]\n",
      "loss: 1.171744  [ 4900/10672]\n",
      "loss: 1.666266  [ 5000/10672]\n",
      "loss: 1.235713  [ 5100/10672]\n",
      "loss: 1.073487  [ 5200/10672]\n",
      "loss: 1.456853  [ 5300/10672]\n",
      "loss: 1.116303  [ 5400/10672]\n",
      "loss: 0.937705  [ 5500/10672]\n",
      "loss: 1.137061  [ 5600/10672]\n",
      "loss: 1.671521  [ 5700/10672]\n",
      "loss: 1.041465  [ 5800/10672]\n",
      "loss: 1.221379  [ 5900/10672]\n",
      "loss: 1.667933  [ 6000/10672]\n",
      "loss: 1.185405  [ 6100/10672]\n",
      "loss: 1.056362  [ 6200/10672]\n",
      "loss: 1.026306  [ 6300/10672]\n",
      "loss: 1.186139  [ 6400/10672]\n",
      "loss: 1.677092  [ 6500/10672]\n",
      "loss: 1.062186  [ 6600/10672]\n",
      "loss: 1.202859  [ 6700/10672]\n",
      "loss: 1.086609  [ 6800/10672]\n",
      "loss: 1.096358  [ 6900/10672]\n",
      "loss: 1.151766  [ 7000/10672]\n",
      "loss: 0.953844  [ 7100/10672]\n",
      "loss: 1.196670  [ 7200/10672]\n",
      "loss: 1.291343  [ 7300/10672]\n",
      "loss: 1.147010  [ 7400/10672]\n",
      "loss: 0.969515  [ 7500/10672]\n",
      "loss: 1.200102  [ 7600/10672]\n",
      "loss: 1.226928  [ 7700/10672]\n",
      "loss: 1.248273  [ 7800/10672]\n",
      "loss: 1.103754  [ 7900/10672]\n",
      "loss: 1.065937  [ 8000/10672]\n",
      "loss: 1.115304  [ 8100/10672]\n",
      "loss: 1.070879  [ 8200/10672]\n",
      "loss: 1.080585  [ 8300/10672]\n",
      "loss: 1.211064  [ 8400/10672]\n",
      "loss: 1.021906  [ 8500/10672]\n",
      "loss: 1.216042  [ 8600/10672]\n",
      "loss: 1.208420  [ 8700/10672]\n",
      "loss: 1.061018  [ 8800/10672]\n",
      "loss: 1.005913  [ 8900/10672]\n",
      "loss: 1.666949  [ 9000/10672]\n",
      "loss: 1.007616  [ 9100/10672]\n",
      "loss: 1.026271  [ 9200/10672]\n",
      "loss: 1.220971  [ 9300/10672]\n",
      "loss: 1.214769  [ 9400/10672]\n",
      "loss: 1.118654  [ 9500/10672]\n",
      "loss: 1.677530  [ 9600/10672]\n",
      "loss: 1.203045  [ 9700/10672]\n",
      "loss: 1.171261  [ 9800/10672]\n",
      "loss: 1.098331  [ 9900/10672]\n",
      "loss: 1.243940  [10000/10672]\n",
      "loss: 1.089563  [10100/10672]\n",
      "loss: 0.961408  [10200/10672]\n",
      "loss: 1.119681  [10300/10672]\n",
      "loss: 1.184104  [10400/10672]\n",
      "loss: 1.171892  [10500/10672]\n",
      "loss: 1.670467  [10600/10672]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.021448  [    0/10672]\n",
      "loss: 1.095879  [  100/10672]\n",
      "loss: 0.908276  [  200/10672]\n",
      "loss: 0.944752  [  300/10672]\n",
      "loss: 1.263587  [  400/10672]\n",
      "loss: 1.086339  [  500/10672]\n",
      "loss: 1.667094  [  600/10672]\n",
      "loss: 0.993386  [  700/10672]\n",
      "loss: 1.666462  [  800/10672]\n",
      "loss: 1.493443  [  900/10672]\n",
      "loss: 1.667029  [ 1000/10672]\n",
      "loss: 1.287095  [ 1100/10672]\n",
      "loss: 1.124910  [ 1200/10672]\n",
      "loss: 1.134691  [ 1300/10672]\n",
      "loss: 0.997923  [ 1400/10672]\n",
      "loss: 1.028805  [ 1500/10672]\n",
      "loss: 1.034408  [ 1600/10672]\n",
      "loss: 1.006782  [ 1700/10672]\n",
      "loss: 1.244119  [ 1800/10672]\n",
      "loss: 1.062288  [ 1900/10672]\n",
      "loss: 1.143644  [ 2000/10672]\n",
      "loss: 1.144127  [ 2100/10672]\n",
      "loss: 1.095237  [ 2200/10672]\n",
      "loss: 1.243454  [ 2300/10672]\n",
      "loss: 1.132571  [ 2400/10672]\n",
      "loss: 1.106657  [ 2500/10672]\n",
      "loss: 1.126831  [ 2600/10672]\n",
      "loss: 1.666036  [ 2700/10672]\n",
      "loss: 1.094653  [ 2800/10672]\n",
      "loss: 1.669263  [ 2900/10672]\n",
      "loss: 1.668852  [ 3000/10672]\n",
      "loss: 1.226193  [ 3100/10672]\n",
      "loss: 1.000317  [ 3200/10672]\n",
      "loss: 1.027102  [ 3300/10672]\n",
      "loss: 1.677354  [ 3400/10672]\n",
      "loss: 1.670743  [ 3500/10672]\n",
      "loss: 1.671779  [ 3600/10672]\n",
      "loss: 1.093406  [ 3700/10672]\n",
      "loss: 1.072122  [ 3800/10672]\n",
      "loss: 1.213198  [ 3900/10672]\n",
      "loss: 0.997440  [ 4000/10672]\n",
      "loss: 1.666818  [ 4100/10672]\n",
      "loss: 1.670948  [ 4200/10672]\n",
      "loss: 1.141274  [ 4300/10672]\n",
      "loss: 1.665984  [ 4400/10672]\n",
      "loss: 1.079079  [ 4500/10672]\n",
      "loss: 1.303462  [ 4600/10672]\n",
      "loss: 1.071417  [ 4700/10672]\n",
      "loss: 1.671217  [ 4800/10672]\n",
      "loss: 1.172101  [ 4900/10672]\n",
      "loss: 1.666290  [ 5000/10672]\n",
      "loss: 1.236707  [ 5100/10672]\n",
      "loss: 1.072737  [ 5200/10672]\n",
      "loss: 1.458929  [ 5300/10672]\n",
      "loss: 1.116083  [ 5400/10672]\n",
      "loss: 0.935919  [ 5500/10672]\n",
      "loss: 1.136906  [ 5600/10672]\n",
      "loss: 1.671607  [ 5700/10672]\n",
      "loss: 1.040039  [ 5800/10672]\n",
      "loss: 1.222110  [ 5900/10672]\n",
      "loss: 1.667985  [ 6000/10672]\n",
      "loss: 1.185337  [ 6100/10672]\n",
      "loss: 1.055607  [ 6200/10672]\n",
      "loss: 1.025125  [ 6300/10672]\n",
      "loss: 1.186738  [ 6400/10672]\n",
      "loss: 1.677252  [ 6500/10672]\n",
      "loss: 1.061669  [ 6600/10672]\n",
      "loss: 1.202856  [ 6700/10672]\n",
      "loss: 1.086126  [ 6800/10672]\n",
      "loss: 1.095222  [ 6900/10672]\n",
      "loss: 1.151990  [ 7000/10672]\n",
      "loss: 0.952570  [ 7100/10672]\n",
      "loss: 1.196543  [ 7200/10672]\n",
      "loss: 1.292647  [ 7300/10672]\n",
      "loss: 1.146594  [ 7400/10672]\n",
      "loss: 0.968219  [ 7500/10672]\n",
      "loss: 1.199951  [ 7600/10672]\n",
      "loss: 1.227084  [ 7700/10672]\n",
      "loss: 1.248506  [ 7800/10672]\n",
      "loss: 1.103414  [ 7900/10672]\n",
      "loss: 1.065382  [ 8000/10672]\n",
      "loss: 1.114937  [ 8100/10672]\n",
      "loss: 1.069761  [ 8200/10672]\n",
      "loss: 1.079540  [ 8300/10672]\n",
      "loss: 1.211751  [ 8400/10672]\n",
      "loss: 1.021092  [ 8500/10672]\n",
      "loss: 1.216776  [ 8600/10672]\n",
      "loss: 1.208658  [ 8700/10672]\n",
      "loss: 1.060011  [ 8800/10672]\n",
      "loss: 1.004915  [ 8900/10672]\n",
      "loss: 1.666986  [ 9000/10672]\n",
      "loss: 1.005871  [ 9100/10672]\n",
      "loss: 1.025324  [ 9200/10672]\n",
      "loss: 1.221803  [ 9300/10672]\n",
      "loss: 1.215573  [ 9400/10672]\n",
      "loss: 1.117796  [ 9500/10672]\n",
      "loss: 1.677668  [ 9600/10672]\n",
      "loss: 1.202774  [ 9700/10672]\n",
      "loss: 1.171745  [ 9800/10672]\n",
      "loss: 1.098149  [ 9900/10672]\n",
      "loss: 1.245037  [10000/10672]\n",
      "loss: 1.089122  [10100/10672]\n",
      "loss: 0.960167  [10200/10672]\n",
      "loss: 1.118856  [10300/10672]\n",
      "loss: 1.183748  [10400/10672]\n",
      "loss: 1.171398  [10500/10672]\n",
      "loss: 1.670542  [10600/10672]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.020809  [    0/10672]\n",
      "loss: 1.095436  [  100/10672]\n",
      "loss: 0.906844  [  200/10672]\n",
      "loss: 0.943438  [  300/10672]\n",
      "loss: 1.263786  [  400/10672]\n",
      "loss: 1.085199  [  500/10672]\n",
      "loss: 1.667107  [  600/10672]\n",
      "loss: 0.992368  [  700/10672]\n",
      "loss: 1.666471  [  800/10672]\n",
      "loss: 1.495387  [  900/10672]\n",
      "loss: 1.667038  [ 1000/10672]\n",
      "loss: 1.287496  [ 1100/10672]\n",
      "loss: 1.123863  [ 1200/10672]\n",
      "loss: 1.134741  [ 1300/10672]\n",
      "loss: 0.996233  [ 1400/10672]\n",
      "loss: 1.028052  [ 1500/10672]\n",
      "loss: 1.032847  [ 1600/10672]\n",
      "loss: 1.005852  [ 1700/10672]\n",
      "loss: 1.244402  [ 1800/10672]\n",
      "loss: 1.061019  [ 1900/10672]\n",
      "loss: 1.142890  [ 2000/10672]\n",
      "loss: 1.143622  [ 2100/10672]\n",
      "loss: 1.094264  [ 2200/10672]\n",
      "loss: 1.243757  [ 2300/10672]\n",
      "loss: 1.132415  [ 2400/10672]\n",
      "loss: 1.106328  [ 2500/10672]\n",
      "loss: 1.126822  [ 2600/10672]\n",
      "loss: 1.666052  [ 2700/10672]\n",
      "loss: 1.094190  [ 2800/10672]\n",
      "loss: 1.669352  [ 2900/10672]\n",
      "loss: 1.668895  [ 3000/10672]\n",
      "loss: 1.226584  [ 3100/10672]\n",
      "loss: 0.998902  [ 3200/10672]\n",
      "loss: 1.026168  [ 3300/10672]\n",
      "loss: 1.677527  [ 3400/10672]\n",
      "loss: 1.670818  [ 3500/10672]\n",
      "loss: 1.671890  [ 3600/10672]\n",
      "loss: 1.092411  [ 3700/10672]\n",
      "loss: 1.071585  [ 3800/10672]\n",
      "loss: 1.213135  [ 3900/10672]\n",
      "loss: 0.995730  [ 4000/10672]\n",
      "loss: 1.666842  [ 4100/10672]\n",
      "loss: 1.671079  [ 4200/10672]\n",
      "loss: 1.141206  [ 4300/10672]\n",
      "loss: 1.665998  [ 4400/10672]\n",
      "loss: 1.077926  [ 4500/10672]\n",
      "loss: 1.304381  [ 4600/10672]\n",
      "loss: 1.070381  [ 4700/10672]\n",
      "loss: 1.671303  [ 4800/10672]\n",
      "loss: 1.172456  [ 4900/10672]\n",
      "loss: 1.666314  [ 5000/10672]\n",
      "loss: 1.237698  [ 5100/10672]\n",
      "loss: 1.071989  [ 5200/10672]\n",
      "loss: 1.460984  [ 5300/10672]\n",
      "loss: 1.115863  [ 5400/10672]\n",
      "loss: 0.934156  [ 5500/10672]\n",
      "loss: 1.136747  [ 5600/10672]\n",
      "loss: 1.671693  [ 5700/10672]\n",
      "loss: 1.038623  [ 5800/10672]\n",
      "loss: 1.222837  [ 5900/10672]\n",
      "loss: 1.668037  [ 6000/10672]\n",
      "loss: 1.185269  [ 6100/10672]\n",
      "loss: 1.054858  [ 6200/10672]\n",
      "loss: 1.023956  [ 6300/10672]\n",
      "loss: 1.187338  [ 6400/10672]\n",
      "loss: 1.677411  [ 6500/10672]\n",
      "loss: 1.061159  [ 6600/10672]\n",
      "loss: 1.202851  [ 6700/10672]\n",
      "loss: 1.085647  [ 6800/10672]\n",
      "loss: 1.094088  [ 6900/10672]\n",
      "loss: 1.152214  [ 7000/10672]\n",
      "loss: 0.951310  [ 7100/10672]\n",
      "loss: 1.196414  [ 7200/10672]\n",
      "loss: 1.293940  [ 7300/10672]\n",
      "loss: 1.146181  [ 7400/10672]\n",
      "loss: 0.966936  [ 7500/10672]\n",
      "loss: 1.199798  [ 7600/10672]\n",
      "loss: 1.227238  [ 7700/10672]\n",
      "loss: 1.248733  [ 7800/10672]\n",
      "loss: 1.103075  [ 7900/10672]\n",
      "loss: 1.064830  [ 8000/10672]\n",
      "loss: 1.114569  [ 8100/10672]\n",
      "loss: 1.068652  [ 8200/10672]\n",
      "loss: 1.078502  [ 8300/10672]\n",
      "loss: 1.212436  [ 8400/10672]\n",
      "loss: 1.020286  [ 8500/10672]\n",
      "loss: 1.217507  [ 8600/10672]\n",
      "loss: 1.208900  [ 8700/10672]\n",
      "loss: 1.059014  [ 8800/10672]\n",
      "loss: 1.003925  [ 8900/10672]\n",
      "loss: 1.667022  [ 9000/10672]\n",
      "loss: 1.004141  [ 9100/10672]\n",
      "loss: 1.024380  [ 9200/10672]\n",
      "loss: 1.222628  [ 9300/10672]\n",
      "loss: 1.216372  [ 9400/10672]\n",
      "loss: 1.116945  [ 9500/10672]\n",
      "loss: 1.677805  [ 9600/10672]\n",
      "loss: 1.202502  [ 9700/10672]\n",
      "loss: 1.172230  [ 9800/10672]\n",
      "loss: 1.097969  [ 9900/10672]\n",
      "loss: 1.246128  [10000/10672]\n",
      "loss: 1.088680  [10100/10672]\n",
      "loss: 0.958938  [10200/10672]\n",
      "loss: 1.118038  [10300/10672]\n",
      "loss: 1.183396  [10400/10672]\n",
      "loss: 1.170907  [10500/10672]\n",
      "loss: 1.670616  [10600/10672]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.020176  [    0/10672]\n",
      "loss: 1.094992  [  100/10672]\n",
      "loss: 0.905429  [  200/10672]\n",
      "loss: 0.942137  [  300/10672]\n",
      "loss: 1.263980  [  400/10672]\n",
      "loss: 1.084069  [  500/10672]\n",
      "loss: 1.667120  [  600/10672]\n",
      "loss: 0.991356  [  700/10672]\n",
      "loss: 1.666481  [  800/10672]\n",
      "loss: 1.497309  [  900/10672]\n",
      "loss: 1.667047  [ 1000/10672]\n",
      "loss: 1.287889  [ 1100/10672]\n",
      "loss: 1.122817  [ 1200/10672]\n",
      "loss: 1.134789  [ 1300/10672]\n",
      "loss: 0.994560  [ 1400/10672]\n",
      "loss: 1.027304  [ 1500/10672]\n",
      "loss: 1.031299  [ 1600/10672]\n",
      "loss: 1.004927  [ 1700/10672]\n",
      "loss: 1.244684  [ 1800/10672]\n",
      "loss: 1.059759  [ 1900/10672]\n",
      "loss: 1.142138  [ 2000/10672]\n",
      "loss: 1.143123  [ 2100/10672]\n",
      "loss: 1.093300  [ 2200/10672]\n",
      "loss: 1.244059  [ 2300/10672]\n",
      "loss: 1.132255  [ 2400/10672]\n",
      "loss: 1.105997  [ 2500/10672]\n",
      "loss: 1.126811  [ 2600/10672]\n",
      "loss: 1.666068  [ 2700/10672]\n",
      "loss: 1.093724  [ 2800/10672]\n",
      "loss: 1.669440  [ 2900/10672]\n",
      "loss: 1.668939  [ 3000/10672]\n",
      "loss: 1.226978  [ 3100/10672]\n",
      "loss: 0.997505  [ 3200/10672]\n",
      "loss: 1.025238  [ 3300/10672]\n",
      "loss: 1.677700  [ 3400/10672]\n",
      "loss: 1.670893  [ 3500/10672]\n",
      "loss: 1.672000  [ 3600/10672]\n",
      "loss: 1.091424  [ 3700/10672]\n",
      "loss: 1.071049  [ 3800/10672]\n",
      "loss: 1.213071  [ 3900/10672]\n",
      "loss: 0.994039  [ 4000/10672]\n",
      "loss: 1.666867  [ 4100/10672]\n",
      "loss: 1.671209  [ 4200/10672]\n",
      "loss: 1.141134  [ 4300/10672]\n",
      "loss: 1.666011  [ 4400/10672]\n",
      "loss: 1.076782  [ 4500/10672]\n",
      "loss: 1.305297  [ 4600/10672]\n",
      "loss: 1.069357  [ 4700/10672]\n",
      "loss: 1.671388  [ 4800/10672]\n",
      "loss: 1.172807  [ 4900/10672]\n",
      "loss: 1.666338  [ 5000/10672]\n",
      "loss: 1.238684  [ 5100/10672]\n",
      "loss: 1.071241  [ 5200/10672]\n",
      "loss: 1.463022  [ 5300/10672]\n",
      "loss: 1.115640  [ 5400/10672]\n",
      "loss: 0.932416  [ 5500/10672]\n",
      "loss: 1.136585  [ 5600/10672]\n",
      "loss: 1.671778  [ 5700/10672]\n",
      "loss: 1.037218  [ 5800/10672]\n",
      "loss: 1.223557  [ 5900/10672]\n",
      "loss: 1.668089  [ 6000/10672]\n",
      "loss: 1.185205  [ 6100/10672]\n",
      "loss: 1.054112  [ 6200/10672]\n",
      "loss: 1.022803  [ 6300/10672]\n",
      "loss: 1.187934  [ 6400/10672]\n",
      "loss: 1.677571  [ 6500/10672]\n",
      "loss: 1.060651  [ 6600/10672]\n",
      "loss: 1.202851  [ 6700/10672]\n",
      "loss: 1.085169  [ 6800/10672]\n",
      "loss: 1.092961  [ 6900/10672]\n",
      "loss: 1.152437  [ 7000/10672]\n",
      "loss: 0.950062  [ 7100/10672]\n",
      "loss: 1.196286  [ 7200/10672]\n",
      "loss: 1.295224  [ 7300/10672]\n",
      "loss: 1.145772  [ 7400/10672]\n",
      "loss: 0.965665  [ 7500/10672]\n",
      "loss: 1.199646  [ 7600/10672]\n",
      "loss: 1.227391  [ 7700/10672]\n",
      "loss: 1.248957  [ 7800/10672]\n",
      "loss: 1.102733  [ 7900/10672]\n",
      "loss: 1.064280  [ 8000/10672]\n",
      "loss: 1.114198  [ 8100/10672]\n",
      "loss: 1.067555  [ 8200/10672]\n",
      "loss: 1.077476  [ 8300/10672]\n",
      "loss: 1.213114  [ 8400/10672]\n",
      "loss: 1.019484  [ 8500/10672]\n",
      "loss: 1.218233  [ 8600/10672]\n",
      "loss: 1.209147  [ 8700/10672]\n",
      "loss: 1.058028  [ 8800/10672]\n",
      "loss: 1.002942  [ 8900/10672]\n",
      "loss: 1.667058  [ 9000/10672]\n",
      "loss: 1.002426  [ 9100/10672]\n",
      "loss: 1.023444  [ 9200/10672]\n",
      "loss: 1.223448  [ 9300/10672]\n",
      "loss: 1.217166  [ 9400/10672]\n",
      "loss: 1.116101  [ 9500/10672]\n",
      "loss: 1.677942  [ 9600/10672]\n",
      "loss: 1.202230  [ 9700/10672]\n",
      "loss: 1.172711  [ 9800/10672]\n",
      "loss: 1.097790  [ 9900/10672]\n",
      "loss: 1.247209  [10000/10672]\n",
      "loss: 1.088236  [10100/10672]\n",
      "loss: 0.957719  [10200/10672]\n",
      "loss: 1.117228  [10300/10672]\n",
      "loss: 1.183048  [10400/10672]\n",
      "loss: 1.170421  [10500/10672]\n",
      "loss: 1.670690  [10600/10672]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.019550  [    0/10672]\n",
      "loss: 1.094544  [  100/10672]\n",
      "loss: 0.904030  [  200/10672]\n",
      "loss: 0.940847  [  300/10672]\n",
      "loss: 1.264173  [  400/10672]\n",
      "loss: 1.082950  [  500/10672]\n",
      "loss: 1.667132  [  600/10672]\n",
      "loss: 0.990349  [  700/10672]\n",
      "loss: 1.666491  [  800/10672]\n",
      "loss: 1.499211  [  900/10672]\n",
      "loss: 1.667055  [ 1000/10672]\n",
      "loss: 1.288279  [ 1100/10672]\n",
      "loss: 1.121778  [ 1200/10672]\n",
      "loss: 1.134833  [ 1300/10672]\n",
      "loss: 0.992906  [ 1400/10672]\n",
      "loss: 1.026560  [ 1500/10672]\n",
      "loss: 1.029765  [ 1600/10672]\n",
      "loss: 1.004006  [ 1700/10672]\n",
      "loss: 1.244968  [ 1800/10672]\n",
      "loss: 1.058513  [ 1900/10672]\n",
      "loss: 1.141392  [ 2000/10672]\n",
      "loss: 1.142632  [ 2100/10672]\n",
      "loss: 1.092345  [ 2200/10672]\n",
      "loss: 1.244361  [ 2300/10672]\n",
      "loss: 1.132091  [ 2400/10672]\n",
      "loss: 1.105666  [ 2500/10672]\n",
      "loss: 1.126802  [ 2600/10672]\n",
      "loss: 1.666084  [ 2700/10672]\n",
      "loss: 1.093259  [ 2800/10672]\n",
      "loss: 1.669528  [ 2900/10672]\n",
      "loss: 1.668982  [ 3000/10672]\n",
      "loss: 1.227373  [ 3100/10672]\n",
      "loss: 0.996124  [ 3200/10672]\n",
      "loss: 1.024313  [ 3300/10672]\n",
      "loss: 1.677873  [ 3400/10672]\n",
      "loss: 1.670969  [ 3500/10672]\n",
      "loss: 1.672110  [ 3600/10672]\n",
      "loss: 1.090445  [ 3700/10672]\n",
      "loss: 1.070515  [ 3800/10672]\n",
      "loss: 1.213008  [ 3900/10672]\n",
      "loss: 0.992364  [ 4000/10672]\n",
      "loss: 1.666891  [ 4100/10672]\n",
      "loss: 1.671340  [ 4200/10672]\n",
      "loss: 1.141057  [ 4300/10672]\n",
      "loss: 1.666025  [ 4400/10672]\n",
      "loss: 1.075646  [ 4500/10672]\n",
      "loss: 1.306210  [ 4600/10672]\n",
      "loss: 1.068344  [ 4700/10672]\n",
      "loss: 1.671473  [ 4800/10672]\n",
      "loss: 1.173155  [ 4900/10672]\n",
      "loss: 1.666361  [ 5000/10672]\n",
      "loss: 1.239664  [ 5100/10672]\n",
      "loss: 1.070493  [ 5200/10672]\n",
      "loss: 1.465044  [ 5300/10672]\n",
      "loss: 1.115415  [ 5400/10672]\n",
      "loss: 0.930701  [ 5500/10672]\n",
      "loss: 1.136417  [ 5600/10672]\n",
      "loss: 1.671864  [ 5700/10672]\n",
      "loss: 1.035825  [ 5800/10672]\n",
      "loss: 1.224271  [ 5900/10672]\n",
      "loss: 1.668142  [ 6000/10672]\n",
      "loss: 1.185145  [ 6100/10672]\n",
      "loss: 1.053368  [ 6200/10672]\n",
      "loss: 1.021665  [ 6300/10672]\n",
      "loss: 1.188526  [ 6400/10672]\n",
      "loss: 1.677730  [ 6500/10672]\n",
      "loss: 1.060146  [ 6600/10672]\n",
      "loss: 1.202854  [ 6700/10672]\n",
      "loss: 1.084691  [ 6800/10672]\n",
      "loss: 1.091839  [ 6900/10672]\n",
      "loss: 1.152658  [ 7000/10672]\n",
      "loss: 0.948826  [ 7100/10672]\n",
      "loss: 1.196161  [ 7200/10672]\n",
      "loss: 1.296496  [ 7300/10672]\n",
      "loss: 1.145367  [ 7400/10672]\n",
      "loss: 0.964405  [ 7500/10672]\n",
      "loss: 1.199495  [ 7600/10672]\n",
      "loss: 1.227543  [ 7700/10672]\n",
      "loss: 1.249178  [ 7800/10672]\n",
      "loss: 1.102392  [ 7900/10672]\n",
      "loss: 1.063730  [ 8000/10672]\n",
      "loss: 1.113824  [ 8100/10672]\n",
      "loss: 1.066471  [ 8200/10672]\n",
      "loss: 1.076460  [ 8300/10672]\n",
      "loss: 1.213786  [ 8400/10672]\n",
      "loss: 1.018688  [ 8500/10672]\n",
      "loss: 1.218952  [ 8600/10672]\n",
      "loss: 1.209399  [ 8700/10672]\n",
      "loss: 1.057055  [ 8800/10672]\n",
      "loss: 1.001966  [ 8900/10672]\n",
      "loss: 1.667093  [ 9000/10672]\n",
      "loss: 1.000728  [ 9100/10672]\n",
      "loss: 1.022512  [ 9200/10672]\n",
      "loss: 1.224261  [ 9300/10672]\n",
      "loss: 1.217955  [ 9400/10672]\n",
      "loss: 1.115265  [ 9500/10672]\n",
      "loss: 1.678079  [ 9600/10672]\n",
      "loss: 1.201958  [ 9700/10672]\n",
      "loss: 1.173190  [ 9800/10672]\n",
      "loss: 1.097612  [ 9900/10672]\n",
      "loss: 1.248284  [10000/10672]\n",
      "loss: 1.087792  [10100/10672]\n",
      "loss: 0.956511  [10200/10672]\n",
      "loss: 1.116424  [10300/10672]\n",
      "loss: 1.182703  [10400/10672]\n",
      "loss: 1.169938  [10500/10672]\n",
      "loss: 1.670763  [10600/10672]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.018930  [    0/10672]\n",
      "loss: 1.094094  [  100/10672]\n",
      "loss: 0.902649  [  200/10672]\n",
      "loss: 0.939570  [  300/10672]\n",
      "loss: 1.264364  [  400/10672]\n",
      "loss: 1.081840  [  500/10672]\n",
      "loss: 1.667145  [  600/10672]\n",
      "loss: 0.989348  [  700/10672]\n",
      "loss: 1.666500  [  800/10672]\n",
      "loss: 1.501092  [  900/10672]\n",
      "loss: 1.667063  [ 1000/10672]\n",
      "loss: 1.288664  [ 1100/10672]\n",
      "loss: 1.120741  [ 1200/10672]\n",
      "loss: 1.134874  [ 1300/10672]\n",
      "loss: 0.991270  [ 1400/10672]\n",
      "loss: 1.025821  [ 1500/10672]\n",
      "loss: 1.028245  [ 1600/10672]\n",
      "loss: 1.003092  [ 1700/10672]\n",
      "loss: 1.245250  [ 1800/10672]\n",
      "loss: 1.057277  [ 1900/10672]\n",
      "loss: 1.140648  [ 2000/10672]\n",
      "loss: 1.142146  [ 2100/10672]\n",
      "loss: 1.091399  [ 2200/10672]\n",
      "loss: 1.244660  [ 2300/10672]\n",
      "loss: 1.131926  [ 2400/10672]\n",
      "loss: 1.105334  [ 2500/10672]\n",
      "loss: 1.126793  [ 2600/10672]\n",
      "loss: 1.666100  [ 2700/10672]\n",
      "loss: 1.092794  [ 2800/10672]\n",
      "loss: 1.669616  [ 2900/10672]\n",
      "loss: 1.669024  [ 3000/10672]\n",
      "loss: 1.227770  [ 3100/10672]\n",
      "loss: 0.994759  [ 3200/10672]\n",
      "loss: 1.023393  [ 3300/10672]\n",
      "loss: 1.678046  [ 3400/10672]\n",
      "loss: 1.671043  [ 3500/10672]\n",
      "loss: 1.672219  [ 3600/10672]\n",
      "loss: 1.089472  [ 3700/10672]\n",
      "loss: 1.069982  [ 3800/10672]\n",
      "loss: 1.212946  [ 3900/10672]\n",
      "loss: 0.990707  [ 4000/10672]\n",
      "loss: 1.666915  [ 4100/10672]\n",
      "loss: 1.671471  [ 4200/10672]\n",
      "loss: 1.140978  [ 4300/10672]\n",
      "loss: 1.666038  [ 4400/10672]\n",
      "loss: 1.074517  [ 4500/10672]\n",
      "loss: 1.307118  [ 4600/10672]\n",
      "loss: 1.067339  [ 4700/10672]\n",
      "loss: 1.671558  [ 4800/10672]\n",
      "loss: 1.173502  [ 4900/10672]\n",
      "loss: 1.666384  [ 5000/10672]\n",
      "loss: 1.240642  [ 5100/10672]\n",
      "loss: 1.069746  [ 5200/10672]\n",
      "loss: 1.467046  [ 5300/10672]\n",
      "loss: 1.115190  [ 5400/10672]\n",
      "loss: 0.929007  [ 5500/10672]\n",
      "loss: 1.136248  [ 5600/10672]\n",
      "loss: 1.671949  [ 5700/10672]\n",
      "loss: 1.034440  [ 5800/10672]\n",
      "loss: 1.224982  [ 5900/10672]\n",
      "loss: 1.668193  [ 6000/10672]\n",
      "loss: 1.185086  [ 6100/10672]\n",
      "loss: 1.052631  [ 6200/10672]\n",
      "loss: 1.020539  [ 6300/10672]\n",
      "loss: 1.189119  [ 6400/10672]\n",
      "loss: 1.677889  [ 6500/10672]\n",
      "loss: 1.059646  [ 6600/10672]\n",
      "loss: 1.202856  [ 6700/10672]\n",
      "loss: 1.084217  [ 6800/10672]\n",
      "loss: 1.090720  [ 6900/10672]\n",
      "loss: 1.152882  [ 7000/10672]\n",
      "loss: 0.947604  [ 7100/10672]\n",
      "loss: 1.196034  [ 7200/10672]\n",
      "loss: 1.297760  [ 7300/10672]\n",
      "loss: 1.144964  [ 7400/10672]\n",
      "loss: 0.963158  [ 7500/10672]\n",
      "loss: 1.199343  [ 7600/10672]\n",
      "loss: 1.227692  [ 7700/10672]\n",
      "loss: 1.249392  [ 7800/10672]\n",
      "loss: 1.102051  [ 7900/10672]\n",
      "loss: 1.063186  [ 8000/10672]\n",
      "loss: 1.113451  [ 8100/10672]\n",
      "loss: 1.065395  [ 8200/10672]\n",
      "loss: 1.075451  [ 8300/10672]\n",
      "loss: 1.214458  [ 8400/10672]\n",
      "loss: 1.017900  [ 8500/10672]\n",
      "loss: 1.219670  [ 8600/10672]\n",
      "loss: 1.209653  [ 8700/10672]\n",
      "loss: 1.056090  [ 8800/10672]\n",
      "loss: 1.000999  [ 8900/10672]\n",
      "loss: 1.667129  [ 9000/10672]\n",
      "loss: 0.999042  [ 9100/10672]\n",
      "loss: 1.021587  [ 9200/10672]\n",
      "loss: 1.225071  [ 9300/10672]\n",
      "loss: 1.218740  [ 9400/10672]\n",
      "loss: 1.114436  [ 9500/10672]\n",
      "loss: 1.678215  [ 9600/10672]\n",
      "loss: 1.201683  [ 9700/10672]\n",
      "loss: 1.173668  [ 9800/10672]\n",
      "loss: 1.097436  [ 9900/10672]\n",
      "loss: 1.249354  [10000/10672]\n",
      "loss: 1.087350  [10100/10672]\n",
      "loss: 0.955315  [10200/10672]\n",
      "loss: 1.115626  [10300/10672]\n",
      "loss: 1.182359  [10400/10672]\n",
      "loss: 1.169456  [10500/10672]\n",
      "loss: 1.670836  [10600/10672]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.018319  [    0/10672]\n",
      "loss: 1.093644  [  100/10672]\n",
      "loss: 0.901285  [  200/10672]\n",
      "loss: 0.938305  [  300/10672]\n",
      "loss: 1.264549  [  400/10672]\n",
      "loss: 1.080738  [  500/10672]\n",
      "loss: 1.667157  [  600/10672]\n",
      "loss: 0.988355  [  700/10672]\n",
      "loss: 1.666509  [  800/10672]\n",
      "loss: 1.502951  [  900/10672]\n",
      "loss: 1.667071  [ 1000/10672]\n",
      "loss: 1.289042  [ 1100/10672]\n",
      "loss: 1.119706  [ 1200/10672]\n",
      "loss: 1.134914  [ 1300/10672]\n",
      "loss: 0.989650  [ 1400/10672]\n",
      "loss: 1.025089  [ 1500/10672]\n",
      "loss: 1.026736  [ 1600/10672]\n",
      "loss: 1.002185  [ 1700/10672]\n",
      "loss: 1.245528  [ 1800/10672]\n",
      "loss: 1.056048  [ 1900/10672]\n",
      "loss: 1.139904  [ 2000/10672]\n",
      "loss: 1.141665  [ 2100/10672]\n",
      "loss: 1.090459  [ 2200/10672]\n",
      "loss: 1.244957  [ 2300/10672]\n",
      "loss: 1.131759  [ 2400/10672]\n",
      "loss: 1.105004  [ 2500/10672]\n",
      "loss: 1.126787  [ 2600/10672]\n",
      "loss: 1.666115  [ 2700/10672]\n",
      "loss: 1.092332  [ 2800/10672]\n",
      "loss: 1.669704  [ 2900/10672]\n",
      "loss: 1.669066  [ 3000/10672]\n",
      "loss: 1.228165  [ 3100/10672]\n",
      "loss: 0.993408  [ 3200/10672]\n",
      "loss: 1.022481  [ 3300/10672]\n",
      "loss: 1.678218  [ 3400/10672]\n",
      "loss: 1.671118  [ 3500/10672]\n",
      "loss: 1.672328  [ 3600/10672]\n",
      "loss: 1.088505  [ 3700/10672]\n",
      "loss: 1.069451  [ 3800/10672]\n",
      "loss: 1.212882  [ 3900/10672]\n",
      "loss: 0.989065  [ 4000/10672]\n",
      "loss: 1.666938  [ 4100/10672]\n",
      "loss: 1.671602  [ 4200/10672]\n",
      "loss: 1.140894  [ 4300/10672]\n",
      "loss: 1.666050  [ 4400/10672]\n",
      "loss: 1.073397  [ 4500/10672]\n",
      "loss: 1.308023  [ 4600/10672]\n",
      "loss: 1.066346  [ 4700/10672]\n",
      "loss: 1.671643  [ 4800/10672]\n",
      "loss: 1.173844  [ 4900/10672]\n",
      "loss: 1.666406  [ 5000/10672]\n",
      "loss: 1.241612  [ 5100/10672]\n",
      "loss: 1.068999  [ 5200/10672]\n",
      "loss: 1.469031  [ 5300/10672]\n",
      "loss: 1.114962  [ 5400/10672]\n",
      "loss: 0.927336  [ 5500/10672]\n",
      "loss: 1.136074  [ 5600/10672]\n",
      "loss: 1.672034  [ 5700/10672]\n",
      "loss: 1.033067  [ 5800/10672]\n",
      "loss: 1.225685  [ 5900/10672]\n",
      "loss: 1.668245  [ 6000/10672]\n",
      "loss: 1.185030  [ 6100/10672]\n",
      "loss: 1.051896  [ 6200/10672]\n",
      "loss: 1.019429  [ 6300/10672]\n",
      "loss: 1.189708  [ 6400/10672]\n",
      "loss: 1.678048  [ 6500/10672]\n",
      "loss: 1.059149  [ 6600/10672]\n",
      "loss: 1.202862  [ 6700/10672]\n",
      "loss: 1.083743  [ 6800/10672]\n",
      "loss: 1.089609  [ 6900/10672]\n",
      "loss: 1.153102  [ 7000/10672]\n",
      "loss: 0.946394  [ 7100/10672]\n",
      "loss: 1.195909  [ 7200/10672]\n",
      "loss: 1.299012  [ 7300/10672]\n",
      "loss: 1.144565  [ 7400/10672]\n",
      "loss: 0.961922  [ 7500/10672]\n",
      "loss: 1.199192  [ 7600/10672]\n",
      "loss: 1.227840  [ 7700/10672]\n",
      "loss: 1.249604  [ 7800/10672]\n",
      "loss: 1.101707  [ 7900/10672]\n",
      "loss: 1.062641  [ 8000/10672]\n",
      "loss: 1.113072  [ 8100/10672]\n",
      "loss: 1.064331  [ 8200/10672]\n",
      "loss: 1.074454  [ 8300/10672]\n",
      "loss: 1.215121  [ 8400/10672]\n",
      "loss: 1.017116  [ 8500/10672]\n",
      "loss: 1.220380  [ 8600/10672]\n",
      "loss: 1.209913  [ 8700/10672]\n",
      "loss: 1.055137  [ 8800/10672]\n",
      "loss: 1.000037  [ 8900/10672]\n",
      "loss: 1.667164  [ 9000/10672]\n",
      "loss: 0.997372  [ 9100/10672]\n",
      "loss: 1.020666  [ 9200/10672]\n",
      "loss: 1.225874  [ 9300/10672]\n",
      "loss: 1.219519  [ 9400/10672]\n",
      "loss: 1.113615  [ 9500/10672]\n",
      "loss: 1.678350  [ 9600/10672]\n",
      "loss: 1.201410  [ 9700/10672]\n",
      "loss: 1.174144  [ 9800/10672]\n",
      "loss: 1.097261  [ 9900/10672]\n",
      "loss: 1.250414  [10000/10672]\n",
      "loss: 1.086904  [10100/10672]\n",
      "loss: 0.954128  [10200/10672]\n",
      "loss: 1.114837  [10300/10672]\n",
      "loss: 1.182022  [10400/10672]\n",
      "loss: 1.168980  [10500/10672]\n",
      "loss: 1.670908  [10600/10672]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.017712  [    0/10672]\n",
      "loss: 1.093188  [  100/10672]\n",
      "loss: 0.899936  [  200/10672]\n",
      "loss: 0.937050  [  300/10672]\n",
      "loss: 1.264734  [  400/10672]\n",
      "loss: 1.079648  [  500/10672]\n",
      "loss: 1.667168  [  600/10672]\n",
      "loss: 0.987366  [  700/10672]\n",
      "loss: 1.666517  [  800/10672]\n",
      "loss: 1.504792  [  900/10672]\n",
      "loss: 1.667079  [ 1000/10672]\n",
      "loss: 1.289418  [ 1100/10672]\n",
      "loss: 1.118677  [ 1200/10672]\n",
      "loss: 1.134949  [ 1300/10672]\n",
      "loss: 0.988049  [ 1400/10672]\n",
      "loss: 1.024359  [ 1500/10672]\n",
      "loss: 1.025242  [ 1600/10672]\n",
      "loss: 1.001281  [ 1700/10672]\n",
      "loss: 1.245808  [ 1800/10672]\n",
      "loss: 1.054832  [ 1900/10672]\n",
      "loss: 1.139166  [ 2000/10672]\n",
      "loss: 1.141192  [ 2100/10672]\n",
      "loss: 1.089529  [ 2200/10672]\n",
      "loss: 1.245254  [ 2300/10672]\n",
      "loss: 1.131588  [ 2400/10672]\n",
      "loss: 1.104673  [ 2500/10672]\n",
      "loss: 1.126781  [ 2600/10672]\n",
      "loss: 1.666131  [ 2700/10672]\n",
      "loss: 1.091868  [ 2800/10672]\n",
      "loss: 1.669791  [ 2900/10672]\n",
      "loss: 1.669108  [ 3000/10672]\n",
      "loss: 1.228563  [ 3100/10672]\n",
      "loss: 0.992072  [ 3200/10672]\n",
      "loss: 1.021573  [ 3300/10672]\n",
      "loss: 1.678391  [ 3400/10672]\n",
      "loss: 1.671193  [ 3500/10672]\n",
      "loss: 1.672436  [ 3600/10672]\n",
      "loss: 1.087545  [ 3700/10672]\n",
      "loss: 1.068923  [ 3800/10672]\n",
      "loss: 1.212818  [ 3900/10672]\n",
      "loss: 0.987440  [ 4000/10672]\n",
      "loss: 1.666962  [ 4100/10672]\n",
      "loss: 1.671732  [ 4200/10672]\n",
      "loss: 1.140807  [ 4300/10672]\n",
      "loss: 1.666063  [ 4400/10672]\n",
      "loss: 1.072283  [ 4500/10672]\n",
      "loss: 1.308923  [ 4600/10672]\n",
      "loss: 1.065361  [ 4700/10672]\n",
      "loss: 1.671728  [ 4800/10672]\n",
      "loss: 1.174185  [ 4900/10672]\n",
      "loss: 1.666428  [ 5000/10672]\n",
      "loss: 1.242579  [ 5100/10672]\n",
      "loss: 1.068254  [ 5200/10672]\n",
      "loss: 1.470999  [ 5300/10672]\n",
      "loss: 1.114733  [ 5400/10672]\n",
      "loss: 0.925688  [ 5500/10672]\n",
      "loss: 1.135896  [ 5600/10672]\n",
      "loss: 1.672118  [ 5700/10672]\n",
      "loss: 1.031703  [ 5800/10672]\n",
      "loss: 1.226384  [ 5900/10672]\n",
      "loss: 1.668297  [ 6000/10672]\n",
      "loss: 1.184976  [ 6100/10672]\n",
      "loss: 1.051165  [ 6200/10672]\n",
      "loss: 1.018332  [ 6300/10672]\n",
      "loss: 1.190296  [ 6400/10672]\n",
      "loss: 1.678206  [ 6500/10672]\n",
      "loss: 1.058656  [ 6600/10672]\n",
      "loss: 1.202869  [ 6700/10672]\n",
      "loss: 1.083272  [ 6800/10672]\n",
      "loss: 1.088500  [ 6900/10672]\n",
      "loss: 1.153324  [ 7000/10672]\n",
      "loss: 0.945197  [ 7100/10672]\n",
      "loss: 1.195782  [ 7200/10672]\n",
      "loss: 1.300256  [ 7300/10672]\n",
      "loss: 1.144168  [ 7400/10672]\n",
      "loss: 0.960699  [ 7500/10672]\n",
      "loss: 1.199038  [ 7600/10672]\n",
      "loss: 1.227984  [ 7700/10672]\n",
      "loss: 1.249808  [ 7800/10672]\n",
      "loss: 1.101366  [ 7900/10672]\n",
      "loss: 1.062101  [ 8000/10672]\n",
      "loss: 1.112695  [ 8100/10672]\n",
      "loss: 1.063276  [ 8200/10672]\n",
      "loss: 1.073464  [ 8300/10672]\n",
      "loss: 1.215782  [ 8400/10672]\n",
      "loss: 1.016340  [ 8500/10672]\n",
      "loss: 1.221088  [ 8600/10672]\n",
      "loss: 1.210175  [ 8700/10672]\n",
      "loss: 1.054193  [ 8800/10672]\n",
      "loss: 0.999084  [ 8900/10672]\n",
      "loss: 1.667199  [ 9000/10672]\n",
      "loss: 0.995716  [ 9100/10672]\n",
      "loss: 1.019750  [ 9200/10672]\n",
      "loss: 1.226672  [ 9300/10672]\n",
      "loss: 1.220293  [ 9400/10672]\n",
      "loss: 1.112800  [ 9500/10672]\n",
      "loss: 1.678485  [ 9600/10672]\n",
      "loss: 1.201134  [ 9700/10672]\n",
      "loss: 1.174618  [ 9800/10672]\n",
      "loss: 1.097088  [ 9900/10672]\n",
      "loss: 1.251468  [10000/10672]\n",
      "loss: 1.086460  [10100/10672]\n",
      "loss: 0.952952  [10200/10672]\n",
      "loss: 1.114053  [10300/10672]\n",
      "loss: 1.181685  [10400/10672]\n",
      "loss: 1.168505  [10500/10672]\n",
      "loss: 1.670980  [10600/10672]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.017113  [    0/10672]\n",
      "loss: 1.092734  [  100/10672]\n",
      "loss: 0.898605  [  200/10672]\n",
      "loss: 0.935809  [  300/10672]\n",
      "loss: 1.264913  [  400/10672]\n",
      "loss: 1.078565  [  500/10672]\n",
      "loss: 1.667180  [  600/10672]\n",
      "loss: 0.986385  [  700/10672]\n",
      "loss: 1.666525  [  800/10672]\n",
      "loss: 1.506610  [  900/10672]\n",
      "loss: 1.667086  [ 1000/10672]\n",
      "loss: 1.289785  [ 1100/10672]\n",
      "loss: 1.117648  [ 1200/10672]\n",
      "loss: 1.134984  [ 1300/10672]\n",
      "loss: 0.986462  [ 1400/10672]\n",
      "loss: 1.023637  [ 1500/10672]\n",
      "loss: 1.023757  [ 1600/10672]\n",
      "loss: 1.000384  [ 1700/10672]\n",
      "loss: 1.246085  [ 1800/10672]\n",
      "loss: 1.053623  [ 1900/10672]\n",
      "loss: 1.138428  [ 2000/10672]\n",
      "loss: 1.140722  [ 2100/10672]\n",
      "loss: 1.088603  [ 2200/10672]\n",
      "loss: 1.245545  [ 2300/10672]\n",
      "loss: 1.131418  [ 2400/10672]\n",
      "loss: 1.104344  [ 2500/10672]\n",
      "loss: 1.126777  [ 2600/10672]\n",
      "loss: 1.666145  [ 2700/10672]\n",
      "loss: 1.091407  [ 2800/10672]\n",
      "loss: 1.669879  [ 2900/10672]\n",
      "loss: 1.669149  [ 3000/10672]\n",
      "loss: 1.228961  [ 3100/10672]\n",
      "loss: 0.990751  [ 3200/10672]\n",
      "loss: 1.020671  [ 3300/10672]\n",
      "loss: 1.678562  [ 3400/10672]\n",
      "loss: 1.671268  [ 3500/10672]\n",
      "loss: 1.672544  [ 3600/10672]\n",
      "loss: 1.086591  [ 3700/10672]\n",
      "loss: 1.068395  [ 3800/10672]\n",
      "loss: 1.212754  [ 3900/10672]\n",
      "loss: 0.985830  [ 4000/10672]\n",
      "loss: 1.666985  [ 4100/10672]\n",
      "loss: 1.671863  [ 4200/10672]\n",
      "loss: 1.140718  [ 4300/10672]\n",
      "loss: 1.666075  [ 4400/10672]\n",
      "loss: 1.071176  [ 4500/10672]\n",
      "loss: 1.309820  [ 4600/10672]\n",
      "loss: 1.064386  [ 4700/10672]\n",
      "loss: 1.671813  [ 4800/10672]\n",
      "loss: 1.174525  [ 4900/10672]\n",
      "loss: 1.666449  [ 5000/10672]\n",
      "loss: 1.243543  [ 5100/10672]\n",
      "loss: 1.067509  [ 5200/10672]\n",
      "loss: 1.472947  [ 5300/10672]\n",
      "loss: 1.114503  [ 5400/10672]\n",
      "loss: 0.924061  [ 5500/10672]\n",
      "loss: 1.135716  [ 5600/10672]\n",
      "loss: 1.672203  [ 5700/10672]\n",
      "loss: 1.030349  [ 5800/10672]\n",
      "loss: 1.227079  [ 5900/10672]\n",
      "loss: 1.668349  [ 6000/10672]\n",
      "loss: 1.184924  [ 6100/10672]\n",
      "loss: 1.050438  [ 6200/10672]\n",
      "loss: 1.017248  [ 6300/10672]\n",
      "loss: 1.190882  [ 6400/10672]\n",
      "loss: 1.678365  [ 6500/10672]\n",
      "loss: 1.058166  [ 6600/10672]\n",
      "loss: 1.202878  [ 6700/10672]\n",
      "loss: 1.082801  [ 6800/10672]\n",
      "loss: 1.087398  [ 6900/10672]\n",
      "loss: 1.153543  [ 7000/10672]\n",
      "loss: 0.944011  [ 7100/10672]\n",
      "loss: 1.195658  [ 7200/10672]\n",
      "loss: 1.301489  [ 7300/10672]\n",
      "loss: 1.143776  [ 7400/10672]\n",
      "loss: 0.959486  [ 7500/10672]\n",
      "loss: 1.198887  [ 7600/10672]\n",
      "loss: 1.228129  [ 7700/10672]\n",
      "loss: 1.250011  [ 7800/10672]\n",
      "loss: 1.101022  [ 7900/10672]\n",
      "loss: 1.061560  [ 8000/10672]\n",
      "loss: 1.112313  [ 8100/10672]\n",
      "loss: 1.062233  [ 8200/10672]\n",
      "loss: 1.072484  [ 8300/10672]\n",
      "loss: 1.216438  [ 8400/10672]\n",
      "loss: 1.015567  [ 8500/10672]\n",
      "loss: 1.221790  [ 8600/10672]\n",
      "loss: 1.210444  [ 8700/10672]\n",
      "loss: 1.053262  [ 8800/10672]\n",
      "loss: 0.998137  [ 8900/10672]\n",
      "loss: 1.667235  [ 9000/10672]\n",
      "loss: 0.994075  [ 9100/10672]\n",
      "loss: 1.018839  [ 9200/10672]\n",
      "loss: 1.227464  [ 9300/10672]\n",
      "loss: 1.221061  [ 9400/10672]\n",
      "loss: 1.111994  [ 9500/10672]\n",
      "loss: 1.678620  [ 9600/10672]\n",
      "loss: 1.200860  [ 9700/10672]\n",
      "loss: 1.175089  [ 9800/10672]\n",
      "loss: 1.096914  [ 9900/10672]\n",
      "loss: 1.252513  [10000/10672]\n",
      "loss: 1.086013  [10100/10672]\n",
      "loss: 0.951787  [10200/10672]\n",
      "loss: 1.113277  [10300/10672]\n",
      "loss: 1.181353  [10400/10672]\n",
      "loss: 1.168036  [10500/10672]\n",
      "loss: 1.671052  [10600/10672]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.016520  [    0/10672]\n",
      "loss: 1.092276  [  100/10672]\n",
      "loss: 0.897289  [  200/10672]\n",
      "loss: 0.934579  [  300/10672]\n",
      "loss: 1.265091  [  400/10672]\n",
      "loss: 1.077492  [  500/10672]\n",
      "loss: 1.667192  [  600/10672]\n",
      "loss: 0.985409  [  700/10672]\n",
      "loss: 1.666533  [  800/10672]\n",
      "loss: 1.508410  [  900/10672]\n",
      "loss: 1.667094  [ 1000/10672]\n",
      "loss: 1.290150  [ 1100/10672]\n",
      "loss: 1.116625  [ 1200/10672]\n",
      "loss: 1.135015  [ 1300/10672]\n",
      "loss: 0.984894  [ 1400/10672]\n",
      "loss: 1.022918  [ 1500/10672]\n",
      "loss: 1.022288  [ 1600/10672]\n",
      "loss: 0.999492  [ 1700/10672]\n",
      "loss: 1.246362  [ 1800/10672]\n",
      "loss: 1.052427  [ 1900/10672]\n",
      "loss: 1.137695  [ 2000/10672]\n",
      "loss: 1.140260  [ 2100/10672]\n",
      "loss: 1.087689  [ 2200/10672]\n",
      "loss: 1.245838  [ 2300/10672]\n",
      "loss: 1.131242  [ 2400/10672]\n",
      "loss: 1.104012  [ 2500/10672]\n",
      "loss: 1.126773  [ 2600/10672]\n",
      "loss: 1.666160  [ 2700/10672]\n",
      "loss: 1.090945  [ 2800/10672]\n",
      "loss: 1.669966  [ 2900/10672]\n",
      "loss: 1.669190  [ 3000/10672]\n",
      "loss: 1.229360  [ 3100/10672]\n",
      "loss: 0.989446  [ 3200/10672]\n",
      "loss: 1.019774  [ 3300/10672]\n",
      "loss: 1.678734  [ 3400/10672]\n",
      "loss: 1.671342  [ 3500/10672]\n",
      "loss: 1.672652  [ 3600/10672]\n",
      "loss: 1.085644  [ 3700/10672]\n",
      "loss: 1.067870  [ 3800/10672]\n",
      "loss: 1.212689  [ 3900/10672]\n",
      "loss: 0.984236  [ 4000/10672]\n",
      "loss: 1.667008  [ 4100/10672]\n",
      "loss: 1.671994  [ 4200/10672]\n",
      "loss: 1.140625  [ 4300/10672]\n",
      "loss: 1.666086  [ 4400/10672]\n",
      "loss: 1.070076  [ 4500/10672]\n",
      "loss: 1.310713  [ 4600/10672]\n",
      "loss: 1.063421  [ 4700/10672]\n",
      "loss: 1.671897  [ 4800/10672]\n",
      "loss: 1.174861  [ 4900/10672]\n",
      "loss: 1.666471  [ 5000/10672]\n",
      "loss: 1.244502  [ 5100/10672]\n",
      "loss: 1.066765  [ 5200/10672]\n",
      "loss: 1.474879  [ 5300/10672]\n",
      "loss: 1.114272  [ 5400/10672]\n",
      "loss: 0.922455  [ 5500/10672]\n",
      "loss: 1.135532  [ 5600/10672]\n",
      "loss: 1.672287  [ 5700/10672]\n",
      "loss: 1.029006  [ 5800/10672]\n",
      "loss: 1.227767  [ 5900/10672]\n",
      "loss: 1.668400  [ 6000/10672]\n",
      "loss: 1.184876  [ 6100/10672]\n",
      "loss: 1.049715  [ 6200/10672]\n",
      "loss: 1.016178  [ 6300/10672]\n",
      "loss: 1.191465  [ 6400/10672]\n",
      "loss: 1.678523  [ 6500/10672]\n",
      "loss: 1.057679  [ 6600/10672]\n",
      "loss: 1.202889  [ 6700/10672]\n",
      "loss: 1.082332  [ 6800/10672]\n",
      "loss: 1.086301  [ 6900/10672]\n",
      "loss: 1.153762  [ 7000/10672]\n",
      "loss: 0.942837  [ 7100/10672]\n",
      "loss: 1.195534  [ 7200/10672]\n",
      "loss: 1.302712  [ 7300/10672]\n",
      "loss: 1.143387  [ 7400/10672]\n",
      "loss: 0.958284  [ 7500/10672]\n",
      "loss: 1.198736  [ 7600/10672]\n",
      "loss: 1.228271  [ 7700/10672]\n",
      "loss: 1.250210  [ 7800/10672]\n",
      "loss: 1.100677  [ 7900/10672]\n",
      "loss: 1.061023  [ 8000/10672]\n",
      "loss: 1.111931  [ 8100/10672]\n",
      "loss: 1.061200  [ 8200/10672]\n",
      "loss: 1.071513  [ 8300/10672]\n",
      "loss: 1.217089  [ 8400/10672]\n",
      "loss: 1.014802  [ 8500/10672]\n",
      "loss: 1.222488  [ 8600/10672]\n",
      "loss: 1.210715  [ 8700/10672]\n",
      "loss: 1.052339  [ 8800/10672]\n",
      "loss: 0.997198  [ 8900/10672]\n",
      "loss: 1.667270  [ 9000/10672]\n",
      "loss: 0.992449  [ 9100/10672]\n",
      "loss: 1.017935  [ 9200/10672]\n",
      "loss: 1.228251  [ 9300/10672]\n",
      "loss: 1.221826  [ 9400/10672]\n",
      "loss: 1.111193  [ 9500/10672]\n",
      "loss: 1.678755  [ 9600/10672]\n",
      "loss: 1.200584  [ 9700/10672]\n",
      "loss: 1.175559  [ 9800/10672]\n",
      "loss: 1.096743  [ 9900/10672]\n",
      "loss: 1.253553  [10000/10672]\n",
      "loss: 1.085568  [10100/10672]\n",
      "loss: 0.950631  [10200/10672]\n",
      "loss: 1.112507  [10300/10672]\n",
      "loss: 1.181024  [10400/10672]\n",
      "loss: 1.167569  [10500/10672]\n",
      "loss: 1.671123  [10600/10672]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.015933  [    0/10672]\n",
      "loss: 1.091815  [  100/10672]\n",
      "loss: 0.895988  [  200/10672]\n",
      "loss: 0.933359  [  300/10672]\n",
      "loss: 1.265267  [  400/10672]\n",
      "loss: 1.076430  [  500/10672]\n",
      "loss: 1.667203  [  600/10672]\n",
      "loss: 0.984437  [  700/10672]\n",
      "loss: 1.666541  [  800/10672]\n",
      "loss: 1.510191  [  900/10672]\n",
      "loss: 1.667101  [ 1000/10672]\n",
      "loss: 1.290512  [ 1100/10672]\n",
      "loss: 1.115607  [ 1200/10672]\n",
      "loss: 1.135042  [ 1300/10672]\n",
      "loss: 0.983344  [ 1400/10672]\n",
      "loss: 1.022202  [ 1500/10672]\n",
      "loss: 1.020833  [ 1600/10672]\n",
      "loss: 0.998604  [ 1700/10672]\n",
      "loss: 1.246640  [ 1800/10672]\n",
      "loss: 1.051241  [ 1900/10672]\n",
      "loss: 1.136965  [ 2000/10672]\n",
      "loss: 1.139805  [ 2100/10672]\n",
      "loss: 1.086783  [ 2200/10672]\n",
      "loss: 1.246130  [ 2300/10672]\n",
      "loss: 1.131064  [ 2400/10672]\n",
      "loss: 1.103681  [ 2500/10672]\n",
      "loss: 1.126770  [ 2600/10672]\n",
      "loss: 1.666174  [ 2700/10672]\n",
      "loss: 1.090484  [ 2800/10672]\n",
      "loss: 1.670053  [ 2900/10672]\n",
      "loss: 1.669231  [ 3000/10672]\n",
      "loss: 1.229760  [ 3100/10672]\n",
      "loss: 0.988155  [ 3200/10672]\n",
      "loss: 1.018883  [ 3300/10672]\n",
      "loss: 1.678905  [ 3400/10672]\n",
      "loss: 1.671417  [ 3500/10672]\n",
      "loss: 1.672758  [ 3600/10672]\n",
      "loss: 1.084704  [ 3700/10672]\n",
      "loss: 1.067345  [ 3800/10672]\n",
      "loss: 1.212625  [ 3900/10672]\n",
      "loss: 0.982659  [ 4000/10672]\n",
      "loss: 1.667031  [ 4100/10672]\n",
      "loss: 1.672124  [ 4200/10672]\n",
      "loss: 1.140528  [ 4300/10672]\n",
      "loss: 1.666098  [ 4400/10672]\n",
      "loss: 1.068984  [ 4500/10672]\n",
      "loss: 1.311602  [ 4600/10672]\n",
      "loss: 1.062466  [ 4700/10672]\n",
      "loss: 1.671982  [ 4800/10672]\n",
      "loss: 1.175193  [ 4900/10672]\n",
      "loss: 1.666491  [ 5000/10672]\n",
      "loss: 1.245455  [ 5100/10672]\n",
      "loss: 1.066023  [ 5200/10672]\n",
      "loss: 1.476794  [ 5300/10672]\n",
      "loss: 1.114039  [ 5400/10672]\n",
      "loss: 0.920870  [ 5500/10672]\n",
      "loss: 1.135345  [ 5600/10672]\n",
      "loss: 1.672371  [ 5700/10672]\n",
      "loss: 1.027672  [ 5800/10672]\n",
      "loss: 1.228451  [ 5900/10672]\n",
      "loss: 1.668452  [ 6000/10672]\n",
      "loss: 1.184828  [ 6100/10672]\n",
      "loss: 1.048997  [ 6200/10672]\n",
      "loss: 1.015120  [ 6300/10672]\n",
      "loss: 1.192048  [ 6400/10672]\n",
      "loss: 1.678681  [ 6500/10672]\n",
      "loss: 1.057198  [ 6600/10672]\n",
      "loss: 1.202900  [ 6700/10672]\n",
      "loss: 1.081866  [ 6800/10672]\n",
      "loss: 1.085206  [ 6900/10672]\n",
      "loss: 1.153982  [ 7000/10672]\n",
      "loss: 0.941676  [ 7100/10672]\n",
      "loss: 1.195409  [ 7200/10672]\n",
      "loss: 1.303928  [ 7300/10672]\n",
      "loss: 1.142999  [ 7400/10672]\n",
      "loss: 0.957095  [ 7500/10672]\n",
      "loss: 1.198582  [ 7600/10672]\n",
      "loss: 1.228410  [ 7700/10672]\n",
      "loss: 1.250403  [ 7800/10672]\n",
      "loss: 1.100335  [ 7900/10672]\n",
      "loss: 1.060488  [ 8000/10672]\n",
      "loss: 1.111548  [ 8100/10672]\n",
      "loss: 1.060175  [ 8200/10672]\n",
      "loss: 1.070550  [ 8300/10672]\n",
      "loss: 1.217736  [ 8400/10672]\n",
      "loss: 1.014042  [ 8500/10672]\n",
      "loss: 1.223181  [ 8600/10672]\n",
      "loss: 1.210989  [ 8700/10672]\n",
      "loss: 1.051425  [ 8800/10672]\n",
      "loss: 0.996267  [ 8900/10672]\n",
      "loss: 1.667305  [ 9000/10672]\n",
      "loss: 0.990835  [ 9100/10672]\n",
      "loss: 1.017036  [ 9200/10672]\n",
      "loss: 1.229034  [ 9300/10672]\n",
      "loss: 1.222587  [ 9400/10672]\n",
      "loss: 1.110399  [ 9500/10672]\n",
      "loss: 1.678888  [ 9600/10672]\n",
      "loss: 1.200306  [ 9700/10672]\n",
      "loss: 1.176030  [ 9800/10672]\n",
      "loss: 1.096575  [ 9900/10672]\n",
      "loss: 1.254587  [10000/10672]\n",
      "loss: 1.085123  [10100/10672]\n",
      "loss: 0.949488  [10200/10672]\n",
      "loss: 1.111741  [10300/10672]\n",
      "loss: 1.180695  [10400/10672]\n",
      "loss: 1.167103  [10500/10672]\n",
      "loss: 1.671194  [10600/10672]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.015353  [    0/10672]\n",
      "loss: 1.091355  [  100/10672]\n",
      "loss: 0.894705  [  200/10672]\n",
      "loss: 0.932152  [  300/10672]\n",
      "loss: 1.265439  [  400/10672]\n",
      "loss: 1.075374  [  500/10672]\n",
      "loss: 1.667214  [  600/10672]\n",
      "loss: 0.983474  [  700/10672]\n",
      "loss: 1.666548  [  800/10672]\n",
      "loss: 1.511951  [  900/10672]\n",
      "loss: 1.667108  [ 1000/10672]\n",
      "loss: 1.290866  [ 1100/10672]\n",
      "loss: 1.114588  [ 1200/10672]\n",
      "loss: 1.135070  [ 1300/10672]\n",
      "loss: 0.981808  [ 1400/10672]\n",
      "loss: 1.021494  [ 1500/10672]\n",
      "loss: 1.019386  [ 1600/10672]\n",
      "loss: 0.997723  [ 1700/10672]\n",
      "loss: 1.246914  [ 1800/10672]\n",
      "loss: 1.050062  [ 1900/10672]\n",
      "loss: 1.136235  [ 2000/10672]\n",
      "loss: 1.139352  [ 2100/10672]\n",
      "loss: 1.085881  [ 2200/10672]\n",
      "loss: 1.246417  [ 2300/10672]\n",
      "loss: 1.130886  [ 2400/10672]\n",
      "loss: 1.103351  [ 2500/10672]\n",
      "loss: 1.126770  [ 2600/10672]\n",
      "loss: 1.666189  [ 2700/10672]\n",
      "loss: 1.090026  [ 2800/10672]\n",
      "loss: 1.670139  [ 2900/10672]\n",
      "loss: 1.669271  [ 3000/10672]\n",
      "loss: 1.230159  [ 3100/10672]\n",
      "loss: 0.986876  [ 3200/10672]\n",
      "loss: 1.017999  [ 3300/10672]\n",
      "loss: 1.679076  [ 3400/10672]\n",
      "loss: 1.671491  [ 3500/10672]\n",
      "loss: 1.672864  [ 3600/10672]\n",
      "loss: 1.083769  [ 3700/10672]\n",
      "loss: 1.066824  [ 3800/10672]\n",
      "loss: 1.212559  [ 3900/10672]\n",
      "loss: 0.981095  [ 4000/10672]\n",
      "loss: 1.667053  [ 4100/10672]\n",
      "loss: 1.672255  [ 4200/10672]\n",
      "loss: 1.140429  [ 4300/10672]\n",
      "loss: 1.666109  [ 4400/10672]\n",
      "loss: 1.067898  [ 4500/10672]\n",
      "loss: 1.312487  [ 4600/10672]\n",
      "loss: 1.061519  [ 4700/10672]\n",
      "loss: 1.672067  [ 4800/10672]\n",
      "loss: 1.175526  [ 4900/10672]\n",
      "loss: 1.666512  [ 5000/10672]\n",
      "loss: 1.246406  [ 5100/10672]\n",
      "loss: 1.065282  [ 5200/10672]\n",
      "loss: 1.478691  [ 5300/10672]\n",
      "loss: 1.113806  [ 5400/10672]\n",
      "loss: 0.919306  [ 5500/10672]\n",
      "loss: 1.135155  [ 5600/10672]\n",
      "loss: 1.672455  [ 5700/10672]\n",
      "loss: 1.026348  [ 5800/10672]\n",
      "loss: 1.229130  [ 5900/10672]\n",
      "loss: 1.668503  [ 6000/10672]\n",
      "loss: 1.184784  [ 6100/10672]\n",
      "loss: 1.048281  [ 6200/10672]\n",
      "loss: 1.014076  [ 6300/10672]\n",
      "loss: 1.192627  [ 6400/10672]\n",
      "loss: 1.678838  [ 6500/10672]\n",
      "loss: 1.056719  [ 6600/10672]\n",
      "loss: 1.202915  [ 6700/10672]\n",
      "loss: 1.081400  [ 6800/10672]\n",
      "loss: 1.084118  [ 6900/10672]\n",
      "loss: 1.154201  [ 7000/10672]\n",
      "loss: 0.940527  [ 7100/10672]\n",
      "loss: 1.195284  [ 7200/10672]\n",
      "loss: 1.305133  [ 7300/10672]\n",
      "loss: 1.142615  [ 7400/10672]\n",
      "loss: 0.955917  [ 7500/10672]\n",
      "loss: 1.198429  [ 7600/10672]\n",
      "loss: 1.228548  [ 7700/10672]\n",
      "loss: 1.250593  [ 7800/10672]\n",
      "loss: 1.099990  [ 7900/10672]\n",
      "loss: 1.059955  [ 8000/10672]\n",
      "loss: 1.111162  [ 8100/10672]\n",
      "loss: 1.059162  [ 8200/10672]\n",
      "loss: 1.069596  [ 8300/10672]\n",
      "loss: 1.218379  [ 8400/10672]\n",
      "loss: 1.013288  [ 8500/10672]\n",
      "loss: 1.223870  [ 8600/10672]\n",
      "loss: 1.211268  [ 8700/10672]\n",
      "loss: 1.050523  [ 8800/10672]\n",
      "loss: 0.995341  [ 8900/10672]\n",
      "loss: 1.667340  [ 9000/10672]\n",
      "loss: 0.989238  [ 9100/10672]\n",
      "loss: 1.016141  [ 9200/10672]\n",
      "loss: 1.229811  [ 9300/10672]\n",
      "loss: 1.223342  [ 9400/10672]\n",
      "loss: 1.109613  [ 9500/10672]\n",
      "loss: 1.679022  [ 9600/10672]\n",
      "loss: 1.200029  [ 9700/10672]\n",
      "loss: 1.176496  [ 9800/10672]\n",
      "loss: 1.096407  [ 9900/10672]\n",
      "loss: 1.255612  [10000/10672]\n",
      "loss: 1.084677  [10100/10672]\n",
      "loss: 0.948354  [10200/10672]\n",
      "loss: 1.110984  [10300/10672]\n",
      "loss: 1.180372  [10400/10672]\n",
      "loss: 1.166642  [10500/10672]\n",
      "loss: 1.671265  [10600/10672]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.014779  [    0/10672]\n",
      "loss: 1.090892  [  100/10672]\n",
      "loss: 0.893436  [  200/10672]\n",
      "loss: 0.930956  [  300/10672]\n",
      "loss: 1.265607  [  400/10672]\n",
      "loss: 1.074328  [  500/10672]\n",
      "loss: 1.667225  [  600/10672]\n",
      "loss: 0.982516  [  700/10672]\n",
      "loss: 1.666555  [  800/10672]\n",
      "loss: 1.513691  [  900/10672]\n",
      "loss: 1.667115  [ 1000/10672]\n",
      "loss: 1.291215  [ 1100/10672]\n",
      "loss: 1.113574  [ 1200/10672]\n",
      "loss: 1.135094  [ 1300/10672]\n",
      "loss: 0.980289  [ 1400/10672]\n",
      "loss: 1.020789  [ 1500/10672]\n",
      "loss: 1.017954  [ 1600/10672]\n",
      "loss: 0.996847  [ 1700/10672]\n",
      "loss: 1.247188  [ 1800/10672]\n",
      "loss: 1.048894  [ 1900/10672]\n",
      "loss: 1.135510  [ 2000/10672]\n",
      "loss: 1.138907  [ 2100/10672]\n",
      "loss: 1.084990  [ 2200/10672]\n",
      "loss: 1.246704  [ 2300/10672]\n",
      "loss: 1.130704  [ 2400/10672]\n",
      "loss: 1.103020  [ 2500/10672]\n",
      "loss: 1.126769  [ 2600/10672]\n",
      "loss: 1.666203  [ 2700/10672]\n",
      "loss: 1.089566  [ 2800/10672]\n",
      "loss: 1.670226  [ 2900/10672]\n",
      "loss: 1.669311  [ 3000/10672]\n",
      "loss: 1.230561  [ 3100/10672]\n",
      "loss: 0.985614  [ 3200/10672]\n",
      "loss: 1.017118  [ 3300/10672]\n",
      "loss: 1.679247  [ 3400/10672]\n",
      "loss: 1.671566  [ 3500/10672]\n",
      "loss: 1.672970  [ 3600/10672]\n",
      "loss: 1.082842  [ 3700/10672]\n",
      "loss: 1.066303  [ 3800/10672]\n",
      "loss: 1.212494  [ 3900/10672]\n",
      "loss: 0.979549  [ 4000/10672]\n",
      "loss: 1.667076  [ 4100/10672]\n",
      "loss: 1.672386  [ 4200/10672]\n",
      "loss: 1.140326  [ 4300/10672]\n",
      "loss: 1.666120  [ 4400/10672]\n",
      "loss: 1.066820  [ 4500/10672]\n",
      "loss: 1.313368  [ 4600/10672]\n",
      "loss: 1.060581  [ 4700/10672]\n",
      "loss: 1.672152  [ 4800/10672]\n",
      "loss: 1.175855  [ 4900/10672]\n",
      "loss: 1.666532  [ 5000/10672]\n",
      "loss: 1.247352  [ 5100/10672]\n",
      "loss: 1.064541  [ 5200/10672]\n",
      "loss: 1.480571  [ 5300/10672]\n",
      "loss: 1.113571  [ 5400/10672]\n",
      "loss: 0.917763  [ 5500/10672]\n",
      "loss: 1.134962  [ 5600/10672]\n",
      "loss: 1.672538  [ 5700/10672]\n",
      "loss: 1.025034  [ 5800/10672]\n",
      "loss: 1.229805  [ 5900/10672]\n",
      "loss: 1.668555  [ 6000/10672]\n",
      "loss: 1.184741  [ 6100/10672]\n",
      "loss: 1.047570  [ 6200/10672]\n",
      "loss: 1.013043  [ 6300/10672]\n",
      "loss: 1.193206  [ 6400/10672]\n",
      "loss: 1.678995  [ 6500/10672]\n",
      "loss: 1.056245  [ 6600/10672]\n",
      "loss: 1.202930  [ 6700/10672]\n",
      "loss: 1.080937  [ 6800/10672]\n",
      "loss: 1.083035  [ 6900/10672]\n",
      "loss: 1.154418  [ 7000/10672]\n",
      "loss: 0.939389  [ 7100/10672]\n",
      "loss: 1.195161  [ 7200/10672]\n",
      "loss: 1.306328  [ 7300/10672]\n",
      "loss: 1.142235  [ 7400/10672]\n",
      "loss: 0.954749  [ 7500/10672]\n",
      "loss: 1.198276  [ 7600/10672]\n",
      "loss: 1.228683  [ 7700/10672]\n",
      "loss: 1.250778  [ 7800/10672]\n",
      "loss: 1.099646  [ 7900/10672]\n",
      "loss: 1.059426  [ 8000/10672]\n",
      "loss: 1.110776  [ 8100/10672]\n",
      "loss: 1.058158  [ 8200/10672]\n",
      "loss: 1.068650  [ 8300/10672]\n",
      "loss: 1.219019  [ 8400/10672]\n",
      "loss: 1.012541  [ 8500/10672]\n",
      "loss: 1.224556  [ 8600/10672]\n",
      "loss: 1.211549  [ 8700/10672]\n",
      "loss: 1.049629  [ 8800/10672]\n",
      "loss: 0.994425  [ 8900/10672]\n",
      "loss: 1.667374  [ 9000/10672]\n",
      "loss: 0.987653  [ 9100/10672]\n",
      "loss: 1.015253  [ 9200/10672]\n",
      "loss: 1.230583  [ 9300/10672]\n",
      "loss: 1.224092  [ 9400/10672]\n",
      "loss: 1.108833  [ 9500/10672]\n",
      "loss: 1.679155  [ 9600/10672]\n",
      "loss: 1.199751  [ 9700/10672]\n",
      "loss: 1.176962  [ 9800/10672]\n",
      "loss: 1.096241  [ 9900/10672]\n",
      "loss: 1.256632  [10000/10672]\n",
      "loss: 1.084231  [10100/10672]\n",
      "loss: 0.947230  [10200/10672]\n",
      "loss: 1.110232  [10300/10672]\n",
      "loss: 1.180051  [10400/10672]\n",
      "loss: 1.166183  [10500/10672]\n",
      "loss: 1.671335  [10600/10672]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.014212  [    0/10672]\n",
      "loss: 1.090427  [  100/10672]\n",
      "loss: 0.892183  [  200/10672]\n",
      "loss: 0.929771  [  300/10672]\n",
      "loss: 1.265773  [  400/10672]\n",
      "loss: 1.073291  [  500/10672]\n",
      "loss: 1.667236  [  600/10672]\n",
      "loss: 0.981564  [  700/10672]\n",
      "loss: 1.666562  [  800/10672]\n",
      "loss: 1.515413  [  900/10672]\n",
      "loss: 1.667121  [ 1000/10672]\n",
      "loss: 1.291561  [ 1100/10672]\n",
      "loss: 1.112564  [ 1200/10672]\n",
      "loss: 1.135115  [ 1300/10672]\n",
      "loss: 0.978786  [ 1400/10672]\n",
      "loss: 1.020089  [ 1500/10672]\n",
      "loss: 1.016533  [ 1600/10672]\n",
      "loss: 0.995977  [ 1700/10672]\n",
      "loss: 1.247460  [ 1800/10672]\n",
      "loss: 1.047735  [ 1900/10672]\n",
      "loss: 1.134786  [ 2000/10672]\n",
      "loss: 1.138467  [ 2100/10672]\n",
      "loss: 1.084105  [ 2200/10672]\n",
      "loss: 1.246988  [ 2300/10672]\n",
      "loss: 1.130521  [ 2400/10672]\n",
      "loss: 1.102691  [ 2500/10672]\n",
      "loss: 1.126771  [ 2600/10672]\n",
      "loss: 1.666216  [ 2700/10672]\n",
      "loss: 1.089109  [ 2800/10672]\n",
      "loss: 1.670312  [ 2900/10672]\n",
      "loss: 1.669350  [ 3000/10672]\n",
      "loss: 1.230961  [ 3100/10672]\n",
      "loss: 0.984363  [ 3200/10672]\n",
      "loss: 1.016244  [ 3300/10672]\n",
      "loss: 1.679417  [ 3400/10672]\n",
      "loss: 1.671641  [ 3500/10672]\n",
      "loss: 1.673075  [ 3600/10672]\n",
      "loss: 1.081919  [ 3700/10672]\n",
      "loss: 1.065786  [ 3800/10672]\n",
      "loss: 1.212428  [ 3900/10672]\n",
      "loss: 0.978016  [ 4000/10672]\n",
      "loss: 1.667098  [ 4100/10672]\n",
      "loss: 1.672517  [ 4200/10672]\n",
      "loss: 1.140221  [ 4300/10672]\n",
      "loss: 1.666131  [ 4400/10672]\n",
      "loss: 1.065748  [ 4500/10672]\n",
      "loss: 1.314245  [ 4600/10672]\n",
      "loss: 1.059653  [ 4700/10672]\n",
      "loss: 1.672236  [ 4800/10672]\n",
      "loss: 1.176181  [ 4900/10672]\n",
      "loss: 1.666552  [ 5000/10672]\n",
      "loss: 1.248293  [ 5100/10672]\n",
      "loss: 1.063801  [ 5200/10672]\n",
      "loss: 1.482434  [ 5300/10672]\n",
      "loss: 1.113334  [ 5400/10672]\n",
      "loss: 0.916240  [ 5500/10672]\n",
      "loss: 1.134766  [ 5600/10672]\n",
      "loss: 1.672621  [ 5700/10672]\n",
      "loss: 1.023729  [ 5800/10672]\n",
      "loss: 1.230474  [ 5900/10672]\n",
      "loss: 1.668606  [ 6000/10672]\n",
      "loss: 1.184700  [ 6100/10672]\n",
      "loss: 1.046863  [ 6200/10672]\n",
      "loss: 1.012023  [ 6300/10672]\n",
      "loss: 1.193783  [ 6400/10672]\n",
      "loss: 1.679152  [ 6500/10672]\n",
      "loss: 1.055773  [ 6600/10672]\n",
      "loss: 1.202947  [ 6700/10672]\n",
      "loss: 1.080476  [ 6800/10672]\n",
      "loss: 1.081954  [ 6900/10672]\n",
      "loss: 1.154637  [ 7000/10672]\n",
      "loss: 0.938263  [ 7100/10672]\n",
      "loss: 1.195037  [ 7200/10672]\n",
      "loss: 1.307515  [ 7300/10672]\n",
      "loss: 1.141856  [ 7400/10672]\n",
      "loss: 0.953593  [ 7500/10672]\n",
      "loss: 1.198123  [ 7600/10672]\n",
      "loss: 1.228817  [ 7700/10672]\n",
      "loss: 1.250959  [ 7800/10672]\n",
      "loss: 1.099301  [ 7900/10672]\n",
      "loss: 1.058897  [ 8000/10672]\n",
      "loss: 1.110387  [ 8100/10672]\n",
      "loss: 1.057164  [ 8200/10672]\n",
      "loss: 1.067713  [ 8300/10672]\n",
      "loss: 1.219652  [ 8400/10672]\n",
      "loss: 1.011798  [ 8500/10672]\n",
      "loss: 1.225236  [ 8600/10672]\n",
      "loss: 1.211835  [ 8700/10672]\n",
      "loss: 1.048746  [ 8800/10672]\n",
      "loss: 0.993513  [ 8900/10672]\n",
      "loss: 1.667409  [ 9000/10672]\n",
      "loss: 0.986082  [ 9100/10672]\n",
      "loss: 1.014368  [ 9200/10672]\n",
      "loss: 1.231350  [ 9300/10672]\n",
      "loss: 1.224838  [ 9400/10672]\n",
      "loss: 1.108060  [ 9500/10672]\n",
      "loss: 1.679287  [ 9600/10672]\n",
      "loss: 1.199472  [ 9700/10672]\n",
      "loss: 1.177425  [ 9800/10672]\n",
      "loss: 1.096076  [ 9900/10672]\n",
      "loss: 1.257644  [10000/10672]\n",
      "loss: 1.083784  [10100/10672]\n",
      "loss: 0.946116  [10200/10672]\n",
      "loss: 1.109486  [10300/10672]\n",
      "loss: 1.179731  [10400/10672]\n",
      "loss: 1.165727  [10500/10672]\n",
      "loss: 1.671404  [10600/10672]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.013650  [    0/10672]\n",
      "loss: 1.089961  [  100/10672]\n",
      "loss: 0.890945  [  200/10672]\n",
      "loss: 0.928598  [  300/10672]\n",
      "loss: 1.265937  [  400/10672]\n",
      "loss: 1.072263  [  500/10672]\n",
      "loss: 1.667246  [  600/10672]\n",
      "loss: 0.980618  [  700/10672]\n",
      "loss: 1.666569  [  800/10672]\n",
      "loss: 1.517115  [  900/10672]\n",
      "loss: 1.667128  [ 1000/10672]\n",
      "loss: 1.291899  [ 1100/10672]\n",
      "loss: 1.111556  [ 1200/10672]\n",
      "loss: 1.135135  [ 1300/10672]\n",
      "loss: 0.977298  [ 1400/10672]\n",
      "loss: 1.019395  [ 1500/10672]\n",
      "loss: 1.015124  [ 1600/10672]\n",
      "loss: 0.995112  [ 1700/10672]\n",
      "loss: 1.247730  [ 1800/10672]\n",
      "loss: 1.046584  [ 1900/10672]\n",
      "loss: 1.134063  [ 2000/10672]\n",
      "loss: 1.138029  [ 2100/10672]\n",
      "loss: 1.083225  [ 2200/10672]\n",
      "loss: 1.247269  [ 2300/10672]\n",
      "loss: 1.130338  [ 2400/10672]\n",
      "loss: 1.102363  [ 2500/10672]\n",
      "loss: 1.126775  [ 2600/10672]\n",
      "loss: 1.666230  [ 2700/10672]\n",
      "loss: 1.088653  [ 2800/10672]\n",
      "loss: 1.670398  [ 2900/10672]\n",
      "loss: 1.669390  [ 3000/10672]\n",
      "loss: 1.231362  [ 3100/10672]\n",
      "loss: 0.983127  [ 3200/10672]\n",
      "loss: 1.015375  [ 3300/10672]\n",
      "loss: 1.679588  [ 3400/10672]\n",
      "loss: 1.671715  [ 3500/10672]\n",
      "loss: 1.673180  [ 3600/10672]\n",
      "loss: 1.081002  [ 3700/10672]\n",
      "loss: 1.065269  [ 3800/10672]\n",
      "loss: 1.212362  [ 3900/10672]\n",
      "loss: 0.976499  [ 4000/10672]\n",
      "loss: 1.667120  [ 4100/10672]\n",
      "loss: 1.672647  [ 4200/10672]\n",
      "loss: 1.140112  [ 4300/10672]\n",
      "loss: 1.666141  [ 4400/10672]\n",
      "loss: 1.064683  [ 4500/10672]\n",
      "loss: 1.315119  [ 4600/10672]\n",
      "loss: 1.058733  [ 4700/10672]\n",
      "loss: 1.672321  [ 4800/10672]\n",
      "loss: 1.176506  [ 4900/10672]\n",
      "loss: 1.666571  [ 5000/10672]\n",
      "loss: 1.249230  [ 5100/10672]\n",
      "loss: 1.063063  [ 5200/10672]\n",
      "loss: 1.484281  [ 5300/10672]\n",
      "loss: 1.113097  [ 5400/10672]\n",
      "loss: 0.914736  [ 5500/10672]\n",
      "loss: 1.134566  [ 5600/10672]\n",
      "loss: 1.672704  [ 5700/10672]\n",
      "loss: 1.022434  [ 5800/10672]\n",
      "loss: 1.231139  [ 5900/10672]\n",
      "loss: 1.668657  [ 6000/10672]\n",
      "loss: 1.184662  [ 6100/10672]\n",
      "loss: 1.046159  [ 6200/10672]\n",
      "loss: 1.011016  [ 6300/10672]\n",
      "loss: 1.194357  [ 6400/10672]\n",
      "loss: 1.679309  [ 6500/10672]\n",
      "loss: 1.055305  [ 6600/10672]\n",
      "loss: 1.202966  [ 6700/10672]\n",
      "loss: 1.080016  [ 6800/10672]\n",
      "loss: 1.080881  [ 6900/10672]\n",
      "loss: 1.154853  [ 7000/10672]\n",
      "loss: 0.937147  [ 7100/10672]\n",
      "loss: 1.194915  [ 7200/10672]\n",
      "loss: 1.308691  [ 7300/10672]\n",
      "loss: 1.141482  [ 7400/10672]\n",
      "loss: 0.952446  [ 7500/10672]\n",
      "loss: 1.197971  [ 7600/10672]\n",
      "loss: 1.228951  [ 7700/10672]\n",
      "loss: 1.251137  [ 7800/10672]\n",
      "loss: 1.098955  [ 7900/10672]\n",
      "loss: 1.058369  [ 8000/10672]\n",
      "loss: 1.109996  [ 8100/10672]\n",
      "loss: 1.056181  [ 8200/10672]\n",
      "loss: 1.066786  [ 8300/10672]\n",
      "loss: 1.220280  [ 8400/10672]\n",
      "loss: 1.011060  [ 8500/10672]\n",
      "loss: 1.225911  [ 8600/10672]\n",
      "loss: 1.212125  [ 8700/10672]\n",
      "loss: 1.047873  [ 8800/10672]\n",
      "loss: 0.992608  [ 8900/10672]\n",
      "loss: 1.667444  [ 9000/10672]\n",
      "loss: 0.984526  [ 9100/10672]\n",
      "loss: 1.013489  [ 9200/10672]\n",
      "loss: 1.232111  [ 9300/10672]\n",
      "loss: 1.225578  [ 9400/10672]\n",
      "loss: 1.107295  [ 9500/10672]\n",
      "loss: 1.679419  [ 9600/10672]\n",
      "loss: 1.199195  [ 9700/10672]\n",
      "loss: 1.177886  [ 9800/10672]\n",
      "loss: 1.095910  [ 9900/10672]\n",
      "loss: 1.258649  [10000/10672]\n",
      "loss: 1.083336  [10100/10672]\n",
      "loss: 0.945011  [10200/10672]\n",
      "loss: 1.108749  [10300/10672]\n",
      "loss: 1.179418  [10400/10672]\n",
      "loss: 1.165276  [10500/10672]\n",
      "loss: 1.671474  [10600/10672]\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 74. 正解率の計測\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = SimpleNet()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(\"<train>\")\n",
    "test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"<test>\")\n",
    "test_loop(test_dataloader, model, loss_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<train>\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.220320 \n",
      "\n",
      "<test>\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.226426 \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 75. 損失と正解率のプロット\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 76. チェックポイント\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "learning_rate = 1e-3\n",
    "#batch_size = 64\n",
    "epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return test_loss, correct"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "epochs = 50\n",
    "x = [i for i in range(1, epochs+1)]\n",
    "losses = []\n",
    "acus = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    loss, accuracy = test_loop(train_dataloader, model, loss_fn)\n",
    "    print(f\"loss: {loss}, accuracy: {accuracy}\\n\")\n",
    "    losses.append(loss)\n",
    "    acus.append(accuracy)\n",
    "    outfile = f\"./checkpoint/out_{t:0=2}.cpt\"\n",
    "    torch.save(\n",
    "        {'iter': t,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'opt_state_dict': optimizer.state_dict(),\n",
    "        'loss': losses,\n",
    "        }, outfile\n",
    "    )\n",
    "plt.plot(x, losses, color=\"b\", label=\"loss\")\n",
    "plt.plot(x, acus, color=\"r\", label=\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Done!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.216791142459618, accuracy: 0.5513493253373314\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.2165833510726556, accuracy: 0.5514430284857571\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.2163750674335405, accuracy: 0.5512556221889056\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.2161682940617404, accuracy: 0.5512556221889056\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.2159631994107496, accuracy: 0.5513493253373314\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.215759788764113, accuracy: 0.5511619190404797\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.2155580220931117, accuracy: 0.5512556221889056\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.215357843747382, accuracy: 0.5515367316341829\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.215159274006578, accuracy: 0.5518178410794603\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.2149622939482354, accuracy: 0.5518178410794603\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.2147669110732011, accuracy: 0.5521926536731634\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.2145730457708486, accuracy: 0.5521926536731634\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.2143807248331082, accuracy: 0.5526611694152923\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.2141899457969318, accuracy: 0.5530359820089955\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.2140006426124201, accuracy: 0.5531296851574213\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.2138128049358883, accuracy: 0.5535044977511244\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.2136264526448597, accuracy: 0.5535982008995503\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.213441532948564, accuracy: 0.553691904047976\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.2132580208311463, accuracy: 0.5531296851574213\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.213075924904897, accuracy: 0.553223388305847\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.2128952420924022, accuracy: 0.5535044977511244\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.2127159174191005, accuracy: 0.5535982008995503\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.2125379784923533, accuracy: 0.5534107946026986\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.2123613773804645, accuracy: 0.5538793103448276\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.2121861358543207, accuracy: 0.5539730134932533\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.2120121681093723, accuracy: 0.5539730134932533\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.2118395172349963, accuracy: 0.554160419790105\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.2116681588297036, accuracy: 0.5542541229385307\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.2114980723289983, accuracy: 0.5540667166416792\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.211329246244241, accuracy: 0.5543478260869565\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.2111616610274323, accuracy: 0.5543478260869565\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.210995310518159, accuracy: 0.554160419790105\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.2108301545480737, accuracy: 0.5538793103448276\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.210666217485155, accuracy: 0.5539730134932533\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.2105034795579972, accuracy: 0.553691904047976\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.210341920844395, accuracy: 0.553691904047976\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.2101815121619777, accuracy: 0.5537856071964018\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.2100222782138317, accuracy: 0.5537856071964018\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.2098641741345044, accuracy: 0.5537856071964018\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.2097071663405317, accuracy: 0.5538793103448276\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.2095512912135342, accuracy: 0.5539730134932533\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.2093964991127621, accuracy: 0.554160419790105\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.2092428107535107, accuracy: 0.5545352323838081\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.209090192390346, accuracy: 0.5545352323838081\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.208938660532951, accuracy: 0.5546289355322339\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.2087881694892715, accuracy: 0.5546289355322339\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.2086387245149268, accuracy: 0.5550974512743628\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.2084903167071996, accuracy: 0.5550974512743628\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.208342957766964, accuracy: 0.555003748125937\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.2081965693011545, accuracy: 0.5549100449775113\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-09-03T09:02:59.398206</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mf433a08cbc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.109804\" xlink:href=\"#mf433a08cbc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(35.928554 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"101.224832\" xlink:href=\"#mf433a08cbc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(94.862332 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.33986\" xlink:href=\"#mf433a08cbc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(156.97736 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.454888\" xlink:href=\"#mf433a08cbc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(219.092388 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"287.569915\" xlink:href=\"#mf433a08cbc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(281.207415 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.684943\" xlink:href=\"#mf433a08cbc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(343.322443 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mbf77658b5f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mbf77658b5f\" y=\"200.252859\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 204.052078)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mbf77658b5f\" y=\"170.555736\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.7 -->\n      <g transform=\"translate(7.2 174.354955)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mbf77658b5f\" y=\"140.858613\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 144.657832)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mbf77658b5f\" y=\"111.161491\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.9 -->\n      <g transform=\"translate(7.2 114.960709)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mbf77658b5f\" y=\"81.464368\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 85.263587)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mbf77658b5f\" y=\"51.767245\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.1 -->\n      <g transform=\"translate(7.2 55.566464)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mbf77658b5f\" y=\"22.070123\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.2 -->\n      <g transform=\"translate(7.2 25.869341)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#pbe26930664)\" d=\"M 45.321307 17.083636 \nL 51.53281 17.145344 \nL 57.744312 17.207199 \nL 63.955815 17.268604 \nL 70.167318 17.329512 \nL 76.378821 17.389919 \nL 82.590324 17.449838 \nL 88.801826 17.509285 \nL 95.013329 17.568254 \nL 101.224832 17.626752 \nL 107.436335 17.684775 \nL 113.647837 17.742347 \nL 119.85934 17.799461 \nL 126.070843 17.856117 \nL 132.282346 17.912335 \nL 138.493849 17.968117 \nL 144.705351 18.023458 \nL 150.916854 18.078374 \nL 157.128357 18.132872 \nL 163.33986 18.186949 \nL 169.551362 18.240607 \nL 175.762865 18.293861 \nL 181.974368 18.346704 \nL 188.185871 18.399149 \nL 194.397374 18.451191 \nL 200.608876 18.502854 \nL 206.820379 18.554127 \nL 213.031882 18.605015 \nL 219.243385 18.655526 \nL 225.454888 18.705662 \nL 231.66639 18.75543 \nL 237.877893 18.804832 \nL 244.089396 18.853878 \nL 250.300899 18.902563 \nL 256.512401 18.950891 \nL 262.723904 18.99887 \nL 268.935407 19.046506 \nL 275.14691 19.093794 \nL 281.358413 19.140747 \nL 287.569915 19.187373 \nL 293.781418 19.233664 \nL 299.992921 19.279633 \nL 306.204424 19.325274 \nL 312.415926 19.370597 \nL 318.627429 19.415598 \nL 324.838932 19.460289 \nL 331.050435 19.50467 \nL 337.261938 19.548743 \nL 343.47344 19.592504 \nL 349.684943 19.635977 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pbe26930664)\" d=\"M 45.321307 214.700709 \nL 51.53281 214.672882 \nL 57.744312 214.728536 \nL 63.955815 214.728536 \nL 70.167318 214.700709 \nL 76.378821 214.756364 \nL 82.590324 214.728536 \nL 88.801826 214.645055 \nL 95.013329 214.561574 \nL 101.224832 214.561574 \nL 107.436335 214.450265 \nL 113.647837 214.450265 \nL 119.85934 214.311129 \nL 126.070843 214.199821 \nL 132.282346 214.171994 \nL 138.493849 214.060685 \nL 144.705351 214.032858 \nL 150.916854 214.005031 \nL 157.128357 214.171994 \nL 163.33986 214.144167 \nL 169.551362 214.060685 \nL 175.762865 214.032858 \nL 181.974368 214.088512 \nL 188.185871 213.949377 \nL 194.397374 213.921549 \nL 200.608876 213.921549 \nL 206.820379 213.865895 \nL 213.031882 213.838068 \nL 219.243385 213.893722 \nL 225.454888 213.810241 \nL 231.66639 213.810241 \nL 237.877893 213.865895 \nL 244.089396 213.949377 \nL 250.300899 213.921549 \nL 256.512401 214.005031 \nL 262.723904 214.005031 \nL 268.935407 213.977204 \nL 275.14691 213.977204 \nL 281.358413 213.977204 \nL 287.569915 213.949377 \nL 293.781418 213.921549 \nL 299.992921 213.865895 \nL 306.204424 213.754587 \nL 312.415926 213.754587 \nL 318.627429 213.726759 \nL 324.838932 213.726759 \nL 331.050435 213.587624 \nL 337.261938 213.587624 \nL 343.47344 213.615451 \nL 349.684943 213.643278 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 280.784375 132.098125 \nL 357.903125 132.098125 \nQ 359.903125 132.098125 359.903125 130.098125 \nL 359.903125 101.741875 \nQ 359.903125 99.741875 357.903125 99.741875 \nL 280.784375 99.741875 \nQ 278.784375 99.741875 278.784375 101.741875 \nL 278.784375 130.098125 \nQ 278.784375 132.098125 280.784375 132.098125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 282.784375 107.840312 \nL 302.784375 107.840312 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_14\">\n     <!-- loss -->\n     <g transform=\"translate(310.784375 111.340312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 282.784375 122.518437 \nL 302.784375 122.518437 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_15\">\n     <!-- accuracy -->\n     <g transform=\"translate(310.784375 126.018437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-79\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pbe26930664\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 77. ミニバッチ化\n",
    "問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import time\n",
    "times = []\n",
    "sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "for size in sizes:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=size)\n",
    "    start = time.time()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    elapsed_time = time.time() - start\n",
    "    times.append(elapsed_time)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "times"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3.5974998474121094,\n",
       " 1.5193920135498047,\n",
       " 0.7776110172271729,\n",
       " 0.42693185806274414,\n",
       " 0.2495279312133789,\n",
       " 0.15306806564331055,\n",
       " 0.10625100135803223,\n",
       " 0.08716487884521484,\n",
       " 0.07274293899536133,\n",
       " 0.07892298698425293]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "plt.plot(sizes, times)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.title(\"relationship between epoch & runtime\")\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-09-03T09:52:11.643685</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me053572c41\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.403808\" xlink:href=\"#me053572c41\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.222558 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"117.966164\" xlink:href=\"#me053572c41\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(108.422414 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"177.528519\" xlink:href=\"#me053572c41\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(167.984769 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"237.090875\" xlink:href=\"#me053572c41\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(227.547125 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"296.65323\" xlink:href=\"#me053572c41\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(287.10948 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.215586\" xlink:href=\"#me053572c41\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(346.671836 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epochs -->\n     <g transform=\"translate(193.348438 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m937488ca59\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"233.954003\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 237.753221)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"205.913384\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 209.712603)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"177.872766\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 181.671984)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"149.832147\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.878125 153.631366)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"121.791529\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.878125 125.590748)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"93.75091\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2.5 -->\n      <g transform=\"translate(20.878125 97.550129)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"65.710292\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 3.0 -->\n      <g transform=\"translate(20.878125 69.509511)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m937488ca59\" y=\"37.669673\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 3.5 -->\n      <g transform=\"translate(20.878125 41.468892)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- time -->\n     <g transform=\"translate(14.798438 142.334219)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"66.992188\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"164.404297\" xlink:href=\"#DejaVuSans-65\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p0941d99f72)\" d=\"M 58.999432 32.201761 \nL 59.595055 148.744619 \nL 60.786302 190.344615 \nL 63.168797 210.011136 \nL 67.933785 219.960168 \nL 77.463762 225.369756 \nL 96.523716 227.995315 \nL 134.643623 229.065688 \nL 210.883438 229.874489 \nL 363.363068 229.527904 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- relationship between epoch &amp; runtime -->\n    <g transform=\"translate(95.76375 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 269 3500 \nL 844 3500 \nL 1563 769 \nL 2278 3500 \nL 2956 3500 \nL 3675 769 \nL 4391 3500 \nL 4966 3500 \nL 4050 0 \nL 3372 0 \nL 2619 2869 \nL 1863 0 \nL 1184 0 \nL 269 3500 \nz\n\" id=\"DejaVuSans-77\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 1556 2509 \nQ 1272 2256 1139 2004 \nQ 1006 1753 1006 1478 \nQ 1006 1022 1337 719 \nQ 1669 416 2169 416 \nQ 2466 416 2725 514 \nQ 2984 613 3213 813 \nL 1556 2509 \nz\nM 1997 2859 \nL 3584 1234 \nQ 3769 1513 3872 1830 \nQ 3975 2147 3994 2503 \nL 4575 2503 \nQ 4538 2091 4375 1687 \nQ 4213 1284 3922 891 \nL 4794 0 \nL 4006 0 \nL 3559 459 \nQ 3234 181 2878 45 \nQ 2522 -91 2113 -91 \nQ 1359 -91 881 339 \nQ 403 769 403 1441 \nQ 403 1841 612 2192 \nQ 822 2544 1241 2853 \nQ 1091 3050 1012 3245 \nQ 934 3441 934 3628 \nQ 934 4134 1281 4442 \nQ 1628 4750 2203 4750 \nQ 2463 4750 2720 4694 \nQ 2978 4638 3244 4525 \nL 3244 3956 \nQ 2972 4103 2725 4179 \nQ 2478 4256 2266 4256 \nQ 1938 4256 1733 4082 \nQ 1528 3909 1528 3634 \nQ 1528 3475 1620 3314 \nQ 1713 3153 1997 2859 \nz\n\" id=\"DejaVuSans-26\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"38.863281\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"100.386719\" xlink:href=\"#DejaVuSans-6c\"/>\n     <use x=\"128.169922\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"189.449219\" xlink:href=\"#DejaVuSans-74\"/>\n     <use x=\"228.658203\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"256.441406\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"317.623047\" xlink:href=\"#DejaVuSans-6e\"/>\n     <use x=\"381.001953\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"433.101562\" xlink:href=\"#DejaVuSans-68\"/>\n     <use x=\"496.480469\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"524.263672\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"587.740234\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"619.527344\" xlink:href=\"#DejaVuSans-62\"/>\n     <use x=\"683.003906\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"744.527344\" xlink:href=\"#DejaVuSans-74\"/>\n     <use x=\"783.736328\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"865.523438\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"927.046875\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"988.570312\" xlink:href=\"#DejaVuSans-6e\"/>\n     <use x=\"1051.949219\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"1083.736328\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"1145.259766\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"1208.736328\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"1269.917969\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"1324.898438\" xlink:href=\"#DejaVuSans-68\"/>\n     <use x=\"1388.277344\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"1420.064453\" xlink:href=\"#DejaVuSans-26\"/>\n     <use x=\"1498.042969\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"1529.830078\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"1570.943359\" xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"1634.322266\" xlink:href=\"#DejaVuSans-6e\"/>\n     <use x=\"1697.701172\" xlink:href=\"#DejaVuSans-74\"/>\n     <use x=\"1736.910156\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"1764.693359\" xlink:href=\"#DejaVuSans-6d\"/>\n     <use x=\"1862.105469\" xlink:href=\"#DejaVuSans-65\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0941d99f72\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 78. GPU上での学習\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 79. 多層ニューラルネットワーク\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "google colabのGPUを利用した\n",
    "- [code](https://colab.research.google.com/drive/10pzYcGvT5tNxa3WwQIwqX7FDov8kMzCx?usp=sharing)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('.venv': poetry)"
  },
  "interpreter": {
   "hash": "fa5814d71535c4ea6661cd60c92af81183f3ce57fda3f81ad9d28fa330081139"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}